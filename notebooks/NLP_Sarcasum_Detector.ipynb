{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71cc044-ee50-4199-b220-80396639c11d",
   "metadata": {},
   "source": [
    "# 1. Data Collection and Preprocessing\n",
    "\n",
    "The aim of this project is to develop a robust sarcasm detection model using real-world textual data. To improve generalization and model performance, I combined multiple sarcasm-related datasets from different sources and platforms. These included both short texts (like tweets or headlines) and slightly longer social media posts. Because these datasets were heterogeneous in format and structure, a detailed preprocessing pipeline was necessary to unify and clean the data for further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Data Sources\n",
    "\n",
    "Four separate datasets were collected and processed:\n",
    "\n",
    "- **Sarcasm Headlines Dataset (JSON)**: Contains 26,709 labeled headlines from The Onion and HuffPost. Each entry includes a headline, a link, and a binary sarcasm label (`is_sarcastic`).\n",
    "  \n",
    "- **Sarcasm Tweets Dataset (CSV)**: This dataset consists of sarcastic and non-sarcastic tweets labeled as `'yes'` or `'no'`. It required label mapping and cleaning due to informal tweet structure.\n",
    "  \n",
    "- **train-En.csv (CSV)**: A dataset of tweets in English labeled for sarcasm. The `tweet` column contains the text and the `sarcastic` column provides the labels.\n",
    "  \n",
    "- **train-ban-balanced-sarcasm.csv (CSV)**: Another tweet dataset that includes balanced sarcastic and non-sarcastic examples with text in the `comment` column and binary labels in the `label` column.\n",
    "\n",
    "Each dataset provided useful linguistic variation, helping the final model generalize across different tones, platforms, and text styles.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Loading and Unifying the Data\n",
    "\n",
    "Each dataset was loaded into a separate Pandas DataFrame. Because the column names varied (e.g. `'headline'`, `'tweet'`, `'comment'`), all text fields were renamed to a common column called `text`, and all target labels were unified into a column called `label`.\n",
    "\n",
    "The tweet datasets with `'yes'/'no'` labels were mapped to binary format (`1` for sarcastic, `0` for not sarcastic).\n",
    "\n",
    "Once standardized, the datasets were concatenated into a single unified DataFrame using `pd.concat()`. After merging, the dataset contained over **1 million rows**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Initial Dataset Characteristics\n",
    "\n",
    "Before cleaning, the combined dataset had:\n",
    "- **1,046,003 total rows**\n",
    "- Varying text lengths (from single words to multi-sentence posts)\n",
    "- Duplicate entries across datasets (especially common sarcastic examples)\n",
    "\n",
    "A class distribution analysis showed a nearly balanced label distribution, which is beneficial for binary classification and means oversampling or undersampling was unnecessary.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Text Cleaning Pipeline\n",
    "\n",
    "To ensure the data was high quality and consistent, the following cleaning steps were implemented:\n",
    "\n",
    "- **Duplicate Removal**: Text duplicates were removed using `drop_duplicates()` to prevent model bias toward repeated entries.\n",
    "\n",
    "- **Hashtag Extraction (Optional Analysis)**: Hashtags were extracted from tweets using regex to investigate their frequency and semantic content. While not directly used as features, this step helped explore common sarcastic tags like `#sarcasm`, `#irony`, etc.\n",
    "\n",
    "- **URL and Emoji Removal**: All web links and emoji characters were stripped to reduce noise.\n",
    "\n",
    "- **Hashtag and Symbol Cleaning**: The `#` symbol was removed, and only the tag content was kept. Other special characters not relevant to text classification (e.g. `@`, `^`, `$`) were eliminated.\n",
    "\n",
    "- **Whitespace Normalization**: Extra spaces were removed and replaced with single spaces to avoid tokenization issues.\n",
    "\n",
    "- **Repeated Character Normalization**: Repeated characters were reduced (e.g., `\"soooo\"` became `\"so\"`) to help the tokenizer treat them as valid tokens rather than noise.\n",
    "\n",
    "- **Contraction Expansion**: Using the `contractions` library, informal contractions like `\"don't\"` or `\"it's\"` were expanded to `\"do not\"` and `\"it is\"` respectively to improve semantic clarity.\n",
    "\n",
    "- **Negation Handling**: Words following negation cues like `\"not\"`, `\"never\"`, or `\"can't\"` were suffixed with `_NEG` (e.g., `\"not happy\"` → `\"not happy_NEG\"`), a known technique in sentiment and sarcasm detection that helps models understand polarity reversal.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 Filtering Out Problematic Entries\n",
    "\n",
    "Several filters were applied to remove noisy or invalid data:\n",
    "\n",
    "- **Empty Strings**: All entries with missing or whitespace-only `text` fields were removed.\n",
    "\n",
    "- **Very Short Texts**: Texts shorter than 3 characters (e.g. `'OK'`, `'No'`) were filtered out as they lacked enough context for sarcasm detection.Those were checked by hand to ensire no usefull information was missing\n",
    "\n",
    "- **Symbol-Only Entries**: Rows made entirely of punctuation or hashtags were excluded.\n",
    "\n",
    "- **Outlier Lengths**: Extremely long texts (over 500 characters) were removed as outliers. Only 236 rows exceeded this threshold.\n",
    "\n",
    "These steps helped retain only meaningful samples while removing edge cases that could introduce noise during training. This was also investigated by hand and found out that big % of the long text was repeating. \n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Final Dataset Summary\n",
    "\n",
    "After cleaning and filtering:\n",
    "\n",
    "- **Final dataset size**: 972,891 text samples\n",
    "- **Balanced class distribution**: \n",
    "  - Sarcastic (`label = 1`): 50.04%\n",
    "  - Non-sarcastic (`label = 0`): 49.96%\n",
    "\n",
    "This balanced dataset allows for unbiased model training and evaluation, without requiring synthetic data generation or resampling strategies.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6d6e88-6ce9-4c9b-abc2-130ccb392dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',\n",
       "  'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365',\n",
       "  'headline': \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697',\n",
       "  'headline': \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302',\n",
       "  'headline': 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb',\n",
       "  'headline': 'j.k. rowling wishes snape happy birthday in the most magical way',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/advancing-the-worlds-women_b_6810038.html',\n",
       "  'headline': \"advancing the world's women\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-meat-is-grown-in-a-lab_us_561d1189e4b0c5a1ce607e86',\n",
       "  'headline': 'the fascinating case for eating lab-grown meat',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/boxed-college-tuition-ben_n_7445644.html',\n",
       "  'headline': 'this ceo will send your kids to school, if you work for his company',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/top-snake-handler-leaves-sinking-huckabee-campaign-1819578231',\n",
       "  'headline': 'top snake handler leaves sinking huckabee campaign',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/fridays-morning-email-inside-trumps-presser-for-the-ages_us_58a6e33ee4b07602ad53a315',\n",
       "  'headline': \"friday's morning email: inside trump's presser for the ages\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/airline-passengers-tackle-man-who-rushes-cockpit-in-bomb-threat_us_59302e57e4b07572bdbf9460',\n",
       "  'headline': 'airline passengers tackle man who rushes cockpit in bomb threat',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/facebook-healthcare_n_5926140.html',\n",
       "  'headline': 'facebook reportedly working on healthcare features and apps',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.theguardian.com/world/2016/may/31/north-korea-praises-trump-and-urges-us-voters-to-reject-dull-hillary',\n",
       "  'headline': \"north korea praises trump and urges us voters to reject 'dull hillary'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jeffrey-lord-worst-comments_us_598cd410e4b09071f6989d91',\n",
       "  'headline': \"actually, cnn's jeffrey lord has been 'indefensible' for a while\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/barcelona-refugee-protest_us_58aa040ce4b037d17d290230',\n",
       "  'headline': 'barcelona holds huge protest in support of refugees',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/nuclear-bomb-detonates-during-rehearsal-for-spider-man-1819572009',\n",
       "  'headline': \"nuclear bomb detonates during rehearsal for 'spider-man' musical\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/cosby-lawyer-asks-why-accusers-didn-t-come-forward-to-b-1819577265',\n",
       "  'headline': \"cosby lawyer asks why accusers didn't come forward to be smeared by legal team years ago\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/stock-analysts-confused-frightened-by-boar-market-1819567580',\n",
       "  'headline': 'stock analysts confused, frightened by boar market',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/bloomberg-philanthropies-what-works-cities-expands_us_566746f3e4b080eddf55ee73',\n",
       "  'headline': \"bloomberg's program to build better cities just got bigger\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/craig-hicks-indicted-chapel-hill_n_6692980.html',\n",
       "  'headline': 'craig hicks indicted',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/courtroom-sketch-artist-has-clear-manga-influences-1820298494',\n",
       "  'headline': 'courtroom sketch artist has clear manga influences',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/trump-assures-nation-that-decision-for-syrian-airstrike-1819579813',\n",
       "  'headline': 'trump assures nation that decision for syrian airstrikes came after carefully considering all his passing whims',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/qatar-dutch-woman-raped_us_575eb891e4b00f97fba8cead',\n",
       "  'headline': 'qatar deporting dutch woman who reported she was drugged and raped',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://pubx.co/dnWZew',\n",
       "  'headline': \"this is why you shouldn't go to the circus\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ted-cruz-republicans-lose-congress_us_5a9f60cee4b0e9381c135ba6',\n",
       "  'headline': \"ted cruz hits the panic button: 'we could lose both houses of congress'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/why-writers-must-plan-to-_b_8672192.html',\n",
       "  'headline': 'why writers must plan to be surprised',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/obama-veterans-day_us_564372e9e4b08cda3486f09b',\n",
       "  'headline': 'obama visits arlington national cemetery to honor veterans',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/ex-con-back-behind-bar-1819589400',\n",
       "  'headline': 'ex-con back behind bar',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/after-careful-consideration-bush-recommends-oil-drilli-1819586993',\n",
       "  'headline': 'after careful consideration, bush recommends oil drilling',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/remembrance-is-the-beginn_b_5382344.html',\n",
       "  'headline': 'remembrance is the beginning of the task',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nemtsov-killing-motive_n_6830086.html',\n",
       "  'headline': 'allies: islamist motive for killing nemtsov is nonsense',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/gillian-jacobs-life-partners_n_5201193.html',\n",
       "  'headline': \"gillian jacobs on what it's like to kiss adam brody\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/uber-repay-nyc-drivers-millions-tax-commission_us_59259fa0e4b00c8df2a0d702',\n",
       "  'headline': \"uber vows to repay nyc drivers 'tens of millions' after tax snafu\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/apple-may-have-poached-electric-motorcycle-company-to-death_us_5624f1f3e4b0bce347014203',\n",
       "  'headline': 'apple may have poached electric motorcycle company to death',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/drug-resistant-bacteria-often-lurk-in-childrens-dogs-sandboxes_us_5967ae43e4b03389bb15cfd6',\n",
       "  'headline': \"drug-resistant bacteria often lurk in children's, dogs' sandboxes\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/if-you-see-a-muslim-at-the-airport_us_588ddf13e4b0cd25e49049d8',\n",
       "  'headline': 'if you see a muslim at the airport',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/giant-altoid-heading-toward-earth-1819586323',\n",
       "  'headline': 'giant altoid heading toward earth',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/moana-box-office-thanksgiving_us_583b435be4b000af95ee8aa4',\n",
       "  'headline': \"'moana' sails straight to the top of the box office with massive $81.1 million opening\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/bud-selig-baseball_b_6007732.html',\n",
       "  'headline': 'selig counted money while baseball lost the next generation of fans',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/robin-williams-inflicted-on-holiday-moviegoers-for-eigh-1819564977',\n",
       "  'headline': 'robin williams inflicted on holiday moviegoers for eighth straight year',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/devin-nunes-vows-to-never-reveal-source-of-white-house-leak_us_58dae860e4b01ca7b427db5b',\n",
       "  'headline': \"devin nunes vows to 'never' reveal source of surveillance claims\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/scott-longert-sleep-apnea-electronic-stimulation-implant-_n_5460348.html',\n",
       "  'headline': 'scott used to stop breathing nearly 40 times an hour. this device changed his life',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/rescuers-heroically-help-beached-garbage-back-into-ocea-1819578060',\n",
       "  'headline': 'rescuers heroically help beached garbage back into ocean',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/medical-workers-drop-soccer-player-from-stretcher_us_56deebe5e4b0ffe6f8ea9860',\n",
       "  'headline': \"medics drop soccer player from stretcher; he's ticked\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/give-the-gift-of-play-thi_b_6309948.html',\n",
       "  'headline': 'give the gift of play this holiday season',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/christian-bale-visits-sikh-temple-victims-1819573750',\n",
       "  'headline': 'christian bale visits sikh temple victims',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/sean-spicer-maternity-leave-health-care-bill_us_58d419d7e4b0f838c630a352',\n",
       "  'headline': 'spicer denies that ending maternity care guarantee would mean women pay more for health care',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/right-to-live-life-in-complete-stunned-horror-added-t-1819574325',\n",
       "  'headline': \"'right to live life in complete, stunned horror,' added to constitution\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/nasa-now-almost-positive-mars-is-rocky-1819573727',\n",
       "  'headline': 'nasa now almost positive mars is rocky',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/monster-undeterred-by-night-light-1819564473',\n",
       "  'headline': 'monster undeterred by night-light',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/diy-sports-equipment-clos_n_5420357.html',\n",
       "  'headline': 'diy: sports equipment closet',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nyc-concert-shooting_us_57468bbbe4b03ede4413e137',\n",
       "  'headline': '1 dead, 3 injured in shooting at t.i. concert in nyc',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/longtime-teacher-retires-without-changing-a-single-stud-1819573446',\n",
       "  'headline': \"longtime teacher retires without changing a single student's life\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-trump-northeast-state-polls_us_57069d07e4b0b90ac27184eb',\n",
       "  'headline': 'donald trump heading for a series of wins in the northeast, polls say',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.wsj.com/articles/self-financing-campaign-all-the-way-would-have-been-a-stretch-for-trump-1463341722',\n",
       "  'headline': \"donald trump wouldn't have had the ready cash to self-finance entire campaign — analysis\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/new-star-wars-film-once-again-disappoints-die-hard-ni-1821291309',\n",
       "  'headline': \"new 'star wars' film once again disappoints die-hard nien nunb fans\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/bats-shooed-out-of-nations-waterslide-tunnels-in-prepar-1819573552',\n",
       "  'headline': \"bats shooed out of nation's waterslide tunnels in preparation for summer\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/mobile-news-crew-reports-on-own-van-breaking-down-1819587345',\n",
       "  'headline': 'mobile news crew reports on own van breaking down',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/syria-humanitarian-crisis_us_59f2fbf2e4b07fdc5fbd5a75',\n",
       "  'headline': \"un rights chief calls humanitarian situation in syria 'an outrage'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/track-santa-on-christmas-eve-norad_us_5a3a6364e4b0b0e5a79e9b6e',\n",
       "  'headline': \"how to track santa claus' flight around the world this christmas eve\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/obama-has-colorado-appraised-1819576803',\n",
       "  'headline': 'obama has colorado appraised',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/trouble-again-in-tvs-africa-1819565669',\n",
       "  'headline': \"trouble again in tv's africa\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/brita-unveils-new-in-throat-water-filters-1819578824',\n",
       "  'headline': 'brita unveils new in-throat water filters',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/report-john-grisham-slowly-but-surely-climbing-list-of-1826058008',\n",
       "  'headline': 'report: john grisham slowly but surely climbing list of greatest living american authors',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/ghost-cant-make-a-simple-cup-of-coffee-without-everyone-1819567608',\n",
       "  'headline': \"ghost can't make a simple cup of coffee without everyone freaking out\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/tupperware-will-never-truly-recover-from-red-curry-left-1819592714',\n",
       "  'headline': 'tupperware will never truly recover from red curry leftovers',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/one-of-the-planets-most-powerful-forces-for-change_us_59dd0877e4b07a185aa75ec6',\n",
       "  'headline': \"one of the planet's most powerful forces for change? an adolescent girl\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/draymond-green-wnba_us_573c7f0ee4b0646cbeeba0fb',\n",
       "  'headline': 'how does draymond green take his game to the next level? by tuning in to the wnba',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/area-woman-said-sorry-118-times-yesterday-1819576089',\n",
       "  'headline': \"area woman said 'sorry' 118 times yesterday\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ryan-lochte-apology_us_57b715bae4b0b51733a2e327',\n",
       "  'headline': 'ryan lochte apologizes for behavior in rio',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/north-dakota-not-heard-from-in-48-hours-1819564531',\n",
       "  'headline': 'north dakota not heard from in 48 hours',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/4-lessons-prison-taught-m_b_7108198.html',\n",
       "  'headline': '4 lessons prison taught me about power and control',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/sick-fucks-line-up-to-gape-at-dead-body-1819591545',\n",
       "  'headline': 'sick fucks line up to gape at dead body',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/muslim-fraternity-alif-laam-meem_n_5405764.html',\n",
       "  'headline': \"what is america's first muslim fraternity really like?\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-vicious-knot-of-syria-the-untangling-process-contains_us_590ff9efe4b0f7118072462a',\n",
       "  'headline': 'the vicious knot of syria, the untangling process contains solutions in our time',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/peter-defazio-craft-beer_us_56215319e4b0bce34700aa00',\n",
       "  'headline': 'this congressman thinks we can fix the economy by drinking beer',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/breast-implants-found-to-cause-problems-in-laboratory-m-1819586306',\n",
       "  'headline': 'breast implants found to cause problems in laboratory mice',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-strikes-syria-retaliation-chemical-attack_us_5acc7508e4b07a3485e7e642',\n",
       "  'headline': 'trump orders strikes on syria in retaliation for chemical attack',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/ceos-funeral-a-networking-dream-1819569324',\n",
       "  'headline': \"ceo's funeral a networking dream\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/year-round-schooling-how_b_5740932.html',\n",
       "  'headline': 'year-round schooling: how it would help minority students',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/zen-teachers-abuse-letter_n_6488386.html',\n",
       "  'headline': '90 zen teachers pledge to change culture that fosters abuse',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/substantiated-complaints-_n_6658222.html',\n",
       "  'headline': 'nypd weighs allowing chokeholds following eric garner death',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/sonny-bono-foundation-prevents-at-risk-youths-from-skii-1819574621',\n",
       "  'headline': 'sonny bono foundation prevents at-risk youths from skiing into trees',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/pope-francis-reminds-the-world-that-caring-for-the-earth-is-everyones-responsibility_us_593595d7e4b0cfcda9167c04',\n",
       "  'headline': \"pope francis reminds the world that caring for the earth is everyone's responsibility\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/6-year-old-cries-when-told-mtm-productions-kitten-dead-1819565939',\n",
       "  'headline': '6-year-old cries when told mtm productions kitten dead by now',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/ex-boyfriend-just-thought-he-d-check-in-and-throw-entir-1819579341',\n",
       "  'headline': \"ex-boyfriend just thought he'd check in and throw entire day off\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/doctors-restore-ken-burns-full-color-vision-after-remo-1819579412',\n",
       "  'headline': \"doctors restore ken burns' full-color vision after removing massive tumor from filmmaker's visual cortex\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/t-pga-awards-2014_n_5672138.html',\n",
       "  'headline': \"'gravity' & '12 years a slave' tie at 2014 pga awards\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/lgbq-sports-people-come-out_us_59dd21a2e4b01df09b76c149',\n",
       "  'headline': 'for national coming out day, 150 lgbtq sports people who have come out in the last year',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/john-oliver-trump-impact-america_us_5a094caae4b05673aa5a3b68',\n",
       "  'headline': 'john oliver lays out the most disturbing ways in which trump impacts america',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jason-chaffetz-strips-mea_n_7629404.html',\n",
       "  'headline': 'house gop crackdown continues',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/i-must-make-sure-you-have-the-skills-to-please-my-gran-1823835071',\n",
       "  'headline': \"'i must make sure you have the skills to please my grandson,' says queen elizabeth disrobing before meghan markle\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/man-visiting-hometown-amazed-to-find-all-his-childhood-1819576945',\n",
       "  'headline': 'man visiting hometown amazed to find all his childhood insecurities still there',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/bug-james-rodriguez_n_5559326.html',\n",
       "  'headline': 'look: world cup star attacked by giant bug',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/clinton-becomes-first-president-to-clear-18-feet-in-pol-1819586822',\n",
       "  'headline': 'clinton becomes first president to clear 18 feet in pole vault',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/hunter-s-thompson-shoots-mouth-off-one-last-time-1819588025',\n",
       "  'headline': 'hunter s. thompson shoots mouth off one last time',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/inhofe-barbra-streisand_n_6261874.html',\n",
       "  'headline': \"inhofe's grand climate conspiracy theory: it's all about barbra streisand\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/questions-i-wish-younger-people-would-stop-asking_b_6842198.html',\n",
       "  'headline': '5 questions i wish younger people would stop asking me',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/albuquerque-shooting_n_5663923.html',\n",
       "  'headline': 'albuquerque shooter on the loose; gunman leaves 1 dead, 3 injured',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/an-adventure-through-hell_b_7624316.html',\n",
       "  'headline': \"what it's like to lose everything in a flood\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/watch-demi-lovato-drops-n_n_6377942.html',\n",
       "  'headline': \"demi lovato drops emotional 'nightingale' music vid\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/miley-cyrus-liam-hemsworth-nye-kiss_us_586a8feae4b0d9a5945c150c',\n",
       "  'headline': 'miley cyrus and liam hemsworth smooch on nye, and the world notices',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/jealous-gps-clearly-wants-man-to-back-over-wife-1819589581',\n",
       "  'headline': 'jealous gps clearly wants man to back over wife',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/natalie-morales-queer_us_5958ecbae4b0da2c7324148b',\n",
       "  'headline': \"'parks and rec' star natalie morales comes out as queer\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/our-top-italy-tours-for-2_b_5787288.html',\n",
       "  'headline': 'the top italy tours for 2015',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/cindy-crawford-daughter-kaia-gerber_us_56b224a9e4b01d80b244a90d',\n",
       "  'headline': \"kaia gerber, cindy crawford's daughter, lands major fashion campaign\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://sports.theonion.com/l-a-grants-clippers-12-for-new-nets-1819588443',\n",
       "  'headline': 'l.a. grants clippers $12 for new nets',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/david-beckham-matching-victoria_us_593add0be4b0c5a35c9f03d7',\n",
       "  'headline': 'james corden roasts david beckham for matching outfits with posh',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/cafe-adds-heartbreaking-little-lunch-menu-1819577737',\n",
       "  'headline': 'café adds heartbreaking little lunch menu',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/mcdonalds-vows-all-green-packaging-goal_us_5a5f95bde4b054e35176b509',\n",
       "  'headline': \"mcdonald's says its packaging will be 100 percent green by 2025\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kris-jenner-turned-all-the-way-up-for-drunken-valentines-day-karaoke_us_5a859a15e4b0774f31d2ceb0',\n",
       "  'headline': \"kris jenner turned all the way up for drunken valentine's day karaoke\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/whale-regrets-eating-290-000-plastic-poker-chips-that-f-1819579538',\n",
       "  'headline': 'whale regrets eating 290,000 plastic poker chips that fell off container ship',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/texas-ebola-patient-thomas-eric-duncan_n_5935286.html',\n",
       "  'headline': \"texas ebola patient 'fighting for his life'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/is-it-too-late-to-audition-asks-perfect-actor-for-ro-1819579594',\n",
       "  'headline': \"'is it too late to audition?' asks perfect actor for role, poking head into room just as producers were giving up hope\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/robert-de-niro-to-turn-58-for-movie-role-1819586959',\n",
       "  'headline': 'robert de niro to turn 58 for movie role',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/scott-pruitt-donald-trump-climate-change_us_5936979de4b013c4816ae169',\n",
       "  'headline': 'scott pruitt (sort of) answers whether trump believes in climate change',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/father-son-urinal-thieves_n_5420448.html',\n",
       "  'headline': 'accused father and son urinal thieves flushed out by cops',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/just-take-it-slow-and-you-ll-be-fine-drunk-driver-a-1820399426',\n",
       "  'headline': \"'just take it slow, and you'll be fine,' drunk driver assures self while speeding away in stolen police car\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/philadelphia-mayor-super-pac_n_7268872.html',\n",
       "  'headline': \"3 libertarians fuel $7 million super pac in philadelphia's mayoral democratic primary\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/air-force-oath_n_5838802.html',\n",
       "  'headline': \"air force will no longer require 'so help me god' in enlistment oaths\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/paul-newman-dies-after-consuming-51-hard-boiled-eggs-1819589151',\n",
       "  'headline': 'paul newman dies after consuming 51 hard-boiled eggs',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/yak-chews-thoughtfully-1819586953',\n",
       "  'headline': 'yak chews thoughtfully',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/man-worried-about-drug-dealer-whos-not-picking-up-phone-1819575944',\n",
       "  'headline': \"man worried about drug dealer who's not picking up phone\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/dad-recommends-hotel-10-miles-away-from-city-you-re-vis-1823892709',\n",
       "  'headline': \"dad recommends hotel 10 miles away from city you're visiting\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/paris-building-explosion_n_5743914.html',\n",
       "  'headline': 'explosion fells building outside paris, killing at least 2',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/source-of-donald-trumps-military-expertise-finally-revealed_us_57d19fb4e4b00642712c3666',\n",
       "  'headline': \"the source of donald trump's military expertise finally revealed!\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/bernie-sanders-union-workers_us_56aa7ed6e4b05e4e3703b874',\n",
       "  'headline': 'union claims sanders campaign staffers posed as members to influence workers',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/seattle-mayors-accuser-in-sex-abuse-lawsuit-comes-forward_us_58f77208e4b0de5bac429ed3',\n",
       "  'headline': \"seattle mayor's accuser in sex-abuse lawsuit comes forward\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/southern-comfort-comforts-southerner-1819564529',\n",
       "  'headline': 'southern comfort comforts southerner',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/sweden-fox-news-trump-police_us_58ab095ee4b037d17d29be2b',\n",
       "  'headline': 'swedish police featured in film shown by fox news say they were selectively edited',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/chromat-plus-size-models_us_55f420e9e4b063ecbfa48f3c',\n",
       "  'headline': 'chromat features not 1, but 2 plus-size models on its runway',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.baltimoresun.com/news/maryland/bs-md-eyeam-baltimore-tour-20150515-story.html',\n",
       "  'headline': 'councilman calls on baltimore rappers to inspire students',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/10-qualities-of-your-inner-spirit_b_7067524.html',\n",
       "  'headline': '10 qualities of your inner spirit',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/chubby-jewish-boy-dreams-of-one-day-being-next-apatow-m-1819570967',\n",
       "  'headline': 'chubby jewish boy dreams of one day being next apatow muse',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-only-parenting-advice-id-dare-to-give_us_59c75602e4b0f2df5e83af0b',\n",
       "  'headline': \"the only parenting advice i'd dare to give\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/family-chooses-different-dog-than-reincarnated-grandfat-1819578924',\n",
       "  'headline': 'family chooses different dog than reincarnated grandfather',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/celebrities-fourth-of-july_us_577a5ff6e4b0a629c1aa7a94',\n",
       "  'headline': 'celebrities celebrate fourth of july with some fun in the sun',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/international-womens-day-_23_b_6805300.html',\n",
       "  'headline': 'international women\\'s day: will \"western women save the world\"?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/michelle-phan-empowers-women_n_6271018.html',\n",
       "  'headline': \"michelle phan, youtube's 'beauty bestie,' empowers women from the outside in\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/caitlyn-jenner-responds-ricky-gervais-jokes_us_5698f048e4b0b4eb759e0957',\n",
       "  'headline': \"caitlyn jenner responds to ricky gervais' golden globes jokes\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/joy-behar-response-to-michael-flynn-guilty-plea-with-pure-joy_us_5a21a249e4b03c44072d6cdb',\n",
       "  'headline': 'joy behar responds to michael flynn guilty plea with pure joy',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/6-things-no-one-tells-women-about-their-weight-loss-journey_b_7003184.html',\n",
       "  'headline': '6 things no one tells women about their weight loss journey',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/fan-disappointed-to-learn-l-ron-hubbard-scientologist-1819580236',\n",
       "  'headline': 'fan disappointed to learn l. ron hubbard scientologist',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/marriage-breaks-up-over-procreative-differences-1819586883',\n",
       "  'headline': 'marriage breaks up over procreative differences',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ne-yo-investment-tech-school-holberton_us_59074214e4b02655f83eaad3',\n",
       "  'headline': 'ne-yo helps to raise $2.3 million for california engineering school',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/elizabeth-warren-donald-trump_us_575a0b38e4b0ced23ca7a3b4',\n",
       "  'headline': \"elizabeth warren calls donald trump a 'racist bully'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/baltimore-bus-crash_us_58189085e4b064e1b4b4a389',\n",
       "  'headline': 'six dead, 10 hurt in baltimore commuter, school bus crash',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/fox-news-problem-solvers-in-way-over-their-heads-1819587510',\n",
       "  'headline': 'fox news problem solvers in way over their heads',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kim-gordon-lana-del-rey_n_6745694.html',\n",
       "  'headline': 'kim gordon clarifies her comments on lana del rey and feminism',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/bashar-al-assad-tries-tiny-bit-of-sarin-gas-on-self-to-1819575557',\n",
       "  'headline': \"bashar al-assad tries tiny bit of sarin gas on self to see what it's like\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/my-niece-has-cancer-and-im-ticked-about-it_us_5946bf47e4b0d188d027ff8b',\n",
       "  'headline': \"my niece has cancer and i'm ticked about it\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/stephanie-seymour-drunk-driving-arrest_us_569ddb71e4b0cd99679b37c8',\n",
       "  'headline': 'supermodel stephanie seymour arrested, charged with drunken driving',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/scoliosis-what-you-need-to-know_us_590ba7e0e4b046ea176ae971',\n",
       "  'headline': 'scoliosis: what you need to know',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/controversy-erupts-after-uk-retailer-removes-gender-labels-from-kids-clothes_us_59aebad4e4b0dfaafcf2bdd7',\n",
       "  'headline': \"controversy erupts after uk retailer removes gender labels from kids' clothes\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/teacher-bitches-about-paycheck-to-sixth-grade-class-1819566662',\n",
       "  'headline': 'teacher bitches about paycheck to sixth-grade class',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/area-insurance-salesman-celebrates-14th-year-of-quoting-1819565058',\n",
       "  'headline': 'area insurance salesman celebrates 14th year of quoting fletch',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/friends-always-on-best-behavior-around-neil-labute-1819567779',\n",
       "  'headline': 'friends always on best behavior around neil labute',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/man-vows-never-to-watch-another-sci-fi-movie-with-physi-1819566728',\n",
       "  'headline': 'man vows never to watch another sci-fi movie with physicist friend',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/biologists-announce-they-re-all-done-with-rodents-1819578401',\n",
       "  'headline': \"biologists announce they're all done with rodents\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttps://www.washingtonpost.com/lifestyle/megyn-kelly-preps-for-her-trump-interview-a-chance-to-go-to-a-different-place/2016/05/16/15e46206-187a-11e6-9e16-2e5a123aac62_story.html?hpid=hp_hp-top-table-main_kelly-2pm:homepage/story',\n",
       "  'headline': \"megyn kelly on donald trump: 'i have done my level best to not make this story about me'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-secret-behind-a-one-d_b_7194404.html',\n",
       "  'headline': 'the secret behind a one day project going viral',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/creating-leverage-where-n_b_6303448.html',\n",
       "  'headline': 'creating leverage where none seems to exist',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/new-movie-taps-into-nations-love-of-rapping-kangaroos-1819566721',\n",
       "  'headline': \"new movie taps into nation's love of rapping kangaroos\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/newspaper-starting-to-worry-spending-so-much-time-on-fa-1819580373',\n",
       "  'headline': 'newspaper starting to worry spending so much time on facebook not healthy for it',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-lawsuit-against-black-lives-matter-and-the-central_us_59640672e4b0911162fc2e6b',\n",
       "  'headline': 'the lawsuit against black lives matter and the central meaning of the first amendment',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/vatican-putting-out-feelers-for-how-public-would-react-1819579340',\n",
       "  'headline': \"vatican putting out feelers for how public would react to another children's crusade\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://baltimore.cbslocal.com/2015/07/18/the-wire-cast-reunited-in-baltimore-to-uplift-community/',\n",
       "  'headline': \"'the wire' cast reunited in baltimore to uplift community\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/merrick-garland-drug-sentencing-richard-smith_us_57225b30e4b0f309baf0345f',\n",
       "  'headline': \"obama's supreme court nominee just bragged about sending this man to prison. now he's free.\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/alonzo-smith-dc-homicide_us_566ef473e4b0fccee16f3d59',\n",
       "  'headline': 'death of d.c. man in security guard custody ruled a homicide',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/boehner-vows-to-get-stuff-done_us_560aa588e4b0dd8503092743',\n",
       "  'headline': 'boehner vows to leave successor with clean slate',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/trump-makes-last-minute-push-to-appeal-to-whites-1819579415',\n",
       "  'headline': 'trump makes last-minute push to appeal to whites',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/shishmaref-alaska-relocation-vote-2016_us_57b4faa7e4b095b2f5427b1b',\n",
       "  'headline': 'facing rising seas, remote alaskan village votes to move (again)',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/all-of-child-s-fondest-memories-times-when-dad-trying-t-1819577766',\n",
       "  'headline': \"all of child's fondest memories times when dad trying to make up for things\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-worst-place-in-the-wo_b_6192524.html',\n",
       "  'headline': 'the worst place in the world for a child',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/mom-25-years-death-4-month-old-daughter_n_6451446.html',\n",
       "  'headline': \"mom sentenced for encouraging boyfriend's sex assault on baby\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttps://www.washingtonpost.com/opinions/an-assault-on-our-values/2016/06/13/a0eadc98-31ae-11e6-8758-d58e76e11b12_story.html',\n",
       "  'headline': \"donald trump's assault on our values\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/womens-march-inspired-democrats-unseating-gop-men_us_5a03099de4b06ff32c94cb55',\n",
       "  'headline': \"the women's march inspired them to run. now they're unseating gop men.\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/was-it-worth-it-america_b_6956846.html',\n",
       "  'headline': 'was it worth it, america?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.politico.com/story/2016/04/trump-gop-resistance-222551',\n",
       "  'headline': \"gop senators aren't ready to accept trump as their champion\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/martin-shkreli-kalobios-ceo_us_564f55f0e4b0258edb313e7c',\n",
       "  'headline': 'price-gouging pharma ceo takes over cancer company',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/man-failing-to-heed-harsh-lessons-of-past-orders-sonic-1819576683',\n",
       "  'headline': 'man failing to heed harsh lessons of past orders sonic bacon cheeseburger toaster',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/ken-burns-completes-documentary-about-fucking-liars-who-1819579264',\n",
       "  'headline': \"ken burns completes documentary about fucking liars who claimed they watched entire 'jazz' series\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/magic-markered-initials-fail-to-deter-breakroom-rice-ca-1819567927',\n",
       "  'headline': 'magic-markered initials fail to deter breakroom rice-cake thief',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/straight-outta-compton-is_b_7993458.html',\n",
       "  'headline': \"'straight outta compton' is a stunning surprise\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/carly-rae-jepsen-all-that_n_7007920.html',\n",
       "  'headline': \"carly rae jepsen just released her new single, 'all that'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/god-humbled-to-be-the-answer-to-jeopardy-clue-1826072334',\n",
       "  'headline': \"god humbled to be the answer to 'jeopardy!' clue\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/female-barista-getting-a-lot-better-at-avoiding-touchin-1822847043',\n",
       "  'headline': \"female barista getting a lot better at avoiding touching male patrons' hands when they pay\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/konstanz-nightclub-shooting-germany_us_597d9422e4b02a8434b6e44a',\n",
       "  'headline': 'gunman kills one, wounds four in shooting at german nightclub',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/couple-at-point-where-theyre-comfortable-using-toilet-a-1819574876',\n",
       "  'headline': \"couple at point where they're comfortable using toilet at same time\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/cougar-p-23-killed-malibu_us_5a73c590e4b0905433b2a507',\n",
       "  'headline': 'mountain lion tracked by scientists is found dead near malibu road',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/noah-cyrus-has-pipes_us_58920c14e4b0522c7d3e625f',\n",
       "  'headline': \"noah cyrus makes her late-night debut belting out 'make me (cry)'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/scalia-goes-on-abortion-bender-after-being-passed-over-1819568026',\n",
       "  'headline': 'scalia goes on abortion bender after being passed over for chief justice',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/campbells-unveils-one-big-can-sized-noodle-1822424458',\n",
       "  'headline': \"campbell's unveils one big can-sized noodle\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/is-area-man-going-to-finish-those-fries-1819565422',\n",
       "  'headline': 'is area man going to finish those fries?',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/taylor-swift-gremlin-voice_n_6049578.html',\n",
       "  'headline': \"taylor swift used a 'gremlin voice' while writing '1989'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/restaurant-s-eating-challenge-rewards-any-patron-who-ca-1819579155',\n",
       "  'headline': \"restaurant's eating challenge rewards any patron who can consume reasonably portioned meal\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/fun-sticker-placed-on-child-s-ventilator-1819591576',\n",
       "  'headline': \"fun sticker placed on child's ventilator\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-best-way-to-eat-avoca_b_5602606.html',\n",
       "  'headline': 'the best way to eat avocados: avocado pasta',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/that-one-mcdonalds-plate-from-the-70s-holy-shit-there-1819586984',\n",
       "  'headline': \"that one mcdonald's plate from the '70s: holy shit, there it is\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/an-open-letter-to-progres_b_7257776.html',\n",
       "  'headline': \"an open letter to progressives: tpp is not yet 'the most progressive trade agreement in history'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/modern-day-lancelot-offers-to-pay-for-abortion-1819577414',\n",
       "  'headline': 'modern-day lancelot offers to pay for abortion',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/united-airlines-offering-immigrants-special-flights-tha-1819580058',\n",
       "  'headline': 'united airlines offering immigrants special flights that circle u.s. awaiting gaps in travel ban',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/sun-thinking-of-just-collapsing-now-and-getting-this-al-1821437839',\n",
       "  'headline': 'sun thinking of just collapsing now and getting this all over with',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/clarence-thomas-retiring-ginni-thomas_us_57684942e4b015db1bca42e5',\n",
       "  'headline': \"justice thomas' wife calls supreme court retirement report 'bogus'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/trump-retweets-video-from-anti-muslim-hate-group-1820885422',\n",
       "  'headline': 'trump retweets video from anti-muslim hate group',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/long-silent-facebook-friend-comes-out-of-woodwork-with-1819577520',\n",
       "  'headline': 'long-silent facebook friend comes out of woodwork with post asking about insulating windows',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/struggling-airline-helped-by-friendly-giant-1819566439',\n",
       "  'headline': 'struggling airline helped by friendly giant',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-to-breakup-like-a-boss_n_7166338.html',\n",
       "  'headline': '7 ways to breakup like a boss',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/malcolm-in-the-middle-reboot_us_55facc4ae4b0fde8b0cd23fe',\n",
       "  'headline': \"sure seems like frankie muniz wants a 'malcolm in the middle' reboot\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/using-the-united-fiasco-to-flourish-in-the-future_us_58f39cebe4b048372700d953',\n",
       "  'headline': 'using the united fiasco to flourish in the future',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/unemployed-sibling-makes-last-push-for-group-mother-s-d-1819577778',\n",
       "  'headline': \"unemployed sibling makes last push for group mother's day gift\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/3822-voted-americas-favorite-pin-number-1819566070',\n",
       "  'headline': \"3822 voted america's favorite pin number\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/thank-a-teacher-thursday--_b_7463640.html',\n",
       "  'headline': 'thank a teacher thursday: dominic casulli and the power of encouragement, part 1',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-toe-in-the-arctic-ocean_b_7895942.html',\n",
       "  'headline': \"a toe in the arctic ocean: canada's northwest territories on the looney front, part 2\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/it-will-never-happen-to-me_us_590cb4b0e4b0f71180724429',\n",
       "  'headline': \"'it will never happen to me'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/mark-zuckerberg-cited-for-contempt-of-congress-after-re-1825178093',\n",
       "  'headline': 'mark zuckerberg cited for contempt of congress after refusing to shut the fuck up about how he started company in dorm room',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://pubx.co/ik1f48',\n",
       "  'headline': \"rubio supporters get in a scuffle with a 'rubiobot'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-music-of-strangers-a-film-review-by-dr-lloyd_us_5766d293e4b0092652d7a275',\n",
       "  'headline': 'the music of strangers: a film review by dr. lloyd sederer',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/dsm-5-updated-to-accommodate-man-who-is-legitimately-1819579090',\n",
       "  'headline': \"'dsm-5' updated to accommodate man who is legitimately being ordered to kill by the moon\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/purina-debuts-new-slovenly-feast-for-nasty-ass-shelte-1820086710',\n",
       "  'headline': \"purina debuts new 'slovenly feast' for nasty-ass shelter cats\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/obama-darrell-issa_us_580e2324e4b000d0b1578c14',\n",
       "  'headline': 'obama chides darrell issa for touting alliance with him in re-election fight',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/pope-francis-iraq-peace_n_5669666.html',\n",
       "  'headline': \"pope francis' iraq peace message meets the reality of war\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ant-man-and-the-wasp-trailer_us_5ae877bfe4b02baed1be36eb',\n",
       "  'headline': \"'ant-man and the wasp' trailer brings the fun after 'avengers: infinity war'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/frontier-airlines-tells-customers-to-just-fucking-deal-1819580035',\n",
       "  'headline': 'frontier airlines tells customers to just fucking deal with it',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/moses-and-the-red-sea_b_7004042.html',\n",
       "  'headline': 'moses and the red sea',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/everyone-forgets-to-bring-swimsuits-to-coworker-s-party-1819575001',\n",
       "  'headline': \"everyone forgets to bring swimsuits to coworker's party\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/george-takei-accused-sexual-assault_us_5a0661dde4b01d21c83e9fc8',\n",
       "  'headline': 'george takei accused of groping former male model in 1981',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/new-york-times-adds-color-to-target-under-70-demographi-1819564443',\n",
       "  'headline': 'new york times adds color to target under-70 demographic',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-hollywood-boys-club-that-supports-casey-affleck_us_58b34bbbe4b0658fc20f9720',\n",
       "  'headline': \"the hollywood boys' club that supports casey affleck is a total disgrace\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/3-myths-about-low-libido_b_7640656.html',\n",
       "  'headline': '3 myths about low libido',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/girl-scouts-rocked-by-cookies-for-cash-fundraising-sc-1819586265',\n",
       "  'headline': \"girl scouts rocked by 'cookies for cash' fundraising scandal\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/christian-juggler-regrets-years-wasted-as-secular-juggl-1819568246',\n",
       "  'headline': 'christian juggler regrets years wasted as secular juggler',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/huffpollster-voting-rights-remain-a-problem_us_55c4addee4b0d9b743dbbd78',\n",
       "  'headline': 'huffpollster: americans see progress, room for improvement on voting rights',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/world-will-miss-goal-for-universal-education-by-50-years-un_us_57cec2c1e4b0a22de096dbad',\n",
       "  'headline': 'world will miss goal for universal education by 50 years: un',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jennifer-lopez-name-change_n_6413536.html',\n",
       "  'headline': \"jennifer lopez's name is jennifer lopez again\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-to-rebuild-your-credi_b_5790860.html',\n",
       "  'headline': 'how to rebuild your credit after bankruptcy -- fast',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/google-timelapse-climate-change_us_58491117e4b0f9723d0046d7',\n",
       "  'headline': 'discover how climate change is rapidly transforming our earth with google timelapse',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/man-who-does-everything-at-last-minute-wonders-how-you-1819568263',\n",
       "  'headline': 'man who does everything at last minute wonders how you do it',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/voting-lines-are-shorter-but-mostly-for-whites_us_5a85a1bbe4b00e7aba2d2978',\n",
       "  'headline': 'voting lines are shorter — but mostly for whites',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/conrad-bain-steps-down-as-national-kitsch-reference-lau-1819566330',\n",
       "  'headline': 'conrad bain steps down as national kitsch-reference laureate',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/amanda-slavin-not-just-a-_b_5794432.html',\n",
       "  'headline': 'amanda slavin: not just a statistic',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/laurie-hernandez-and-val-chmerkovskiy-are-already-our-favorite-dancing-with-the-stars-couple_us_57c87863e4b0a22de094ce58',\n",
       "  'headline': \"laurie hernandez and val chmerkovskiy are already our favorite 'dancing with the stars' couple\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/hurricane-matthew-photos_us_57f69868e4b00885f2c68128',\n",
       "  'headline': \"new photos show hurricane matthew's path of destruction in haiti\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/florida-child-marriage-law-under-17_us_5aa5066fe4b086698a9e91d4',\n",
       "  'headline': 'florida lawmakers vote to ban marriage under the age of 17',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/nasa-inadvertently-launches-unmanned-space-shuttle-1819589966',\n",
       "  'headline': 'nasa inadvertently launches unmanned space shuttle',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nick-viall-bachelor-winner_us_58c1bee2e4b0ed71826b408d',\n",
       "  'headline': \"nick viall is no longer a 'bachelor'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/why-america-demonizes-its-teachers_b_7463084.html',\n",
       "  'headline': 'why america demonizes its teachers',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/new-instant-lottery-game-features-three-ways-to-win-19-1819586628',\n",
       "  'headline': 'new instant lottery game features three ways to win, 19,839,947 ways to lose',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/podcast-a-cry-for-help-1819567977',\n",
       "  'headline': 'podcast a cry for help',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/peter-soms-minestrone-soup_b_5777290.html',\n",
       "  'headline': \"fashion designer peter som's legendary minestrone soup recipe\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/lola-flash-photos_n_7172994.html',\n",
       "  'headline': 'photography series spotlighting iconic women over 70 proves the best is yet to come',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/clairvoyant-vince-vaughn-accepts-movie-role-before-its-1819568014',\n",
       "  'headline': \"clairvoyant vince vaughn accepts movie role before it's offered\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kenan-thompson-leaving-snl_n_5862614.html',\n",
       "  'headline': \"report kenan thompson is leaving 'snl' deemed 'inaccurate'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jill-roy-butt_n_5947842.html',\n",
       "  'headline': 'woman hid heroin, oxy under fake butt, cops say',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dont-cry-for-me_b_5646199.html',\n",
       "  'headline': \"'don't cry for me'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/isis-and-the-g41-world_b_5681474.html',\n",
       "  'headline': 'isis and the g-41 world',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/melania-trump-hangs-decayed-badger-carcass-over-white-h-1820886857',\n",
       "  'headline': 'melania trump hangs decayed badger carcass over white house mantel to finish off traditional slovenian christmas decor',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/woman-who-admits-to-having-watched-golden-globes-thinks-1819574376',\n",
       "  'headline': 'woman who admits to having watched golden globes thinks jodie foster embarrassed herself',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-attacks-kirsten-gillibrand_us_5a300bcfe4b07895028418cb',\n",
       "  'headline': \"sarah huckabee sanders defends trump's sexist attack on kirsten gillibrand\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/villa-carlotta-renovation_n_6496596.html',\n",
       "  'headline': \"saying goodbye to hollywood's hottest, seediest address\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/bush-increasingly-focused-on-how-revisionist-history-wi-1819568342',\n",
       "  'headline': 'bush increasingly focused on how revisionist history will see him',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/11-years-later-the-human-_b_5242182.html',\n",
       "  'headline': '11 years later: the human genome paves the way for genomic technonlogy',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nigeria-boko-haram_us_57e81279e4b0e80b1ba2a0fe',\n",
       "  'headline': 'man claiming to be boko haram leader appears in new video',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-do-i-live-with-proof-_b_5812372.html',\n",
       "  'headline': 'how do i live knowing proof of heaven?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/on-pilgrimage-in-india_b_6265672.html',\n",
       "  'headline': 'on pilgrimage in india',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/will-religious-freedom-advocates-oppose-roy-moore_us_5a2987b0e4b09ee35b8ae73e',\n",
       "  'headline': 'will religious freedom advocates oppose roy moore?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/uaw-fca-negotiations_us_55f8228de4b09ecde1d9a378',\n",
       "  'headline': \"uaw, fca still negotiating under 'hour-by-hour' contract extension\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/if-bernie-sanders-wanted-to-solve-problems-like-donald-trump_us_565df661e4b072e9d1c38c37',\n",
       "  'headline': 'if bernie sanders wanted to solve problems like donald trump',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/there-is-no-right-way-just-write_b_5619037.html',\n",
       "  'headline': 'there is no right way, just write',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/rest-of-evening-spent-declaring-asshole-not-going-to-ru-1819576790',\n",
       "  'headline': 'rest of evening spent declaring asshole not going to ruin evening',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/north-carolina-police-camera-footage_us_57850a43e4b0ed2111d7952a',\n",
       "  'headline': \"north carolina doesn't seem to want people to see police camera footage\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/transgender-student-bathrooms_us_5a81c443e4b0c6726e15d2bb',\n",
       "  'headline': \"the education department officially won't deal with transgender students experiencing bathroom discrimination\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/why-we-wont-be-participat_b_6234830.html',\n",
       "  'headline': \"why we won't be participating in black friday\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/hooded-members-of-congress-drown-another-love-child-in-1820838488',\n",
       "  'headline': 'hooded members of congress drown another love child in potomac to prevent affair from getting out',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/merkel-isis_n_5747976.html',\n",
       "  'headline': 'merkel: isis poses major risk to europe',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/alcoholic-kindergarten-teacher-stretches-naptime-to-thr-1819568162',\n",
       "  'headline': 'alcoholic kindergarten teacher stretches naptime to three hours',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/report-store-out-of-good-kind-1819579851',\n",
       "  'headline': 'report: store out of good kind',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/devastating-floods-west-virginia_us_576d52a4e4b0dbb1bbba576f',\n",
       "  'headline': 'devastating floods leave 23 dead in west virginia',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/obamas-grace-the-atlantic_n_7680000.html',\n",
       "  'headline': \"obama's greatest oratory performance\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/grandmother-palms-grandson-10-like-she-fixing-boxing-m-1819578572',\n",
       "  'headline': 'grandmother palms grandson $10 like she fixing boxing match',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/bumper-nilla-crop-spells-profit-for-wafer-growers-1819568618',\n",
       "  'headline': 'bumper nilla crop spells profit for wafer growers',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/6-summer-salads-youll-act_b_5517682.html',\n",
       "  'headline': \"6 summer salads you'll actually crave\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-black-keys-turn-blue_n_5272454.html',\n",
       "  'headline': \"the black keys new album 'turn blue' is now available to stream in full\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/isis-vs-isil-whats-in-a-n_b_5807198.html',\n",
       "  'headline': \"isis vs isil -- what's in a name?\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/national-trust-for-historic-preservation-to-pay-for-and-1819568738',\n",
       "  'headline': \"national trust for historic preservation to pay for andy rooney's upkeep\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/list-of-names-on-gchat-sidebar-like-a-portal-into-area-1819579005',\n",
       "  'headline': \"list of names on gchat sidebar like a portal into area man's past lives\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/tomi-lahren-sues-glenn-beck_us_58e7f216e4b058f0a02f3841',\n",
       "  'headline': 'tomi lahren is suing glenn beck and theblaze',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/greg-hardy-unapologetically-refutes-domestic-abuse-allegations_us_5702cdffe4b083f5c6088971',\n",
       "  'headline': 'greg hardy unapologetically denies domestic abuse allegations',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/tunisia-hotel-attack-survivors_n_7677574.html',\n",
       "  'headline': 'tourists describe scenes of horror in tunisian beach massacre',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-voters-blink_us_596d85e5e4b05561da5a5a40',\n",
       "  'headline': 'trump voters blink',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ahs-pregnant-gaga-photos-may-explain-creepy-kids_us_561fa5c3e4b028dd7ea6bc14',\n",
       "  'headline': \"photos from 'ahs' set may help explain all those creepy blond children\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/study-more-couples-delaying-divorce-until-kids-old-eno-1819576618',\n",
       "  'headline': 'study: more couples delaying divorce until kids old enough to remember every painful detail',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/never-before-heard-buzzword-flying-around-office-can-t-1819578329',\n",
       "  'headline': \"never-before-heard buzzword flying around office can't be good\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/man-not-certain-what-any-of-his-coworkers-names-are-1819574786',\n",
       "  'headline': \"man not certain what any of his coworkers' names are\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/smartphone-warning-label_us_56152dc6e4b0cf9984d7c43a',\n",
       "  'headline': 'psychologists push for smartphone warning labels',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/clinton-receives-400-000-honorary-degrees-for-college-c-1819592824',\n",
       "  'headline': 'clinton receives 400,000 honorary degrees for college commencement speech',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/scavenger-hunt-party-not-leaving-without-twine-1819568822',\n",
       "  'headline': \"scavenger-hunt party 'not leaving without twine'\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/republican-obamacare-problem_us_58af114be4b057efdce9e743',\n",
       "  'headline': 'the republican obamacare dilemma in one 6-minute video',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/rohingya-mass-graves-myanmar_us_5a734e7ae4b0905433b21a6e',\n",
       "  'headline': 'mass graves suggest systematic killing of rohingya in myanmar',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/final-german-u-boat-surrenders-to-allied-powers-1819588851',\n",
       "  'headline': 'final german u-boat surrenders to allied powers',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/private-prison-companies-will-keep-locking-up-immigrants-after-doj-decision_us_57b60b6be4b00d9c3a165506',\n",
       "  'headline': 'private prison companies will still lock up immigrants, despite doj decision',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/woman-s-head-feared-lost-forever-inside-infinity-scarf-1819592751',\n",
       "  'headline': \"woman's head feared lost forever inside infinity scarf\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/fed-up-brookstone-body-massage-chair-now-only-entertain-1819588319',\n",
       "  'headline': 'fed-up brookstone body-massage chair now only entertaining serious buyers',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/trump-asks-entire-senate-to-clear-out-of-chamber-so-he-1819579987',\n",
       "  'headline': 'trump asks entire senate to clear out of chamber so he can speak to comey alone',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/john-perry-barlow-dead_us_5a7beda5e4b08dfc92fff7c7',\n",
       "  'headline': 'grateful dead lyricist, internet pioneer john perry barlow dead at 70',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/new-report-finds-americans-most-interested-in-science-w-1819579453',\n",
       "  'headline': 'new report finds americans most interested in science when moon looks different than usual',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/new-york-times-moves-all-content-you-wont-give-a-shit-a-1819572243',\n",
       "  'headline': \"'new york times' moves all content you won't give a shit about unless you make at least $200k a year into one convenient section\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/philippines-president-encourages-civilians-to-go-ahead-and-kill-drug-addicts_us_57767fc8e4b04164640f91c2',\n",
       "  'headline': 'philippines president calls on civilians to kill drug addicts',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/woman-breaks-down-why-shes-not-just-a-nurse_us_57fd012fe4b0e655eab7a2b7',\n",
       "  'headline': \"woman perfectly breaks down why she's not 'just' a nurse\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/chicago-police-department-to-monitor-all-interactions-w-1819578516',\n",
       "  'headline': 'chicago police department to monitor all interactions with public using new bullet cams',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/davos-meditation_n_6522806.html',\n",
       "  'headline': 'amid the chattering of the global elite, a silent interlude',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/scott-pruitt-scandals-list_us_5ac66dffe4b09d0a1191647f',\n",
       "  'headline': 'at least 23 ethical issues are dogging epa administrator scott pruitt',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/wont-ask-dont-tell_b_7255748.html',\n",
       "  'headline': \"won't ask, don't tell\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/weird-black-dot-actually-part-of-bowl-1819591326',\n",
       "  'headline': 'weird black dot actually part of bowl',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/vegas-baby-well-minus-our-babies_b_5684367.html',\n",
       "  'headline': 'vegas, baby! (well, minus our babies)',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kanye-west-new-song-real-friends_us_56900997e4b0c8beacf6dfee',\n",
       "  'headline': \"people really like kanye's new song 'real friends' because it's really g.o.o.d.\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/suede-spring-pieces_n_6859940.html',\n",
       "  'headline': \"20 suede pieces you'll want to wear all spring\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/fcc-sentences-artie-lange-to-death-1819587542',\n",
       "  'headline': 'fcc sentences artie lange to death',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/behold-the-title-of-american-horror-story-season-7_us_5970e60be4b0aa14ea7872e5',\n",
       "  'headline': \"behold the title of 'american horror story' season 7\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/sikh-turbans-football-game_us_566db725e4b0e292150e3f6a',\n",
       "  'headline': 'football-loving americans harassed for wearing turbans to nfl game',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/to-your-health_b_6931932.html',\n",
       "  'headline': 'to your health',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/new-job-posting-on-craigslist-clearly-for-secretary-of-1819568699',\n",
       "  'headline': 'new job posting on craigslist clearly for secretary of the interior',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/seth-meyers-christmas-close-thanksgiving_us_5837f0e0e4b000af95ee0991',\n",
       "  'headline': \"seth meyers loses it over thanksgiving's proximity to christmas\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/15-espn-trade-machineappr_n_6457158.html',\n",
       "  'headline': '15 ways the knicks can trade carmelo anthony',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/charlotte-gwenyth-gray-batten-disease_n_7558614.html',\n",
       "  'headline': 'parents fight to save their two daughters after tragic diagnosis',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-quiet-practice-where-i-found-my-voice_us_56a988c2e4b0d82286d4ef18',\n",
       "  'headline': 'the quiet practice where i found my voice',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/fcc-assures-nation-their-favorite-verizon-websites-wont-1821305714',\n",
       "  'headline': \"fcc assures nation their favorite verizon websites won't be affected by net neutrality repeal\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/everyone-told-me-my-second-child-would-be-so-much-harder_us_5aecb3e1e4b066cd764091cc',\n",
       "  'headline': 'everyone told me my second child would be so much harder than my first, but they were wrong',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/report-it-still-nowhere-near-okay-to-act-like-donald-t-1819579444',\n",
       "  'headline': 'report: it still nowhere near okay to act like donald trump',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/arizona-church-trump-win_us_5850355be4b0e05aded6267e',\n",
       "  'headline': 'man reportedly unleashes trump-inspired anti-lgbtq rant at church',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://sports.theonion.com/white-sprinter-finishes-fifth-1819586495',\n",
       "  'headline': 'white sprinter finishes fifth',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/wendy-davis-campaign_b_5440570.html',\n",
       "  'headline': \"how i'm finding my voice with wendy davis\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/obama-begins-inauguration-festivities-with-ceremonial-d-1819574406',\n",
       "  'headline': 'obama begins inauguration festivities with ceremonial drone flyover',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/lindsay-wagner-to-star-in-anything-offered-her-1819586354',\n",
       "  'headline': 'lindsay wagner to star in anything offered her',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/democratic-senator-strides-down-corridors-of-powerlessn-1819587815',\n",
       "  'headline': 'democratic senator strides down corridors of powerlessness',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/days-of-rage_us_5913159ce4b07e366cebb793',\n",
       "  'headline': \"here's how trump's rage campaign opened the door to the james comey firing\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/happiness_b_5222784.html',\n",
       "  'headline': 'looking for happiness in all the wrong places',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/drones-beethoven-intel-world-record_us_5693ae9de4b0a2b6fb70b843',\n",
       "  'headline': \"swarm of 100 drones dance to beethoven's 'symphony no. 5' in the night sky\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/fatal-school-bus-crash-cements-bff-status-1819588524',\n",
       "  'headline': 'fatal school bus crash cements bff status',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/no-makeup-look-easier-to-achieve-than-elle-claims-1819567390',\n",
       "  'headline': 'no-makeup look easier to achieve than elle claims',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-outfit-that-got-this-woman-kicked-out-of-her-schools-gym_us_56b4f11ce4b04f9b57d97572',\n",
       "  'headline': \"the outfit that got this woman kicked out of her school's gym\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/bluetooth-headset-worn-throughout-date-1819588665',\n",
       "  'headline': 'bluetooth headset worn throughout date',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/breakroom-tension-at-all-time-high-following-mug-disput-1819565122',\n",
       "  'headline': 'breakroom tension at all-time high following mug dispute',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/depression-symptom-checklist-speaking-to-area-man-as-no-1819578201',\n",
       "  'headline': 'depression symptom checklist speaking to area man as no poem ever could',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/parents-formally-announce-transfer-of-expectations-to-s-1819577993',\n",
       "  'headline': 'parents formally announce transfer of expectations to second child',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/halloween-saturday-night-live-snl-video_us_581472dbe4b0990edc3182ae',\n",
       "  'headline': \"'saturday night live' celebrates halloween with spooktacular montage\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/steven-spielberg-can-his-career-be-salvaged-1819586503',\n",
       "  'headline': 'steven spielberg: can his career be salvaged?',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/local-laundromat-employs-social-media-coordinator-1819575080',\n",
       "  'headline': 'local laundromat employs social media coordinator',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/partygoer-vows-to-fix-keg-1819566083',\n",
       "  'headline': 'partygoer vows to fix keg',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/right-to-own-handheld-device-that-shoots-deadly-metal-p-1819574320',\n",
       "  'headline': 'right to own handheld device that shoots deadly metal pellets at high speed worth all of this',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/oscar-countdown-2002-begins-1819565949',\n",
       "  'headline': 'oscar countdown 2002 begins',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/asteroid-boiled-oceans_n_7457722.html',\n",
       "  'headline': 'disaster movies are tame compared to what happened 3.3 billion years ago',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/washington-dc-officer-shoots-woman-carrying-knife_us_55c6b569e4b0923c12bd22ec',\n",
       "  'headline': 'washington d.c. officer shoots woman carrying knife',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/in-your-face-the-hidden-h_b_5351726.html',\n",
       "  'headline': 'in your face: the hidden history of plastic surgery and why looks matter',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-knicks-fan-open-letter-to-santa_b_6384792.html',\n",
       "  'headline': \"a knicks' fan's open letter to santa\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://sports.theonion.com/wolf-blitzer-decks-boston-man-who-hasn-t-been-healed-by-1819574858',\n",
       "  'headline': \"wolf blitzer decks boston man who hasn't been healed by red sox baseball\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-trump-and-americas-blood-sport-of-choice_us_59e900dce4b05b4f1c3a0717',\n",
       "  'headline': \"donald trump and america's blood sport of choice\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/time-to-end-subsidies-that-are-destroying-forests_us_5975aaeae4b06b511b02c4ac',\n",
       "  'headline': 'time to end subsidies that are destroying forests',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/netflix-to-shut-down-planned-louis-ck-comedy-special_us_5a05ceede4b0e37d2f37310c',\n",
       "  'headline': 'netflix to shut down planned louis c.k. comedy special',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-trump-bill-clinton-obamacare_us_57f4e4c5e4b04c71d6f118a8',\n",
       "  'headline': \"trump undercuts easy obamacare attack with dig about bill clinton's infidelities\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/narcissist-mentally-undresses-self-1819567215',\n",
       "  'headline': 'narcissist mentally undresses self',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/we-got-the-exclusive-look-at-hillarys-dnc-speech-notes_us_579a5161e4b08a8e8b5d1cc8',\n",
       "  'headline': \"we got the exclusive look at hillary's dnc speech notes\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/john-kasich-david-bowie_us_56a570e9e4b0404eb8f20825',\n",
       "  'headline': 'watch john kasich lead an awkward david bowie sing-along',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/pro-governing-is-it-faked-1819586544',\n",
       "  'headline': 'pro governing: is it faked?',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ed-burns-social-media-detox_us_55dc7cb2e4b08cd3359d48da',\n",
       "  'headline': 'how a social media detox helped ed burns become more productive',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/outdoor-movie-guest-excited-to-watch-barely-audible-ba-1819577856',\n",
       "  'headline': \"outdoor movie guest excited to watch barely audible 'back to the future' while sitting on tree root\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/north-korea-guam_us_598c3996e4b0449ed5081ef1',\n",
       "  'headline': \"north korea claims it's planning to fire missiles near guam\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/gorillagram-employee-shot-by-white-house-security-1819587417',\n",
       "  'headline': 'gorillagram employee shot by white house security',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-do-i-make-my-4-month-old-fall-in-love-with-reading_us_5789a482e4b0cbf01e9fd238',\n",
       "  'headline': 'how do i make my 4-month-old fall in love with reading?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/weird-things-left-in-uber_us_58de0c19e4b08194e3b8e46a',\n",
       "  'headline': '15 weirdest things that people have left behind in an uber',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ledell-lee-execution-arkansas_us_58f8134ee4b0cb086d7df22e',\n",
       "  'headline': 'arkansas executes first inmate in 12 years',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/meet-the-other-baldwin-brother-james-1819586063',\n",
       "  'headline': 'meet the other baldwin brother, james!',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-sadder-pride-because-of-washington-inaction_us_59405935e4b09ad4fbe3e9ca',\n",
       "  'headline': 'a sadder pride because of washington inaction',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/brad-pitt-stumbles-across-old-cardboard-box-with-gwynet-1822454158',\n",
       "  'headline': \"brad pitt stumbles across old cardboard box with gwyneth paltrow's head in attic\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/shonda-rhimes-on-the-motivation-behind-her-weight-loss-journey_us_560be4b3e4b0dd850309ea0b',\n",
       "  'headline': 'shonda rhimes on the motivation behind her weight loss journey',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/excellent-educators-for-all_n_5562269.html',\n",
       "  'headline': 'obama plans to tackle major education inequality',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/teaching-learning-and-the_b_6362768.html',\n",
       "  'headline': 'teaching, learning and the college ratings framework',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/pigeon-bread-necklace_us_589b324ce4b04061313a8df3',\n",
       "  'headline': 'fashion-forward pigeon sports bread necklace in bold style choice',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/philip-coleman-chicago-video_us_56670fb1e4b079b2819028f1',\n",
       "  'headline': 'video relaunches investigation into death of man held by chicago police',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/whoopi-goldberg-oscars_us_56a76fe1e4b0172c659413b6',\n",
       "  'headline': \"whoopi goldberg says the oscars 'can't be that racist' because she won once\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/housefly-tracks-dog-shit-all-over-cucumber-slice-1819592647',\n",
       "  'headline': 'housefly tracks dog shit all over cucumber slice',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/hillary-clinton-opens-new-presidential-library-charting-1819580195',\n",
       "  'headline': 'hillary clinton opens new presidential library charting course of purely theoretical tenure as commander in chief',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/bounty-brawny-ceos-wearing-down-patience-of-mutual-fri-1819570942',\n",
       "  'headline': 'bounty, brawny ceos wearing down patience of mutual friend',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/sherman-alexie-writers-under-trump_us_584998f8e4b08283d6b4ed0c',\n",
       "  'headline': \"sherman alexie says artists under trump will be 'noise-canceling headphones'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/visibly-flu-stricken-choir-kid-really-dragging-down-who-1821530831',\n",
       "  'headline': 'visibly flu-stricken choir kid really dragging down whole christmas pageant',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nbc-ayman-mohyeldin-return-to-gaza_n_5601145.html',\n",
       "  'headline': 'nbc news correspondent ayman mohyeldin returning to gaza',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/tim-robbins-tired-of-being-typecast-as-relatively-tall-1819589144',\n",
       "  'headline': 'tim robbins tired of being typecast as relatively tall characters',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/tom-cotton-waterboarding_us_5823a501e4b0e80b02ceb6e4',\n",
       "  'headline': \"sen. tom cotton thinks 'tough guy' trump is ready to resume waterboarding\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/leather-clad-ted-cruz-greeting-voters-at-reno-area-feti-1819592513',\n",
       "  'headline': 'leather-clad ted cruz greeting voters at reno-area fetish club',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/man-s-family-rises-to-record-high-fourth-priority-1819577274',\n",
       "  'headline': \"man's family rises to record-high fourth priority\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/mike-ditka-oppression_us_59dcd977e4b00377980c03ad',\n",
       "  'headline': 'mike ditka has not been paying attention to history',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/what-mom-would-have-wanted-evolving-over-course-of-fune-1819576934',\n",
       "  'headline': 'what mom would have wanted evolving over course of funeral planning',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/simple-task-of-going-to-post-office-feels-like-weight-o-1819570794',\n",
       "  'headline': 'simple task of going to post office feels like weight of 10,000 boulders',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/report-some-crazy-shit-probably-happened-to-classmate-1819578929',\n",
       "  'headline': 'report: some crazy shit probably happened to classmate being raised by grandmother',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ivanka-trump-judaism-largest-world-religions_us_593e7933e4b0c5a35ca112f4',\n",
       "  'headline': \"ivanka trump incorrectly names judaism as 1 of the 3 'largest world religions'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://pubx.co/VVLx1k',\n",
       "  'headline': 'rescued lion has been obsessed with blankets since he was a baby',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/county-fair-judges-blown-away-by-heifer-1819567995',\n",
       "  'headline': 'county fair judges blown away by heifer',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/best-toys-kid-products-prime-day_us_59553139e4b0da2c7321e754',\n",
       "  'headline': 'the best toys to shop on prime day for kids of all ages',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-sanders-phenomenon_b_10180994.html',\n",
       "  'headline': 'the sanders phenomenon',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/clinton-staff-readies-emp-launch-to-disable-all-nation-1819579399',\n",
       "  'headline': \"clinton staff readies emp launch to disable all nation's electronic devices\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/khloe-kardashian-waist-instagram_n_5259261.html',\n",
       "  'headline': \"khloe kardashian joins 'waist gang'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/gop-debate-utah-canceled_us_56e9966ce4b0b25c9184203e',\n",
       "  'headline': \"next week's republican debate in utah canceled\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-trove-of-lost-and-found_n_5738476.html',\n",
       "  'headline': \"a trove of 'lost and found' photos reveal one mystery couple's beautiful life\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/bear-family-pool-party_us_55d734dee4b08cd3359bb180',\n",
       "  'headline': 'bear family pool party is the cutest backyard invasion ever',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dustin-hoffman-accusers-nbc-interview_us_5a3924b1e4b0fc99878eceb1',\n",
       "  'headline': 'dustin hoffman accusers speak out about alleged abuse in joint nbc interview',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/another-open-letter-to-betsy-devos-from-a-public-school_us_5921a1fee4b07617ae4cbd07',\n",
       "  'headline': 'another open letter to betsy devos from a public school teacher',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/6-tips-to-jump-start-your_b_7727554.html',\n",
       "  'headline': '6 tips to boost your career',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/transphobic-target-man-confrontation_us_5743655ce4b0613b512b044b',\n",
       "  'headline': 'video shows transphobic man preaching in target getting shut down by customer',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/xmas-gifts-from-trump_us_5a28f8a8e4b03ece03001d15',\n",
       "  'headline': '#xmasgiftsfromtrump wish list will give trump a very un-merry christmas',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ronda-rousey-predicted-lose_us_56488171e4b045bf3def7cd5',\n",
       "  'headline': 'ronda rousey eerily predicted how she would lose',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/police-release-haircut-progressed-photo-of-missing-woma-1819577510',\n",
       "  'headline': 'police release haircut-progressed photo of missing woman',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-brief-pun-intended-hist_b_7053048.html',\n",
       "  'headline': 'a brief (pun intended) history of lawyers in movies',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/oklahoma-city-thunder-durant-shot-gifs_n_5188756.html',\n",
       "  'headline': 'gifs: the memphis grizzlies weathered this epic oklahoma city thunder storm',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/stomach-sets-aside-synthetic-additives-until-it-has-a-f-1819578356',\n",
       "  'headline': 'stomach sets aside synthetic additives until it has a few minutes to figure out how to digest them',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/russia-trump_us_58760102e4b092a6cae3fe90',\n",
       "  'headline': 'russia denies it has compromising information on donald trump',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/bible-comics_n_5111481.html',\n",
       "  'headline': 'what comics can offer to bible readers',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://magazine.good.is/features/issue-35-ego-roxane-gay',\n",
       "  'headline': 'breaking uniform',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/congress-approves-of-250-billion-1819567635',\n",
       "  'headline': 'congress approves of $250 billion',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/7-fascinating-but-forgott_b_5697800.html',\n",
       "  'headline': '7 fascinating but forgotten facts from world war i (new book)',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/taylor-swift-now-dating-watertown-boat-1819574857',\n",
       "  'headline': 'taylor swift now dating watertown boat',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/rapture-wreaks-havoc-on-local-book-club-1819568980',\n",
       "  'headline': 'rapture wreaks havoc on local book club',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/setting-goals-steve-harvey_n_6367942.html',\n",
       "  'headline': 'the key to setting achievable goals',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/cyber-monday-guide-2015_us_5654ba94e4b0879a5b0cbc23',\n",
       "  'headline': 'the only shopping guide for cyber monday you need',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-brexit-refugees_us_587bf562e4b09281d0eb80da',\n",
       "  'headline': 'donald trump says refugee crisis and threats to uk identity drove brexit',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/science-fiction-writer-admits-unstoppable-killing-machi-1819569329',\n",
       "  'headline': 'science fiction writer admits unstoppable killing machine based on mother',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/gunman-kills-two-american-advisors-in-kabul-shooting_us_580788fee4b0dd54ce369996',\n",
       "  'headline': 'gunman kills at least two american advisers in kabul shooting',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kris-kobach-private-email_us_59c15bfbe4b0f22c4a8d1dbd',\n",
       "  'headline': 'kris kobach defends using a private email for government business',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/10-year-old-denies-girl-liking-allegations-1819564679',\n",
       "  'headline': '10-year-old denies girl-liking allegations',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/stray-dad-found-in-lumber-section-of-the-home-depot-1819575703',\n",
       "  'headline': 'stray dad found in lumber section of the home depot',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/office-manager-unveils-new-rule-1819579195',\n",
       "  'headline': 'office manager unveils new rule',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/charles-darwin--the-sunmi_b_6162514.html',\n",
       "  'headline': 'charles darwin and the sunmine',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/women-breastfeeding-at-work_us_561d624ee4b0c5a1ce60dba3',\n",
       "  'headline': 'another way companies make it harder for new mothers',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/sheets-changed-after-every-breakup-1819567289',\n",
       "  'headline': 'sheets changed after every breakup',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/walking-meetings-productive_n_5333120.html',\n",
       "  'headline': 'want to make meetings more productive? start walking',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/man-overjoyed-he-no-longer-has-to-purchase-entire-day-s-1819578278',\n",
       "  'headline': \"man overjoyed he no longer has to purchase entire day's worth of egg mcmuffins in morning\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/stone-brewing-evacuates_n_5335412.html',\n",
       "  'headline': 'watch: stone brewing evacuates as wildfire approaches',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttps://www.washingtonpost.com/world/national-security/military-prosecutor-senate-report-on-cia-interrogation-program-is-accurate/2016/02/10/d75d51a8-cf47-11e5-88cd-753e80cd29ad_story.html',\n",
       "  'headline': 'military prosecutor: senate report on cia interrogation program is accurate',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/national-parks-lgbtq_us_57fd5c09e4b0e9c70229af70',\n",
       "  'headline': 'national park service studies historic lgbtq sites for possible recognition',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/safeguarding-americas-health-system-from-sabotage_us_5a1ecf33e4b0e37da0447b66',\n",
       "  'headline': \"safeguarding america's health system from sabotage\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/jonathan-safran-foer-guesses-it-s-time-to-give-up-on-si-1824077900',\n",
       "  'headline': \"jonathan safran foer guesses it's time to give up on silly little dream of becoming good writer\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/luke-aikins-skydive-parachute_us_579d7485e4b08a8e8b5e5a04',\n",
       "  'headline': 'skydiver luke aikins makes jump without a parachute',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/study-universe-actually-shrunk-by-about-19-inches-last-1819589814',\n",
       "  'headline': 'study: universe actually shrunk by about 19 inches last year',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/power-crazed-orkin-man-burns-house-to-ground-1819567429',\n",
       "  'headline': 'power-crazed orkin man burns house to ground',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/couple-s-fucked-up-presex-ritual-involves-tucking-both-1819580337',\n",
       "  'headline': \"couple's fucked-up presex ritual involves tucking both kids into bed\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/will-all-senate-republicans-kowtow-to-trump-and-the_us_58e50789e4b09dbd42f3dc4a',\n",
       "  'headline': 'will all senate republicans kowtow to trump and the far right?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/deputy-brings-homeless-man-he-saw-on-side-of-highway-to-mcdonalds-so-hed-be-warm_us_568d5698e4b0a2b6fb6e4265',\n",
       "  'headline': 'viral photo captures incredible moment between police officer, homeless man',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/google-employees-disappointed-15th-anniversary-party-on-1819575642',\n",
       "  'headline': 'google employees disappointed 15th anniversary party only has one solar-powered lego drag race reffed by david pogue',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/study-average-american-tries-getting-out-of-10-000-thi-1819576586',\n",
       "  'headline': 'study: average american tries getting out of 10,000 things each year',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/rnc-builds-levee-out-of-poor-people-to-protect-conventi-1819573810',\n",
       "  'headline': 'rnc builds levee out of poor people to protect convention site',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/romney-delivers-stern-warning-to-china-speaking-direct-1819574101',\n",
       "  'headline': 'romney delivers stern warning to china, speaking directly into the camera in fluent mandarin',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/congress-bailout-great-recession_us_599343e2e4b009141640806f',\n",
       "  'headline': 'the inside story of how congress sent the stock market tumbling',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/voters-shocked-christie-botched-such-an-easy-political-1819575996',\n",
       "  'headline': 'voters shocked christie botched such an easy political cover-up',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/desperate-dole-promises-best-prom-ever-1819564084',\n",
       "  'headline': 'desperate dole promises best prom ever',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-different-kind-of-mom_us_57b8bf69e4b007f18198889d',\n",
       "  'headline': 'a different kind of mom',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/uber-halt-self-driving-cars-california_us_5abaa6ede4b03e2a5c7704ad',\n",
       "  'headline': \"uber halts self-driving car tests in california, where it didn't test much anyway\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/from-tiger-mothers-to-fre_b_6773744.html',\n",
       "  'headline': \"from tiger mothers to fresh off the boat: eddie huang's mom is not every asian-american mom\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-trump-goodell_us_58934586e4b0af07cb6bbebc',\n",
       "  'headline': \"donald trump thinks roger goodell is 'weak,' 'stupid' and a 'dope'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-congress-libel-wall-street-journal_us_5a57d91ae4b0720dc4c58976',\n",
       "  'headline': \"trump says congress won't change libel laws, but that's a decision for the states\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/flag-underwear-world-series_us_59f75619e4b0c0c8e67b2c7f',\n",
       "  'headline': 'shirtless goofball in flag underwear invades field at world series',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/local-moviegoer-enjoying-movie-so-far-1819564030',\n",
       "  'headline': 'local moviegoer enjoying movie so far',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/why-chinese-parents-dont-say-i-love-you_us_584b5739e4b01713310510a9',\n",
       "  'headline': \"why chinese parents don't say 'i love you'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/americans-think-white-house-creating-problems_us_598a217ce4b0449ed5062d2f',\n",
       "  'headline': 'americans say the white house is creating more problems than it solves',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/theater-major-has-too-long-borne-shakespeare-teachers-b-1819568815',\n",
       "  'headline': \"theater major has too long borne shakespeare teacher's blunt upbraidings, bitter scoffs\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-lancet-breakthrough-pub_b_7777494.html',\n",
       "  'headline': 'a lancet breakthrough: publishing about faith and health',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/area-grasshopper-kind-of-a-thorax-man-himself-1819566187',\n",
       "  'headline': 'area grasshopper kind of a thorax man himself',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/freemasons-return-to-jupiter-1819586200',\n",
       "  'headline': 'freemasons return to jupiter',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/banjo-wielding-matt-damon-makes-last-minute-bid-for-bes-1823505244',\n",
       "  'headline': 'banjo-wielding matt damon makes last-minute bid for best original song',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/after-criticism-cleveland-officials-to-outline-convention-security-plans_us_574ae78ee4b03ede441510d3',\n",
       "  'headline': 'after criticism, cleveland officials to outline convention security plans',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/new-teen-trend-walking-wet-and-nude-couldn-t-have-cau-1819591534',\n",
       "  'headline': \"new teen trend 'walking wet and nude' couldn't have caught on at worse time\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/justin-bieber-scold-spanish_us_5638b5cbe4b079a43c047be0',\n",
       "  'headline': 'justin bieber interrupts performance to scold spanish audience',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/mudbound-netflix-oscar-nominations_us_5a67440fe4b0e5630073a15d',\n",
       "  'headline': \"'mudbound' oscar nominations place netflix in big leagues\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/area-man-uses-wtc-attack-as-excuse-to-call-ex-girlfrien-1819566193',\n",
       "  'headline': 'area man uses wtc attack as excuse to call ex-girlfriend',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/mccain-gets-hammered-at-local-vfw-1819570338',\n",
       "  'headline': 'mccain gets hammered at local vfw',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/12-year-old-s-christmas-list-demonstrates-heartbreaking-1819577264',\n",
       "  'headline': \"12-year-old's christmas list demonstrates heartbreaking awareness of family's financial predicament\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/grammy-nominations-2016_us_56658a7ee4b079b2818f190b',\n",
       "  'headline': 'kendrick lamar, taylor swift and the weeknd lead the 2016 grammy nominations',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/slight-inconvenience-avoided-1819566029',\n",
       "  'headline': 'slight inconvenience avoided',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/frustrated-fcc-unable-to-stop-use-of-word-friggin-1819567102',\n",
       "  'headline': \"frustrated fcc unable to stop use of word 'friggin''\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nbc-law-and-order-true-crime-menendez-brothers-series_us_57893e39e4b0867123e14152',\n",
       "  'headline': \"nbc to dramatize menendez brothers murders in 'law & order: true crime' spinoff\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/berlin-christmas-market-attack_us_5858cecbe4b0b3ddfd8e82c5',\n",
       "  'headline': 'police assume truck was deliberately driven into berlin christmas market',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/area-man-cleans-apartment-once-every-relationship-1819576384',\n",
       "  'headline': 'area man cleans apartment once every relationship',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/orlando-shooting-food-donations_us_57604e3de4b0e4fe5143eb6f',\n",
       "  'headline': 'after shooting, orlando chefs provide thousands of free meals',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jeff-flake-republicans-lead_us_5aab28aae4b0c33361af33a6',\n",
       "  'headline': \"gop sen. jeff flake comes out and says it: 'my party might not deserve to lead'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dentist-buys-back-halloween-candy_us_56376154e4b00aa54a4ea562',\n",
       "  'headline': 'dentist offers to buy back halloween candy',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/mta-officials-assure-new-yorkers-that-today-s-subway-wi-1821190648',\n",
       "  'headline': \"mta officials assure new yorkers that today's subway will run just as fucked up as normal\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/routine-drunk-driving-trip-turns-tragic-for-five-local-1819586615',\n",
       "  'headline': 'routine drunk-driving trip turns tragic for five local teens',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/grizzly-bear-sprained-paw-while-mauling-hunter-reports-1819572712',\n",
       "  'headline': 'grizzly bear sprained paw while mauling hunter, reports ranger',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/trump-selects-longtime-personal-plane-to-head-faa-1823360726',\n",
       "  'headline': 'trump selects longtime personal plane to head faa',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/new-antidepressant-makes-friends-problems-seem-worse-1819575964',\n",
       "  'headline': \"new antidepressant makes friends' problems seem worse\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/brandon-chrostowski-edwins-restaurant-teaches-ex-offenders-how-to-cook_us_56fd7019e4b0daf53aef2350',\n",
       "  'headline': 'restaurant teaches former inmates to cook, helps them get back on their feet',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/5-entrepreneurial-rules-t_b_7780328.html',\n",
       "  'headline': '5 entrepreneurial rules to live by',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/laser-pointer-aimed-toward-space-in-1997-finally-annoyi-1819571377',\n",
       "  'headline': 'laser pointer aimed toward space in 1997 finally annoying planet 13 light-years away',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/man-who-skipped-airport-s-moving-walkway-immediately-re-1819579777',\n",
       "  'headline': \"man who skipped airport's moving walkway immediately realizes what an arrogant fool he's been\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/7-things-you-probably-didnt-know-about-christmas_us_5851d850e4b016e9c118828a',\n",
       "  'headline': \"7 things you probably didn't know about christmas\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/2016-man-booker-prize_us_581023ffe4b001e247df4321',\n",
       "  'headline': 'paul beatty becomes first american to win man booker prize for fiction',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/your-favorite-female-star-wars-heroes-finally-get-their-own-series_us_58ef68eae4b0b9e98489b03a',\n",
       "  'headline': \"your favorite female 'star wars' heroes finally get their own series\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/nation-checks-out-cnn-com-to-see-what-their-old-pals-th-1819574972',\n",
       "  'headline': 'nation checks out cnn.com to see what their old pals the tsarnaevs and castros are up to',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/small-town-beginning-to-wonder-what-taking-heroin-epide-1819579260',\n",
       "  'headline': 'small town beginning to wonder what taking heroin epidemic so long to get there',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/new-ted-cruz-superpacs-ta_n_7024478.html',\n",
       "  'headline': 'new ted cruz super-pacs take in record haul',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/domestic-abuse-olliette-murry-drobot_us_59a5bd08e4b063ae34d96f61',\n",
       "  'headline': 'domestic abuse survivor gives young victims the support she wishes she had',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dogs-smell-grandmas-scent-set-off-on-quest-to-find-her_us_584c2eede4b0bd9c3dfd1730',\n",
       "  'headline': \"dogs smell grandma's scent, set off on quest to find her\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/smbs-are-changing-the-way_b_6582754.html',\n",
       "  'headline': \"smb's are changing the way they do business\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/president-s-american-manufacturing-council-down-to-ceo-1819580159',\n",
       "  'headline': \"president's american manufacturing council down to ceo of shoe carnival\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/worlds-physicists-complete-study-of-physics-1819571253',\n",
       "  'headline': \"world's physicists complete study of physics\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-to-really-listen-in-a_b_8126742.html',\n",
       "  'headline': 'how to really listen in a difficult conversation (6.2)',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/heroic-turtle-dials-most-of-911-1819587282',\n",
       "  'headline': 'heroic turtle dials most of 911',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/miscarriage-stories-what-its-like_us_589e21cee4b094a129eb076a',\n",
       "  'headline': \"what it's really like to have a miscarriage\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/texas-anti-lgbt-bills_us_584873d3e4b0f9723cfff3be',\n",
       "  'headline': 'businesses say anti-lgbt bills could cost texas billions',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/man-accused-of-new-hampsh_n_5462404.html',\n",
       "  'headline': \"accused killer wanted 'army of people who'd do anything he asked'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://sports.theonion.com/avid-fisherman-forever-ruins-fishing-for-son-1819567026',\n",
       "  'headline': 'avid fisherman forever ruins fishing for son',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/british-public-on-the-hunt-for-witches-marks-this-halloween_us_58179b1ae4b0390e69d1e098',\n",
       "  'headline': \"british public on the hunt for 'witches' marks' this halloween\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/nation-relieved-insufferable-little-game-of-thrones-f-1819578815',\n",
       "  'headline': \"nation relieved insufferable little 'game of thrones' fans don't have book to lord over them this season\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-best-volunteer-progra_b_7566478.html',\n",
       "  'headline': 'the best volunteer programs do this',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/nasa-completely-forgot-probe-was-returning-today-1819568305',\n",
       "  'headline': 'nasa completely forgot probe was returning today',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/3m-introduces-new-line-of-protective-foam-eye-plugs-1822590036',\n",
       "  'headline': '3m introduces new line of protective foam eye plugs',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/michael-b-jordan-sets-fire-to-first-fahrenheit-451-trailer_us_5a95795ce4b01f65f59aa9aa',\n",
       "  'headline': \"michael b. jordan sets fire to first 'fahrenheit 451' trailer\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/astronomers-predict-giant-asteroid-will-hit-nations-the-1819564716',\n",
       "  'headline': \"astronomers predict giant asteroid will hit nation's theaters this summer\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/hero-of-the-common-man-talks-to-plumber-for-entire-time-1819577072',\n",
       "  'headline': \"hero of the common man talks to plumber for entire time he's in house\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.huffingtonpost.in/2016/08/15/social-unity-is-most-important-says-pm-modi-on-indias-70th-ind/?utm_hp_ref=in-homepage',\n",
       "  'headline': \"social unity is most important, says pm modi on india's 70th independence day\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/reagan-opinion-roe-wade-harassment_us_5a63d88ee4b0dc592a096aa1',\n",
       "  'headline': \"roe made abortions legal, but it doesn't keep women and providers safe\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/hussein-judge-hoping-for-fair-speedy-assassination-1819568536',\n",
       "  'headline': 'hussein judge hoping for fair, speedy assassination',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/savvy-man-registers-sleepy-romney-twitter-account-just-1819574064',\n",
       "  'headline': \"savvy man registers 'sleepy romney' twitter account just in case candidate looks tired\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.politico.com/story/2016/05/immigration-trump-deportations-dreamers-223658',\n",
       "  'headline': \"dreamers face nightmare of trump's deportation force\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/third-world-disease-eliminated-with-hot-air-hand-dryers-1819565375',\n",
       "  'headline': 'third world disease eliminated with hot-air hand dryers',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/yayoi-kusama-selfie-snapper-smashes-sculpture_us_58b6975de4b0780bac2e9046',\n",
       "  'headline': 'whoops! selfie snapper smashes sculpture days after exhibit opens',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/aspiring-actor-dreams-of-one-day-publicly-voicing-regre-1822199182',\n",
       "  'headline': 'aspiring actor dreams of one day publicly voicing regret for working with woody allen',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/swiss-unable-to-maintain-neutrality-toward-delicious-pa-1819564621',\n",
       "  'headline': 'swiss unable to maintain neutrality toward delicious pastries',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/9-things-all-parents-of-college-kids-do-but-hate-to-admit_us_56cc6f94e4b041136f1845ca',\n",
       "  'headline': '9 things all parents of college kids do but hate to admit',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/thousands-of-new-orleans-households-still-without-polit-1819568190',\n",
       "  'headline': 'thousands of new orleans households still without political power',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/reuniting-families-with-remains_b_5849866.html',\n",
       "  'headline': 'watch: underreported story -- reuniting families with remains of dead migrants',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/yahoos-newfront-pulses-to_b_7195252.html',\n",
       "  'headline': \"yahoo's newfront pulses to steve aoki's edm beat\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/look-at-it-its-goddamn-beautiful-1819590339',\n",
       "  'headline': \"look at it: it's goddamn beautiful\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-to-lose-to-the-islami_b_6244574.html',\n",
       "  'headline': 'how to lose to the islamic state: obama administration considers deploying troops to iraq, focusing on assad in syria',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/erics-bogosians-operation_b_7097268.html',\n",
       "  'headline': \"eric's bogosian's operation nemesis: can a genocide ever truly be avenged?\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/antarctic-observational-comic-running-out-of-ideas-1819566013',\n",
       "  'headline': 'antarctic observational comic running out of ideas',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/moscow-protests_us_59189721e4b0fe039b355e07',\n",
       "  'headline': 'thousands protest in moscow against housing plan',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/new-pixar-employees-required-to-watch-adorable-sexual-h-1819571585',\n",
       "  'headline': 'new pixar employees required to watch adorable sexual harassment video',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/cosmopolitan-releases-40-year-compendium-812-683-ways-1819587929',\n",
       "  'headline': 'cosmopolitan releases 40-year compendium: 812,683 ways to please your man',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/row-of-asterisks-spices-up-otherwise-ordinary-e-mail-1819571919',\n",
       "  'headline': 'row of asterisks spices up otherwise ordinary e-mail',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/queer-tattoo-shop_us_58c95a73e4b01c029d78044f',\n",
       "  'headline': 'this tattoo shop is creating a safe and accepting space for queer bodies',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/sears-gold-card-holder-pushing-weight-around-area-sears-1819569396',\n",
       "  'headline': 'sears gold card holder pushing weight around area sears',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jeff-lindsay-goodbye-dexter_us_55a52cc5e4b0a47ac15d60cf',\n",
       "  'headline': 'author jeff lindsay says goodbye to serial killer dexter with final novel',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/after-work-drinks-enter-third-excruciating-minute-1819573762',\n",
       "  'headline': 'after-work drinks enter third excruciating minute',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://thehill.com/homenews/campaign/267641-koch-network-spent-nearly-400-million-in-2015',\n",
       "  'headline': 'koch network spent nearly $400 million in 2015',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/university-of-delaware-noose_us_5602a0dde4b00310edf93a76',\n",
       "  'headline': \"those weren't nooses at university of delaware\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/man-already-has-whole-sentence-lined-up-for-later-in-co-1819580389',\n",
       "  'headline': 'man already has whole sentence lined up for later in conversation',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/area-waitress-has-one-hell-of-an-ass-on-her-local-man-1819564893',\n",
       "  'headline': 'area waitress has one hell of an ass on her, local man will tell you that right now',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/gifts-for-plant-lovers_us_5aff32b4e4b0463cdba1c5af',\n",
       "  'headline': '30 unbeleafably adorable gifts for plant lovers',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/cuomo-afghanistan_n_5893280.html',\n",
       "  'headline': 'cuomo makes surprise afghanistan trip',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/drummers-girlfriend-thinks-he-should-sing-1819566660',\n",
       "  'headline': \"drummer's girlfriend thinks he should sing\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/matthew-mcconaughey-youtube-channel_us_57b8bd43e4b03d513688ca99',\n",
       "  'headline': \"nobody watched matthew mcconaughey's forgotten youtube channel until now\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/pope-francis-admits-god-really-starting-to-look-old-1819879536',\n",
       "  'headline': 'pope francis admits god really starting to look old',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/getting-lost-in-the-paren_b_6181148.html',\n",
       "  'headline': 'what has becoming a parent done to me?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/video-congressmans-icantb_b_6272128.html',\n",
       "  'headline': 'video:  #icantbreathe poem on house floor',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/man-somehow-overcomes-alcoholism-without-jesus-1819572870',\n",
       "  'headline': 'man somehow overcomes alcoholism without jesus',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-w-bush_us_59ee180ce4b0a484d064d7b7',\n",
       "  'headline': 'donald w. bush?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/mom-uses-full-name-to-refer-to-bisquick-impossibly-easy-1819566208',\n",
       "  'headline': 'mom uses full name to refer to bisquick impossibly easy cheeseburger pie\\x99',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-trump-filemon-vela_us_5a1acb05e4b0cee6c0504421',\n",
       "  'headline': \"congressman calls trump 'an idiot' for using egypt mosque attack to promote border wall\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/luke-bryan-confederate-flag-raising-sisters-kids_us_55b77596e4b0a13f9d1a0ed0',\n",
       "  'headline': \"luke bryan says confederate flag has become a 'symbol of racism'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/rich-first-grader-buys-whole-sheet-of-gold-stars-1819566777',\n",
       "  'headline': 'rich first-grader buys whole sheet of gold stars',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/we-aint-germans_b_5666629.html',\n",
       "  'headline': \"we ain't germans\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/matt-bevin-kentucky-confederate-statues_us_59946af7e4b04b193362462e',\n",
       "  'headline': \"kentucky governor echoes trump: 'all sides' to blame for charlottesville violence\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-caitlyn-jenner-is-helping-me-be-a-better-me_b_7537712.html',\n",
       "  'headline': 'how caitlyn jenner is helping me be a better me',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/new-apple-campaign-urges-consumers-to-buy-iphone-for-ot-1819573673',\n",
       "  'headline': 'new apple campaign urges consumers to buy iphone for other hand',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/woman-dozing-at-coffee-shop-has-that-dave-eggers-sex-dr-1819567737',\n",
       "  'headline': 'woman dozing at coffee shop has that dave eggers sex dream again',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/manager-of-combination-taco-bell-kfc-secretly-considers-1825290602',\n",
       "  'headline': 'manager of combination taco bell/kfc secretly considers it mostly a taco bell',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/could-hillary-clinton-have-what-it-takes-to-defeat-the-1819587781',\n",
       "  'headline': 'could hillary clinton have what it takes to defeat the democrats in 2008?',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/-wild-tales-outstanding-b_b_5389941.html',\n",
       "  'headline': 'wild tales:  outstanding black comedy at cannes',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-trump-cte-kevin-blackistone_us_59c98009e4b06ddf45fa8f7f',\n",
       "  'headline': 'donald trump prefers violent football so more black players get hurt: espn analyst',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/black-ribbon-in-the-balsa_b_6315304.html',\n",
       "  'headline': 'black ribbon in the balsam',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/buttermilk-biscuit-recipes_n_7274192.html',\n",
       "  'headline': 'the buttermilk biscuit recipes you want and need',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ai-weiwei-commemorates-drowned-refugees-with-public-installation-during-berlin-film-festival_us_56c20c0de4b08ffac125f029',\n",
       "  'headline': 'ai weiwei commemorates drowned refugees during berlin film festival',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://thinkprogress.org/media/2015/08/03/3687249/koch-freedom-partners-media-restrictions/',\n",
       "  'headline': \"the restrictions journalists agreed to in order to attend the koch brothers' conference\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/7-year-old-puts-on-uno-face-1819567628',\n",
       "  'headline': '7-year-old puts on uno face',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/little-boy-stands-tall_b_7144538.html',\n",
       "  'headline': \"'little boy' stands tall\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jake-tapper-tweets-response-snl-kellyanne-conway-sketch_us_58a1d246e4b03df370d886c5',\n",
       "  'headline': \"jake tapper has one-word response to creepy kellyanne conway 'snl' sketch\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/hobby-lobby-poll_n_5540769.html',\n",
       "  'headline': 'poll shows support for birth control mandate on eve of court ruling',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/steven-hayes-kosher_n_6240420.html',\n",
       "  'headline': 'death row inmate loses fight over kosher food',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jada-pinkett-smith-gabrielle-union-feud_us_5aec363be4b041fd2d257f8d',\n",
       "  'headline': 'jada pinkett smith and gabrielle union end feud after 17 years',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/huffpost-rise-morning-newsbrief-feb-10_us_56bac83ce4b0c3c5504f6cba',\n",
       "  'headline': 'huffpost rise: what you need to know on february 10',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dear-mr-president-a-dispatch-from-bowling-green_us_58955fd7e4b02bbb1816ba90',\n",
       "  'headline': 'dear mr. president: a dispatch from bowling green',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/shinto-priestess-killed-by-brother-during-sword-attack-at-tokyo-shrine_us_5a2ac2cce4b073789f695096',\n",
       "  'headline': 'shinto priestess killed by brother during sword attack at tokyo shrine',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/journalists-rebuke-of-michelle-wolf-doesnt-celebrate-press-freedom_us_5ae70b2fe4b02baed1bc4dd8',\n",
       "  'headline': \"journalists push back on correspondents' association's response to michelle wolf\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nina-dobrev-return-to-vampire-diaries_us_587e7cf6e4b01cdc64c82984',\n",
       "  'headline': \"nina dobrev addresses her rumored return to 'vampire diaries'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/british-pm-buddies-up-to-trump_us_58664f64e4b0d9a5945af5e8',\n",
       "  'headline': 'theresa may, edging towards donald trump, scolds john kerry over israel',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/child-assured-it-will-be-long-time-before-he-dies-1819574566',\n",
       "  'headline': 'child assured it will be long time before he dies',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/prank-leaves-girl-glued_us_568a8627e4b014efe0dae81f',\n",
       "  'headline': \"new year's eve prank leaves 4-year-old glued to mcdonald's toilet\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/tamir-rice-family-slams-prosecutors_us_5670572ce4b0e292150f67dc',\n",
       "  'headline': 'prosecutors in tamir rice case bizarrely pointed toy gun at witness, lawyers allege',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/obama-isis-iran_n_6165352.html',\n",
       "  'headline': \"what's wrong with this picture? for u.s. fight against isis, everything\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/afi-docs-where-policy-mee_b_7564840.html',\n",
       "  'headline': 'afi docs: where policy meets art',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/school-bully-not-so-tough-since-being-molested-1819587116',\n",
       "  'headline': 'school bully not so tough since being molested',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-real-mothers-of-mothe_b_5297124.html',\n",
       "  'headline': \"the real mothers of mother's day\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dad-shoots-daughter-teaching-gun-safety_us_55bfac35e4b0d4f33a0384a5',\n",
       "  'headline': 'dad shoots daughter while teaching her about gun safety',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/3-unexpected-tips-on-taming-screen-time-in-your-household_us_559f2311e4b01c2162a63ff7',\n",
       "  'headline': '3 unexpected ways to help your kids be mindful about screen time',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dee-bogetti-gps-guide_us_56e729c7e4b0b25c9182fd9f',\n",
       "  'headline': \"dee bogetti's gps guide for living in the moment\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/paul-ryan-confident-american-people-will-warm-up-to-tax-1821509050',\n",
       "  'headline': 'paul ryan confident american people will warm up to tax plan once they realize life a cruel and meaningless farce',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/war-torn-blood-soaked-kosovo-would-bombing-it-help-1819586587',\n",
       "  'headline': 'war-torn, blood-soaked kosovo: would bombing it help?',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/starving-for-a-fantasy_b_6920540.html',\n",
       "  'headline': 'starving for a fantasy',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/bill-clinton-waiting-until-after-primaries-to-endorse-c-1819588484',\n",
       "  'headline': 'bill clinton waiting until after primaries to endorse candidate',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/what-to-do-own-volkswagen-recalled_us_55fdcf38e4b00310edf754c4',\n",
       "  'headline': 'what you should do if you own a volkswagen that was just recalled',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/to-read-or-not-to-read-pa_b_6174326.html',\n",
       "  'headline': 'to read or not to read, part 2',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/bernie-sanders-hillary-clinton-iowa_us_55f1798ae4b093be51bdb35c',\n",
       "  'headline': 'hillary clinton loses lead over bernie sanders in new iowa poll',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/don-hazen-accused-sexual-harassment_us_5a3ad9d1e4b06d1621b192d8',\n",
       "  'headline': 'several women accuse progressive media executive don hazen of sexual harassment',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/success-and-still-enjoying-your-happy-place_b_5579043.html',\n",
       "  'headline': \"success and still enjoying your 'happy place'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/last-12-years-a-real-wake-up-call-for-area-man-1819575039',\n",
       "  'headline': 'last 12 years a real wake-up call for area man',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/pollsters-admit-they-underestimated-voters-adrenal-gla-1819579435',\n",
       "  'headline': \"pollsters admit they underestimated voters' adrenal glands\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/lin-manuel-miranda-ellen-degeneres_us_5825ec17e4b0c4b63b0c4840',\n",
       "  'headline': \"lin-manuel miranda freestyles about life's most annoying minor inconveniences on 'ellen'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/john-lewis-donald-trump-civil-rights-museum_us_5a29a5fbe4b0a290f04f2f70',\n",
       "  'headline': \"john lewis won't attend civil rights museum opening because trump is going\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/is-trouble-brewing-for-the-2015-npt-review-conference_b_7106216.html',\n",
       "  'headline': 'is trouble brewing for the 2015 npt review conference?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/area-man-going-to-go-ahead-and-consider-that-a-date-1819568761',\n",
       "  'headline': 'area man going to go ahead and consider that a date',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/4-senators-mauled-during-congressional-tiger-show-1819576294',\n",
       "  'headline': '4 senators mauled during congressional tiger show',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/huffpost-hill_n_6858772.html',\n",
       "  'headline': 'huffpost hill - secret service agents really glad dark sunglasses hide bloodshot eyes',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/kite-flyer-in-the-zone-1819587609',\n",
       "  'headline': 'kite flyer in the zone',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/bacon-just-one-of-sprints-new-downloadable-ring-scents-1819567731',\n",
       "  'headline': \"bacon just one of sprint's new downloadable ring scents\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/i-photograph-to-remember_b_6281740.html',\n",
       "  'headline': 'i photograph to remember',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/steven-holcomb-tribute_us_5a7d56a2e4b0c6726e11db7d',\n",
       "  'headline': 'u.s. bobsled team pays tribute to late gold medalist steven holcomb',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/with-history-art-culture-_b_6344880.html',\n",
       "  'headline': \"among santa fe's many virtues? history, art, culture, hospitality and killer vintage clothing\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/power-plan-foes-from-mars_b_12068010.html',\n",
       "  'headline': 'power plan foes from mars, backers from venus (earth actually)',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/journalists-who-refuse-to_b_10661042.html',\n",
       "  'headline': 'journalists who refuse to take the same non-answer for an answer',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nukes-and-the-global-schism_us_5967d173e4b022bb9372b023',\n",
       "  'headline': 'nukes and the global schism',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/weird-relative-at-family-reunion-knows-how-everyone-rel-1819579336',\n",
       "  'headline': 'weird relative at family reunion knows how everyone related to each other',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/negro-week-at-the-19391940-new-york-worlds-fair_us_58b26313e4b02f3f81e44893',\n",
       "  'headline': \"negro week at the 1939–1940 new york world's fair\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-threatens-veto-of-spending-bill-over-border-wall-funding_us_5ab4f979e4b008c9e5f676aa',\n",
       "  'headline': 'trump threatens to veto spending bill over border wall funding, then signs it',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/6-diy-stress-hacks-using-whats-in-your-closet_b_7525764.html',\n",
       "  'headline': \"6 diy stress hacks using what's in your closet\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-major-concern-with-the-phone-call-with-taiwan_us_5844423fe4b0b93e10f8e31a',\n",
       "  'headline': 'the major concern with the phone call with taiwan',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-weird-and-wonderful-cab_b_6957164.html',\n",
       "  'headline': \"a weird and wonderful cabaret chronicle: karen mason revisits her roots at 'don't tell mama!'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/companies-urged-to-report-climate-change-risks_us_56fe7911e4b0a06d58057057',\n",
       "  'headline': \"why companies shouldn't hide the financial risks of climate change\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/louvre-rats_n_5631598.html',\n",
       "  'headline': 'the louvre gardens are teeming with rats',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-no-1-polls-macarena_us_55b25af5e4b0224d8831f6ce',\n",
       "  'headline': \"trump is #1 in the polls, and so was the 'macarena'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/boy-george-saves-a-child-from-suicide-among-other_us_58334d6fe4b08c963e344339',\n",
       "  'headline': 'boy george opens up about happiness, being a u.s. politics junkie and more',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/area-woman-decides-not-to-post-facebook-status-that-wou-1819574365',\n",
       "  'headline': 'area woman decides not to post facebook status that would have tipped gun control debate',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/sprint-t-mobile-ceos-merge-into-grotesque-executive-hy-1825660392',\n",
       "  'headline': 'sprint, t-mobile ceos merge into grotesque executive hybrid',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/patrick-stewart-1-star-reviews_us_58afe43fe4b060480e069d26',\n",
       "  'headline': 'patrick stewart reads hilariously bad reviews of iconic tourist attractions',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/safeguarding-the-well-bei_b_7509452.html',\n",
       "  'headline': 'safeguarding the well-being of children',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/anger-gun-violence_us_5a8b18f5e4b05c2bcace143f',\n",
       "  'headline': 'the public health threat of private anger',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/target-dorm-room-essentials-aisle-being-browsed-exclu-1819580224',\n",
       "  'headline': \"target 'dorm room essentials' aisle being browsed exclusively by 30-year-old men with studio apartments\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/suzanne-somers-named-u-s-thighmaster-general-1819586302',\n",
       "  'headline': 'suzanne somers named u.s. thighmaster general',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/oscar-isaac-hamlet_us_595ff12ce4b0615b9e9194e3',\n",
       "  'headline': 'just a friendly and wildly hot reminder that oscar isaac is playing hamlet',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/terror-at-the-mall-kenya-documentary_n_5810932.html',\n",
       "  'headline': \"a new hbo documentary shows what it's really like inside a terror attack\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/watch-stories-you-wont-be_b_5558211.html',\n",
       "  'headline': \"watch: stories you won't believe from some of the world's dirtiest jobs\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/danny-willett-wins-masters_us_570adde0e4b0142232495057',\n",
       "  'headline': 'golf sensation jordan spieth loses masters after horrible meltdown; danny willett wins',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/first-gay-couple-receives-marriage-license-in-jailed-kentucky-clerks-county_us_55e98be0e4b093be51bb2349',\n",
       "  'headline': \"first gay couple receives marriage license at jailed kentucky clerk's office\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/worlds-jews-celebrate-christmas-with-ceremonial-re-murd-1819565421',\n",
       "  'headline': \"world's jews celebrate christmas with ceremonial re-murdering of christ\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/john-mccain-ebola-czar_n_5972988.html',\n",
       "  'headline': 'john mccain, czar hater, calls for ebola czar',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/father-of-trump-tower-climber-also-has-an-important-message-for-you_us_57ae374de4b007c36e4edb09',\n",
       "  'headline': 'father of trump tower climber also has an important message for you',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/that-guy-from-that-one-show-in-rehab-1819567262',\n",
       "  'headline': 'that guy from that one show in rehab',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.politico.com/story/2016/02/bernie-sanders-bill-clinton-welfare-reform-trade-219470',\n",
       "  'headline': 'sanders hits bill clinton on welfare reform, trade',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/white-nationalist-calls-presidents-denouncement-of-hate-groups-kumbaya-nonsense_us_59923778e4b09096429961e8',\n",
       "  'headline': \"white nationalist calls trump's denouncement of hate groups 'kumbaya nonsense'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/old-milwaukee-book-of-world-records-confirms-title-for-1819570578',\n",
       "  'headline': \"'old milwaukee book of world records' confirms title for most punches to shoulder\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/london-terror-attack-suspects_us_59337ebfe4b0c242ca24ba43',\n",
       "  'headline': 'new details paint unsettling pictures of london attackers',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/breast-cancer-dont-ask-us-to-get-over-it_us_57f18712e4b07f20daa10e51',\n",
       "  'headline': \"don't ask me to 'get over' my history with breast cancer\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/facebook-passive-use_us_5a34517ae4b040881beaae39',\n",
       "  'headline': \"being a facebook wallflower isn't good for you, the social site says\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/woody-harrelson-spends-two-hours-drawing-marijuana-leaf-1819586711',\n",
       "  'headline': 'woody harrelson spends two hours drawing marijuana leaf on binder',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/burger-king-franchise-owner-adds-sad-little-personal-to-1819577359',\n",
       "  'headline': 'burger king franchise owner adds sad little personal touches to restaurant',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/verb-to-follow-noun-prepositional-phrase-to-follow-1819564061',\n",
       "  'headline': 'verb to follow noun; prepositional phrase to follow',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/a-friendship-valentine_us_58a0c6efe4b0cd37efcfea0a',\n",
       "  'headline': \"a valentine for my best friend: my life wouldn't be the same without you\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-importance-of-being-c_1_b_5244652.html',\n",
       "  'headline': 'the importance of being collaborative',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/russian-officials-scrambling-as-plan-to-delegitimize-we-1819579674',\n",
       "  'headline': 'russian officials scrambling as plan to delegitimize western democracy moving way faster than intended',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/obama-sort-of-freaked-out-after-not-receiving-single-e-1819572760',\n",
       "  'headline': 'obama sort of freaked out after not receiving single e-mail, phone call for entire day',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/adele-tweets-apology-after-stage-rigging-hits-glasgow-concertgoer_us_56f6d02be4b0a372181a210a',\n",
       "  'headline': 'adele tweets apology after stage rigging hits glasgow concertgoer',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/pastor-talking-to-non-christian-who-just-lost-wife-can-1819579838',\n",
       "  'headline': 'pastor talking to non-christian who just lost wife can smell blood',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/australia-same-sex-marriage-religious-freedom_us_5a0c8e7fe4b0bc648a0f9c6f',\n",
       "  'headline': \"'religious freedom' clauses are point of contention as australia crafts marriage equality laws\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/voters-mixed-feelings-hillary-clinton-donald-trump_us_5730f7ebe4b096e9f0925621',\n",
       "  'headline': 'voters are excited for november despite not really loving the likely nominees',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/everyone-in-family-compliments-grandmother-on-how-small-1819577242',\n",
       "  'headline': \"everyone in family compliments grandmother on how small and feeble she's gotten\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/eating-entire-box-of-donuts-not-originally-part-of-even-1819566706',\n",
       "  'headline': \"eating entire box of donuts not originally part of evening's plan\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/white-house-pretty-sure-uzbekistan-diplomat-stole-a-bun-1819566776',\n",
       "  'headline': 'white house pretty sure uzbekistan diplomat stole a bunch of soap',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/jared-kushner-quietly-transfers-solve-middle-east-cris-1819579797',\n",
       "  'headline': \"jared kushner quietly transfers 'solve middle east crisis' to next week's to-do list\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/15-years-in-environment-of-constant-fear-somehow-fails-1819576202',\n",
       "  'headline': '15 years in environment of constant fear somehow fails to rehabilitate prisoner',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/michael-buckley-sex-tips-play_us_559da0bde4b01c2162a5d17b',\n",
       "  'headline': \"internet personality michael buckley on giving 'sex tips' off broadway\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/legends-of-new-yorks-latex-ball-celebrate-history-of-ballroom-and-voguing_us_55b296a6e4b0a13f9d188c6b',\n",
       "  'headline': \"legends of new york's latex ball celebrate the history of voguing\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/man-unfortunately-sleeps-like-baby-1819573039',\n",
       "  'headline': 'man unfortunately sleeps like baby',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/one-last-ruben-studdard-reference-wafts-gently-into-the-1819569454',\n",
       "  'headline': 'one last ruben studdard reference wafts gently into the cool evening air',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/hubble-telescope-discovers-giant-amelia-earhart-statue-1819592315',\n",
       "  'headline': 'hubble telescope discovers giant amelia earhart statue on distant planet',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-is-telling-jokes-but-nobodys-laughing_us_5983d162e4b0f2c7d93f5499',\n",
       "  'headline': 'trump is telling \"jokes,\" but nobody\\'s laughing',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/hundreds-of-miniature-sean-hannitys-burst-from-roger-ai-1819579957',\n",
       "  'headline': \"hundreds of miniature sean hannitys burst from roger ailes' corpse\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kellie-pickler-family-feud-fail_us_57726de8e4b017b379f73c2d',\n",
       "  'headline': \"kellie pickler hilariously misses the buzzer in celebrity 'family feud' fail\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/military-institutes-new-dont-tell-let-me-guess-policy-1819570876',\n",
       "  'headline': \"military institutes new 'don't tell, let me guess' policy\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jeb-bush-family-dynasty_us_55ce4cf1e4b07addcb4304a7',\n",
       "  'headline': \"jeb bush insists he's a washington outsider\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttps://www.washingtonpost.com/world/jailed-washington-post-correspondent-has-christmas-meal-with-family/2015/12/25/dacdb3ac-ab2d-11e5-bff5-905b92f5f94b_story.html?postshare=2821451070250310&tid=ss_tw',\n",
       "  'headline': 'washington post journalist jailed in iran has christmas meal with family',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/colorado-voters-trump-voter-fraud-probe_us_59f3690ae4b077d8dfc960b9',\n",
       "  'headline': \"coloradans who deregistered after trump request for voter data aren't signing up again\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/vin-diesel-dubs-i-am-groot_n_5635213.html',\n",
       "  'headline': \"watch vin diesel say 'i am groot' in different languages\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/luiso-garcia-tess-asplund-image_us_5730b710e4b096e9f09205c8',\n",
       "  'headline': \"artist's stunning image honors the moment one woman defied a neo-nazi march\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/voters-glad-they-got-hope-in-politicians-out-of-system-1819578986',\n",
       "  'headline': 'voters glad they got hope in politicians out of system for next election cycle or two',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/new-years-resolutionlet-c_b_6407324.html',\n",
       "  'headline': \"new year's resolution -- let colleges lead the way to a new normal in cuba\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/paris-notre-dame-gas-cylinders_us_57d000afe4b0a48094a69500',\n",
       "  'headline': 'paris police arrest second couple over notre dame gas cylinders',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/teen-beauty-products_b_5194220.html',\n",
       "  'headline': 'my 8 favorite beauty products',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/financial-burden-of-cancer-can-harm-quality-of-life_us_56e6df1ae4b065e2e3d68296',\n",
       "  'headline': 'financial burden of cancer can harm quality of life',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/high-schooler-promises-to-have-man-s-impregnated-daught-1819577575',\n",
       "  'headline': \"high schooler promises to have man's impregnated daughter home by midnight\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/cnn-anchor-interviews-al-jazeera-anchor-who-interviewed-1819572373',\n",
       "  'headline': 'cnn anchor interviews al jazeera anchor who interviewed libyan rebels',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-quadruple-bottom-line_b_5413762.html',\n",
       "  'headline': 'the quadruple bottom line: its time has come',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/elizabeth-lesser-repairing-a-broken-relationship_us_57ed5a77e4b082aad9b9f46c',\n",
       "  'headline': 'this may be holding you back from repairing a broken relationship',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/barack-obama-public-lands-trump_us_5837f4ece4b09b6056006007',\n",
       "  'headline': \"in the weeks before trump takes office, obama's mad dash to save public lands\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-changing-holiday-shop_b_13256406.html',\n",
       "  'headline': 'the changing holiday shopping landscape',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/burger-king-introduces-new-thing-to-throw-in-front-of-k-1819573136',\n",
       "  'headline': 'burger king introduces new thing to throw in front of kids after another hellish day at work',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/funny-road-sign_n_5952970.html',\n",
       "  'headline': 'this road sign is a lot less helpful than it looks',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/day-chalked-up-as-loss-by-10-15-a-m-1819577478',\n",
       "  'headline': 'day chalked up as loss by 10:15 a.m.',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/australian-lawmaker-shoots-opponents-in-campaign-ad-draws-ire-after-orlando_us_5761209fe4b05e4be860398d',\n",
       "  'headline': 'australian lawmaker shoots opponents in campaign ad, draws ire after orlando',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/creepy-peter-pan-disney-world_us_5609484ae4b0dd8503081abc',\n",
       "  'headline': 'kinda creepy peter pan pranks disney world',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/brown-ribbon-campaign-hollywood-latinos_us_56d1f358e4b0871f60eba3ce',\n",
       "  'headline': \"#brownribboncampaign reminds us oscar diversity isn't just black and white\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/high-school-elects-gay-45-year-old-homecoming-king-for-1819575825',\n",
       "  'headline': 'high school elects gay 45-year-old homecoming king for first time in school history',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/george-thorogood-fan-disgusted-to-learn-musician-licens-1824210015',\n",
       "  'headline': \"george thorogood fan disgusted to learn musician licensed 'bad to the bone' for commercial purposes\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/for-americas-future-engin_b_6118016.html',\n",
       "  'headline': \"for america's future, engineering needs to diversify\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/darrelle-revis-to-be-charged-in-fight-that-leaves-two-men-unconscious_us_58a716b6e4b045cd34c0ed1c',\n",
       "  'headline': 'darrelle revis to be charged in fight that leaves two men unconscious',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dont-pay-another-bill-unt_b_8200920.html',\n",
       "  'headline': \"don't pay another bill until you pay this\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/move-to-houseboat-regretted-by-third-day-1819566198',\n",
       "  'headline': 'move to houseboat regretted by third day',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/buick-regal-named-best-vehicle-in-class-for-idling-outs-1819579697',\n",
       "  'headline': 'buick regal named best vehicle in class for idling outside off-track betting parlor',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/arizona-iced-tea-unveils-new-4-foot-tall-cans-1819590557',\n",
       "  'headline': 'arizona iced tea unveils new 4-foot-tall cans',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/black-lives-matter-census-project-alicia-garza_us_5a9429fce4b02cb368c42d52',\n",
       "  'headline': \"blm's alicia garza launches census project to mobilize black political power\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/911-audio-leaked-in-killing-of-unarmed-black-teen-christian-taylor_us_55c6757de4b0923c12bd15f6',\n",
       "  'headline': 'police audio leaked in killing of unarmed black teen christian taylor',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/for-a-firsttime-marathone_b_6031016.html',\n",
       "  'headline': \"for a first-time marathoner, there's strength in numbers\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/hate-preachers-on-qatar-c_b_9785706.html',\n",
       "  'headline': 'hate preachers on qatar campus: obama gives qatar undeserved a+ on fighting incitement',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/10th-grade-prodigy-studying-mathematics-at-10th-grade-l-1819577209',\n",
       "  'headline': '10th-grade prodigy studying mathematics at 10th-grade level',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/3-reminders-that-can-help-you-raise-resilient-kids_b_5585453.html',\n",
       "  'headline': '3 reminders that can help you raise resilient kids',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/nation-horrified-to-learn-about-war-in-afghanistan-whil-1819574188',\n",
       "  'headline': 'nation horrified to learn about war in afghanistan while reading up on petraeus sex scandal',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/trump-relaxes-after-debate-by-slipping-back-into-nice-1819579285',\n",
       "  'headline': 'trump relaxes after debate by slipping back into nice, warm personal reality',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/area-liberal-no-longer-recognizes-fanciful-wildly-inac-1819579440',\n",
       "  'headline': 'area liberal no longer recognizes fanciful, wildly inaccurate mental picture of country he lives in',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/update-the-onion-is-immediately-suspending-productio-1820056365',\n",
       "  'headline': \"update: 'the onion' is immediately suspending production on our basketball infographic video directed by brett ratner\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/george-foreman-grill-retires-to-promote-own-grill-1819567575',\n",
       "  'headline': 'george foreman grill retires to promote own grill',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/study-finds-having-it-all-leading-indicator-that-everyt-1822119926',\n",
       "  'headline': 'study finds having it all leading indicator that everything will come crashing down',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/shooting-copenhagen-synagogue-_n_6685512.html',\n",
       "  'headline': 'shooting at copenhagen synagogue leaves 1 dead, 2 officers wounded',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/paris-hilton-becomes-kim-kardashian-lookalike-for-kanye-west-line_us_5a71aee0e4b0be822ba1fe81',\n",
       "  'headline': 'paris hilton impersonates kim kardashian for kanye west fashion line',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/international-space-station-tented-to-spray-for-xenomor-1826362305',\n",
       "  'headline': 'international space station tented to spray for xenomorphs',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/ex-boyfriend-hopes-to-still-be-terrible-incompatible-f-1825299729',\n",
       "  'headline': 'ex-boyfriend hopes to still be terrible, incompatible friends',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/teen-is-accepted-into-113-colleges-4-million-scholarship-accepts-full-ride-to-hbcu_us_5aeb7021e4b0c4f193206248',\n",
       "  'headline': 'teen accepted into 113 colleges chooses full ride to hbcu',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/danica-roem-donald-trump-transgender-lawmaker-virginia_us_5a183d1be4b0d4906cae74fe',\n",
       "  'headline': \"transgender lawmaker danica roem: trump shows there's 'no barrier' to getting elected\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/shit-friend-just-said-something-to-obnoxious-drunk-guy-1819573422',\n",
       "  'headline': 'shit, friend just said something to obnoxious drunk guy on bus',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/weekend-encounter-with-coworker-never-acknowledged-1819574897',\n",
       "  'headline': 'weekend encounter with coworker never acknowledged',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/study-74-of-home-contractors-end-up-accidentally-wall-1819578206',\n",
       "  'headline': 'study: 74% of home contractors end up accidentally walling themselves in during housing construction',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/goth-kid-builds-scary-ass-birdhouse-1819587621',\n",
       "  'headline': 'goth kid builds scary-ass birdhouse',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/outbreak-of-va-va-vooms-traced-to-miniskirt-wearing-blo-1819571745',\n",
       "  'headline': 'outbreak of va-va-vooms traced to miniskirt-wearing blonde',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/grandmother-cant-believe-they-let-people-with-tattoos-o-1819567208',\n",
       "  'headline': \"grandmother can't believe they let people with tattoos on price is right\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/7th-heaven-reunion_n_5837788.html',\n",
       "  'headline': \"the '7th heaven' cast reunites for the first time in 8 years\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/are-we-meeting-the-needs-of-our-nations-rich-1819586293',\n",
       "  'headline': \"are we meeting the needs of our nation's rich?\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/there-is-nothing-libertar_b_6883224.html',\n",
       "  'headline': 'there is nothing libertarian about conservatives',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/yorkshire-terrier-monogrammed-1819588421',\n",
       "  'headline': 'yorkshire terrier monogrammed',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/pope-cleans-up-dead-angel-who-flew-into-sistine-chapel-1819578166',\n",
       "  'headline': 'pope cleans up dead angel who flew into sistine chapel window',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/remainder-of-ross-ice-shelf-now-in-smithsonian-freezer-1819567901',\n",
       "  'headline': 'remainder of ross ice shelf now in smithsonian freezer',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/panicked-agriculture-secretary-momentarily-forgets-what-1819570602',\n",
       "  'headline': 'panicked agriculture secretary momentarily forgets what corn is',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/introvert-holiday-party_us_584ea33be4b04c8e2bb07609',\n",
       "  'headline': \"an introvert's guide to throwing a solid holiday party\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/area-client-would-like-a-different-font-1819564797',\n",
       "  'headline': 'area client would like a different font',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/lauren-jauregui-lgbtq-youth-twitter_us_5a7b620ae4b0c6726e0ec785',\n",
       "  'headline': \"fifth harmony's lauren jauregui blasts 'toxic' homophobia in poignant twitter exchange\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/usda-admits-weight-loss-not-possible-for-people-who-don-1819579231',\n",
       "  'headline': \"usda admits weight loss not possible for people who don't like salmon\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/six-teenagers-in-britain-suspected-of-killing-polish-man-in-hate-crime_us_57c6cef3e4b078581f102a55',\n",
       "  'headline': 'six teenagers in britain suspected of killing polish man in hate crime',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/cubans-new-dictator-doing-it-all-wrong-1819568653',\n",
       "  'headline': 'cubans: new dictator doing it all wrong',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/obama-fills-out-lukewarm-glassdoor-review-after-exiting-1819579553',\n",
       "  'headline': 'obama fills out lukewarm glassdoor review after exiting presidency',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.hollywoodreporter.com/news/bill-henderson-dead-jazz-vocalist-881521',\n",
       "  'headline': 'bill henderson, jazz vocalist and actor, dies at 90',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/very-specific-food-pyramid-recommends-two-to-three-shri-1819569561',\n",
       "  'headline': 'very specific food pyramid recommends two to three shrimp scampis per year',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/alaskan-gray-wolf-cant-believe-no-one-told-him-he-s-got-1819574555',\n",
       "  'headline': \"alaskan gray wolf can't believe no one told him he's got snow on nose\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/how-to-emotionally-recover-from-the-election_us_580a5b92e4b0f8715789f9ee',\n",
       "  'headline': 'how to emotionally recover from the election',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/man-always-taking-good-mood-out-on-friends-1819576983',\n",
       "  'headline': 'man always taking good mood out on friends',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/going-back-to-school-gree_b_5629920.html',\n",
       "  'headline': '6 essentials for a trash-free lunch',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/candidate-delighted-to-be-in-chair-factory-1819565813',\n",
       "  'headline': 'candidate delighted to be in chair factory',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/woman-knows-exactly-which-knife-she-d-grab-out-of-cutle-1824282662',\n",
       "  'headline': \"woman knows exactly which knife she'd grab out of cutlery drawer in event of home invasion\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/bored-predator-drone-pumps-a-few-rounds-into-mountain-g-1819589455',\n",
       "  'headline': 'bored predator drone pumps a few rounds into mountain goat',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/30-year-old-nes-still-wasting-life-playing-video-games-1819591277',\n",
       "  'headline': '30-year-old nes still wasting life playing video games',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/schools-enact-positive-ch_b_6964602.html',\n",
       "  'headline': 'schools enact positive change with drama therapy',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/world-map-rearranged-to-accommodate-poor-geography-skil-1819586194',\n",
       "  'headline': 'world map rearranged to accommodate poor geography skills of americans\\x97nations ordered alphabetically',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/woman-leaving-meeting-worried-she-came-off-as-too-compe-1819578802',\n",
       "  'headline': 'woman leaving meeting worried she came off as too competent',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/professor-wheaton-college_us_568d8218e4b0cad15e632ed7',\n",
       "  'headline': 'professor threatened with firing says wheaton college is changing the rules',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/jfk-jr-celebrates-10-000th-coupling-1819586177',\n",
       "  'headline': 'jfk jr. celebrates 10,000th coupling',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/man-with-stupid-breaks-off-co-dependent-relationship-1819586100',\n",
       "  'headline': 'man with stupid breaks off co-dependent relationship',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/internet-pop-up-quiz-insulting-1819567519',\n",
       "  'headline': 'internet pop-up quiz insulting',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/suspension-of-disbelief-goes-unrewarded-1819572258',\n",
       "  'headline': 'suspension of disbelief goes unrewarded',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-late-show-updated-trumps-election-night-speech-with-annotations_us_5a049748e4b0937b51105bae',\n",
       "  'headline': \"'the late show' updated trump's election night speech with annotations\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/pterosaur-avatar-ancient-flying-creature_n_5811456.html',\n",
       "  'headline': \"ancient flying beast named after 'avatar' creature\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/tennessee-senate-passed-a-bill-to-erect-a-monument-to-unborn-children_us_5ae07e26e4b061c0bfa3f31b',\n",
       "  'headline': \"tennessee senate passes a bill to erect a memorial to 'victims of abortion'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/report-no-one-at-white-castle-wants-to-make-friends-1819571409',\n",
       "  'headline': 'report: no one at white castle wants to make friends',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/45-year-old-to-help-candidate-understand-youth-vote-1819577119',\n",
       "  'headline': '45-year-old to help candidate understand youth vote',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/word-millennials-forced-into-headline-to-boost-pagevi-1819578021',\n",
       "  'headline': \"word 'millennials' forced into headline to boost pageviews\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://sports.theonion.com/dozens-of-social-issues-thankful-they-never-had-to-go-t-1819578940',\n",
       "  'headline': 'dozens of social issues thankful they never had to go toe-to-toe with muhammad ali',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/fox-news-struggling-to-attract-younger-60-75-demographi-1820392371',\n",
       "  'headline': 'fox news struggling to attract younger 60-75 demographic',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/after-a-string-of-accidents-u-haul-announces-closure-o-1819590776',\n",
       "  'headline': 'after a string of accidents, u-haul announces closure of aircraft division',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/why-stakes-is-too-high-to-bother-with-white-tears_us_58cbb678e4b07112b6472c6e',\n",
       "  'headline': 'why stakes is too high to bother with white tears',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/growing-up-with-the-holoc_b_7066722.html',\n",
       "  'headline': 'growing up with the holocaust as a writer',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/horses-alzheimers-_n_5273331.html',\n",
       "  'headline': \"the surprising way horses can help ease alzheimer's symptoms\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/beaver-cant-wait-to-get-started-on-dam-1819587813',\n",
       "  'headline': \"beaver can't wait to get started on dam\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/republican-women-dont-like-trump_us_56f53da2e4b0a3721819aea0',\n",
       "  'headline': \"huffpollster: republican women really don't like trump\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/clues-to-the-new-white-house-org-chart_us_59807bd2e4b0cb4fc1c73c28',\n",
       "  'headline': 'what we know so far about the new white house org chart',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/morbidly-obese-man-enjoys-disabled-privileges-with-moto-1819564913',\n",
       "  'headline': 'morbidly obese man enjoys disabled privileges with motorized cart',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/no-one-will-push-you-into-running-for-president-jeb-1819578581',\n",
       "  'headline': \"'no one will push you into running for president,' jeb bush softly whispers before tucking in sleeping grandson\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/madcap-romp-escalates-into-zany-hijinks-1819564662',\n",
       "  'headline': 'madcap romp escalates into zany hijinks',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.cheatsheet.com/health-fitness/5-longevity-secrets-from-the-worlds-healthiest-cultures.html/?a=viewall',\n",
       "  'headline': \"5 longevity secrets from the world's healthiest cultures\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/8-reasons-the-holidays-after-divorce-are-anything-but-depressing_us_5679cb8be4b0b958f65888cd',\n",
       "  'headline': '8 perks of being divorced during the holidays',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/nation-healed-by-awesome-sports-highlight-1819586415',\n",
       "  'headline': 'nation healed by awesome sports highlight',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/why-moms-demand-action-will-participate-in-day-without-a-woman_us_58bf6a34e4b0ed7182681737',\n",
       "  'headline': 'why moms demand action will participate in day without a woman',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/hillary-clinton-asian-american-outreach-director_us_56edb13fe4b09bf44a9d7743',\n",
       "  'headline': \"hillary clinton's asian american outreach director leaving campaign\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/interview-with-louise-mun_b_5579661.html',\n",
       "  'headline': 'interview with louise munson, playwright of luigi',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/suspect-reportedly-arrested-over-explosives-sent-to-washington-dc-area_us_5aba73cbe4b0decad04e8918',\n",
       "  'headline': 'suspect reportedly arrested over explosives sent to washington, d.c. area',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/lena-dunham-lenny-letter-zinzi-clemmons_us_5a12e341e4b0dd63b1abd2a3',\n",
       "  'headline': \"writer calls on women of color 'to divest from lena dunham' after controversy\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/area-dad-will-only-watch-things-in-hd-1819569627',\n",
       "  'headline': 'area dad will only watch things in hd',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-unexpected-place-youre-probably-overeating_b_7019494.html',\n",
       "  'headline': \"the unexpected place you're probably overeating\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/don-lemon-sean-spicer-dumber_us_58cb6cb0e4b00705db4e0297',\n",
       "  'headline': \"don lemon on sean spicer: everyone 'is dumber for having listened to that'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kansas-state-lawsuit-sexual-assault_us_5718cb99e4b0c9244a7aecff',\n",
       "  'headline': 'kansas state refused to investigate sexual assaults because they happened off-campus, lawsuit says',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/avery-williamson-911-cleats_us_57d6cf2ce4b03d2d459b7908',\n",
       "  'headline': 'nfl player avery williamson wears 9/11 cleats despite threat of fine',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/rbg-for-garland-swap-lol_us_58f645d9e4b0da2ff863a325',\n",
       "  'headline': 'trump confidant floats crazy rbg-for-merrick-garland scotus swap',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/u-s-continues-proud-tradition-of-diversity-on-front-li-1819566786',\n",
       "  'headline': 'u.s. continues proud tradition of  diversity on front lines',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/south-korean-president-meets-north-koreas-kim-jong-un_us_5b094ebae4b0fdb2aa53e504',\n",
       "  'headline': \"south korean president meets north korea's kim jong un to talk trump summit\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/nation-s-financial-advisors-recommend-capturing-magical-1819578300',\n",
       "  'headline': \"nation's financial advisors recommend capturing magical creature that grants wishes\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/judges-rupauls-drag-race_us_5aac09ade4b0c33361b042b7',\n",
       "  'headline': \"michelle, ross and carson on the wild ride to 'rupaul's drag race'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/content-writer-awkwardly-shows-parents-around-website-w-1819577722',\n",
       "  'headline': 'content writer awkwardly shows parents around website where he works',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/viking-discovery-canada-satellite_us_56fe36bfe4b0daf53aef5a26',\n",
       "  'headline': 'possible viking find could rewrite north american history',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/korey-kauffman-arrests_us_55cf6082e4b07addcb43263c',\n",
       "  'headline': 'new details emerge in forgotten murder that snared attorney, highway patrolmen',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/watching-faces-of-students-as-they-finish-the-lottery-h-1819571267',\n",
       "  'headline': \"watching faces of students as they finish 'the lottery' highlight of english teacher's year\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/no-one-in-prison-sure-how-jared-fogle-still-eating-subw-1825829806',\n",
       "  'headline': 'no one in prison sure how jared fogle still eating subway every meal',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/sean-spicer-trump-first-amendment_us_585d2aece4b0d9a59457f827',\n",
       "  'headline': \"sean spicer says donald trump is a 'champion' of first amendment\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/uncovered-california-community-college-students-quest-for-mental-health-services_us_57742736e4b042fba1cf003b',\n",
       "  'headline': \"uncovered california: community college students' quest for mental health services\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/will-grace-will-return-with-10-episodes-in-2017-according-to-leslie-jordan_us_5867e574e4b0de3a08f89afd',\n",
       "  'headline': \"debra messing doesn't want you to freak out about a 'will & grace' revival just yet (update)\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/nation-delighted-as-many-famous-people-in-same-room-tog-1819577507',\n",
       "  'headline': 'nation delighted as many famous people in same room together',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/area-man-winded-after-particularly-lengthy-wendys-order-1819573530',\n",
       "  'headline': \"area man winded after particularly lengthy wendy's order\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-6-real-reasons-im-happy-to-be-married_us_59f92365e4b0de896d3f2c71',\n",
       "  'headline': \"the 6 real reasons i'm happy to be married\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/biden-busted-in-dnc-parking-lot-selling-bootleg-i-m-wi-1819579067',\n",
       "  'headline': \"biden busted in dnc parking lot selling bootleg 'i'm with her' t-shirts\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/white-person-waved-past-beeping-walgreens-security-barr-1819566456',\n",
       "  'headline': 'white person waved past beeping walgreens security barrier',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/khloe-kardashian-denies-cocaine-graduation-party_us_55b6248ae4b0a13f9d18f9f6',\n",
       "  'headline': \"khloe kardashian says no one was doing cocaine at kylie jenner's graduation party\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/genuine-happiness-now-seen-only-on-game-shows-1819586766',\n",
       "  'headline': 'genuine happiness now seen only on game shows',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/chewbacca-in-chewbacca-mom-mask_us_5745e3f0e4b03ede441394b0',\n",
       "  'headline': \"chewbacca just got himself a 'chewbacca mom' mask\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-national-security-monica-crowley_us_58542a74e4b08debb788afc4',\n",
       "  'headline': 'yet another donald trump pick has a habit of spreading dangerous conspiracy theories',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/dog-trying-its-absolute-hardest-1819567282',\n",
       "  'headline': 'dog trying its absolute hardest',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/mississippi-bans-soft-drinks-smaller-than-20-ounces-1819574746',\n",
       "  'headline': 'mississippi bans soft drinks smaller than 20 ounces',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/explosion-fedex-austin_us_5ab0dbe0e4b00549ac7ee78d',\n",
       "  'headline': 'explosion at fedex facility outside san antonio may be linked to austin bombings, fbi says',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/cop-police-officer-kyle-isenor-gives-laurie-burbine-vaentines-day-rose-instead-of-ticket_us_56c34390e4b0c3c550529b0f',\n",
       "  'headline': \"cupid cop gave out roses, cards on valentine's day instead of tickets\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/fox-news-london-attack_us_59343347e4b075bff0f475f8',\n",
       "  'headline': 'fox news host disavows internment camps, after panelists suggest rounding up muslims',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/reince-priebus-trump_us_56eebd78e4b09bf44a9d89a4',\n",
       "  'headline': \"reince priebus says it's 'too late' for a new candidate to stop trump\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/minimum-wage-fee_n_5656278.html',\n",
       "  'headline': \"minnesota café charges 35 cent 'fee' to protest minimum wage hike\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/asshole-from-plane-greeted-at-baggage-claim-by-whole-fa-1819574329',\n",
       "  'headline': 'asshole from plane greeted at baggage claim by whole family',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/an-american-dreamer-in-the-age-of-trump_us_592206a4e4b0e8f558bb27be',\n",
       "  'headline': 'an american dreamer in the age of trump',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/hillary-clinton-confederate-flag-mlk-day_us_569d1a05e4b0778f46fa238a',\n",
       "  'headline': \"hillary clinton celebrates confederate flag's removal at mlk day ceremony\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/david-brooks-obama-manhood-problem_n_5182525.html',\n",
       "  'headline': \"david brooks: obama has a 'manhood problem in the middle east'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/why-terrorists-attack-us_us_57dd6b40e4b053b1ccf29b6a',\n",
       "  'headline': 'why terrorists attack us',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/everyone-proud-of-grandma-for-staying-awake-1819571277',\n",
       "  'headline': 'everyone proud of grandma for staying awake',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/nypd-cell-phone-spying-stingray_us_56bcaf05e4b0b40245c57fd3',\n",
       "  'headline': 'the nypd has secretly been spying on cell phones since 2008',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/republicans-urge-obama-administration-to-crack-down-on-sanctuary-cities_us_55a5370fe4b0b8145f73a258',\n",
       "  'headline': 'republicans urge obama administration to crack down on sanctuary cities',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/floor-plan-of-retirement-community-90-defibrillator-lo-1819592044',\n",
       "  'headline': 'floor plan of retirement community 90% defibrillator locations',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/voting-2014_n_6083338.html',\n",
       "  'headline': 'voters in 14 states navigating new rules while trying to cast ballots',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/arne-duncan-stressed-about-preparing-for-standardized-s-1819578112',\n",
       "  'headline': 'arne duncan stressed about preparing for standardized secretary of education exam',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/tom-brokaw-touched-so-many-women-would-go-out-of-their-1825655136',\n",
       "  'headline': 'tom brokaw touched so many women would go out of their way to defend filthy old pervert like himself',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/study-periods-text-messages_us_566881ede4b009377b236e9d',\n",
       "  'headline': 'stop complaining about the evolution of text language. period.',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/half-dressed-man-frantically-scrambles-out-of-home-afte-1819574273',\n",
       "  'headline': \"half-dressed man frantically scrambles out of home after hearing toyotathon deals won't last long\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/tech-is-the-future-reports-local-dad-1819575329',\n",
       "  'headline': 'tech is the future, reports local dad',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/trump-fulfills-campaign-promise-of-pushing-major-immigr-1819580266',\n",
       "  'headline': 'trump fulfills campaign promise of pushing major immigration decision on someone else so he can watch tv',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/communication-matters-get-your-message-out_b_6646550.html',\n",
       "  'headline': 'communication matters: getting your message out',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/son-surprised-dad-knows-johnny-cash-song-1819566579',\n",
       "  'headline': 'son surprised dad knows johnny cash song',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/man-suddenly-regretting-asking-to-be-taken-seriously-by-1819577563',\n",
       "  'headline': 'man suddenly regretting asking to be taken seriously by peers',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/eleven-year-old-has-miniskirt-pumps-vague-notion-of-w-1819565556',\n",
       "  'headline': 'eleven-year-old has miniskirt, pumps, vague notion of what sex is',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/cinemax-director-wins-award-for-skinematography-1819567518',\n",
       "  'headline': 'cinemax director wins award for skinematography',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/doctor-unable-to-hide-his-excitement-from-patient-with-1819567704',\n",
       "  'headline': 'doctor unable to hide his excitement from patient with ultra-rare disease',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trains-crash-pennsyvlania_us_599bd992e4b04c532f43e596',\n",
       "  'headline': 'dozens injured after trains collide in pennsylvania',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ed-sheeran-wedding-singer_us_57c86759e4b078581f11b3c5',\n",
       "  'headline': \"ed sheeran sang 'chasing cars' at a wedding, and now we're swooning\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/game-of-thrones-producers-reveal-series-moved-beyond-1819580219',\n",
       "  'headline': \"'game of thrones' producers reveal series moved beyond show's written script halfway through current season\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/girls-scouts-announces-they-ll-never-ever-let-gross-fuc-1825752568',\n",
       "  'headline': \"girls scouts announces they'll never ever let gross fucking boys in\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/un-vote-jerusalem-trump_us_5a3beeade4b025f99e158445',\n",
       "  'headline': \"un rebukes trump's jerusalem move in overwhelming vote\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/family-man_b_5406629.html',\n",
       "  'headline': \"father's day tribute to a family man\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/white-house-this-is-not-the-geologic-era-to-debate-gu-1819580367',\n",
       "  'headline': \"white house: 'this is not the geologic era to debate gun control'\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/professors-slang-bae-on-fleek_us_55e4794fe4b0aec9f353e0c4',\n",
       "  'headline': \"professors try to figure out what 'bae' and 'on fleek' mean\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/house-republican-carbon-rules_n_5568362.html',\n",
       "  'headline': \"house republican spending bill seeks to block obama's carbon rules\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/fred-durst-spray-paints-limp-bizkit-on-bridge-1819589561',\n",
       "  'headline': \"fred durst spray paints 'limp bizkit' on bridge\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/poll-donald-trump-ahead_us_58190572e4b07c97c1c50525',\n",
       "  'headline': \"democrats shouldn't panic over one poll showing donald trump ahead\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/bored-kim-jong-un-stacks-entire-north-korean-populace-i-1819576849',\n",
       "  'headline': 'bored kim jong-un stacks entire north korean populace into human pyramid to kill time',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/chikungunya-malaysia-zika-dengue_us_5a5cfd70e4b04f3c55a51fb9',\n",
       "  'headline': 'how humans are laying out the welcome mat for mosquitoes and the diseases they carry',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/jennifer-lawrence-robert-de-niro-glaad-awards_us_57388feae4b077d4d6f35088',\n",
       "  'headline': 'jennifer lawrence honored robert de niro at the glaad media awards the only way she knows how',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/11-unexpected-ways-to-use-grapefruit_b_6708770.html',\n",
       "  'headline': '11 unexpected ways to use grapefruit',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/study-90-of-all-meowing-comes-from-owners-trying-to-g-1819580323',\n",
       "  'headline': 'study: 90% of all meowing comes from owners trying to get cats to meow back',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/photos-of-daughters-samantha-conlon_us_55d4daa7e4b0ab468d9f88a9',\n",
       "  'headline': '14 photos that show the special bond between moms and daughters',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/fans-are-freaking-out-because-the-last-jedi-is-plural-on-foreign-language-posters_us_58a73881e4b07602ad544f47',\n",
       "  'headline': \"'star wars' fans are freaking out because 'jedi' in 'the last jedi' is plural\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/study-headaches-are-the-body-s-way-of-communicating-it-1825925378',\n",
       "  'headline': \"study: headaches are the body's way of communicating it wants pills\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://live.huffingtonpost.com/r/archive/segment/559beeb702a76050e4000059',\n",
       "  'headline': \"how jimmy carter learned to make his wife rosalynn a 'full' partner\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dave-brat-in-my-grill_us_5890ae88e4b0c90eff001cc5',\n",
       "  'headline': \"gop congressman complains women are 'in my grill' over obamacare repeal\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/religious-leaders-groups-are-appalled-by-trumps-immigration-orders_us_58890044e4b0737fd5cb2212',\n",
       "  'headline': \"religious leaders, groups are appalled by trump's immigration orders\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/lice-having-blast-trying-out-different-wigs-at-costume-1819838808',\n",
       "  'headline': 'lice having blast trying out different wigs at costume shop',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/places-you-wouldnt-have-gone-10-years-ago_n_7174776.html',\n",
       "  'headline': \"10 places you wouldn't have gone 10 years ago\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/anita-alvarez-recuse-laquan-mcdonald_us_572b8913e4b0bc9cb046004d',\n",
       "  'headline': \"chicago's top prosecutor will not try the officer who killed laquan mcdonald\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/tim-burton-big-eyes_n_6316508.html',\n",
       "  'headline': \"'big eyes' is about the 'most quiet feminist you've ever met'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://www.capitalnewyork.com/article/media/2015/11/8582731/bloomberg-gadfly-debuts-bid-shake-financial-commentary-space',\n",
       "  'headline': 'bloomberg gadfly debuts in bid to shake-up financial commentary space',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/harry-reid-nra_us_5612e3abe4b0baa355aceb7d',\n",
       "  'headline': \"harry reid: republicans are 'acting as puppets for the nra'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/little-tobacco-hit-with-3-5-hundred-lawsuit-1819566168',\n",
       "  'headline': 'little tobacco hit with $3.5 hundred lawsuit',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/women-in-business-qa-rebe_b_9818202.html',\n",
       "  'headline': 'women in business q&a: rebecca henderson, group president, randstad professional solutions',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/mike-pence-disappointed-in-the-200-000-husbands-and-fat-1819579554',\n",
       "  'headline': 'mike pence disappointed in the 200,000 husbands and fathers who permitted women to attend march',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/jeff-flake-delivers-searing-critical-applause-for-trum-1822559306',\n",
       "  'headline': 'jeff flake delivers searing, critical applause for trump during state of the union',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/michelle-obama-wont-run-for-office_us_58544931e4b0b3ddfd8c8d9e',\n",
       "  'headline': \"michelle obama explains in no uncertain terms why she won't run for office\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/republican-food-stamp-bill_us_5ae9f862e4b022f71a047b9c',\n",
       "  'headline': 'republican food stamp bill would cut benefits, but not the size of government',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/russian-trump-coin_us_587e6cbee4b0f63fcfa366ef',\n",
       "  'headline': \"russians mint 'in trump we trust' coin ahead of u.s. inauguration\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kristen-wiig-fake-trailer_us_562fb79fe4b0c66bae59af99',\n",
       "  'headline': \"kristen wiig's fake trailer from 'jimmy kimmel' needs to be made into a real movie\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/patients-with-limited-english-are-more-likely-to-return-to-the-er_us_57238b07e4b01a5ebde5771a',\n",
       "  'headline': 'patients with limited english are more likely to return to the er',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/dear-baby-boomers-step-as_b_5485858.html',\n",
       "  'headline': 'dear baby boomers, step aside',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/john-oliver-bitcoin-cryptocurrency_us_5aa6291be4b01b9b0a3cce82',\n",
       "  'headline': \"don't spend a cent on bitcoin until you see john oliver's cryptocurrency warning\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/anne-meara-twitter_n_7432772.html',\n",
       "  'headline': 'celebrities mourn anne meara on twitter after news of her death',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/basics-obamacare-enrollment-2018_us_59e004e0e4b0a52aca16bdcf',\n",
       "  'headline': \"here's what you need to know about obamacare enrollment this year\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/relationship-experts-recommend-single-women-try-bathing-1819578424',\n",
       "  'headline': 'relationship experts recommend single women try bathing in open stream until suitor glimpses them through trees',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/married-couple-longs-for-days-when-they-only-quietly-re-1819578667',\n",
       "  'headline': 'married couple longs for days when they only quietly resented one another',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/group-of-friends-chanting-shots-make-compelling-point-1819577043',\n",
       "  'headline': \"group of friends chanting 'shots' make compelling point\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/blind-date-pronounces-every-syllable-of-word-comfortabl-1819566810',\n",
       "  'headline': \"blind date pronounces every syllable of word 'comfortable'\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/post_8367_b_5877750.html',\n",
       "  'headline': 'to hope again',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/therapy-works-better-in-the-morning_us_58054b6fe4b0dd54ce34b8a4',\n",
       "  'headline': 'the science-backed reason to see your therapist in the morning',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/turkish-president-no-muslim-family-should-engage-in-birth-control_us_574c96e9e4b055bb11728a2f',\n",
       "  'headline': 'turkish president: no muslim family should engage in birth control',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/claire-danes-is-expecting-baby-number-two-with-hugh-dancy_us_5ad758c1e4b029ebe0201bcd',\n",
       "  'headline': 'claire danes is expecting baby number two with hugh dancy',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/vlogger-shamed-in-walmart-fitting-room-because-she-might-stretch-clothes_us_56b37e23e4b01d80b2454eba',\n",
       "  'headline': \"vlogger shamed in walmart fitting room because she might 'stretch' clothes\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/at-the-university-of-texa_1_b_7303180.html',\n",
       "  'headline': 'at the university of texas,  echoes of its confederate past reverberate in the present',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-obamacare-replacement_us_587cc478e4b0b3c7a7b205de',\n",
       "  'headline': 'trump hints at obamacare replacement that would look nothing like what republicans have in mind',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/bad-ass-engagement-ring-also-tells-the-time-and-tempera-1819589985',\n",
       "  'headline': 'bad-ass engagement ring also tells the time and temperature',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/director-paul-feig-says-men-have-to-speak-out-following-weinstein-rape-allegations_us_59dce323e4b094496e59805a',\n",
       "  'headline': \"director paul feig says 'men have to speak out' after weinstein sexual assault allegations\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/advertising-firm-unveils-new-mute-resistant-commercials-1819570677',\n",
       "  'headline': 'advertising firm unveils new mute-resistant commercials',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/rookie-justice-gorsuch-assigned-to-supreme-court-overni-1819579826',\n",
       "  'headline': 'rookie justice gorsuch assigned to supreme court overnight shift',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/mike-pence-vice-president_us_58824088e4b0e3a735689542',\n",
       "  'headline': \"mike pence takes oath of office as country's next vice president\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/rex-tillerson-blindsided-by-news-he-still-worked-for-st-1823728644',\n",
       "  'headline': 'rex tillerson blindsided by news he still worked for state department',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/story_n_6374716.html',\n",
       "  'headline': \"fight over obama's treasury nominee underscores battles within democratic party\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/rommel-hummel-dominate-parents-christmas-list-1819587711',\n",
       "  'headline': \"rommel, hummel dominate parents' christmas list\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/sign-of-narcissism_n_7463578.html',\n",
       "  'headline': 'a big myth about how to spot a narcissist',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/doll-housing-crisis-set-to-worsen-mean-older-brother-s-1819569425',\n",
       "  'headline': 'doll-housing crisis set to worsen, mean older brother says',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/slushing-in-utah-at-the-e_b_5519077.html',\n",
       "  'headline': 'slushing in utah at the end of ski season',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/electability-poll_us_56a28d0be4b076aadcc69052',\n",
       "  'headline': 'here are the candidates voters think can actually win in november',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/cameron-beckford_n_6403250.html',\n",
       "  'headline': \"missing maryland toddler's body found in ohio creek\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/terrified-jeb-bush-beginning-to-fade-from-visible-spect-1819578484',\n",
       "  'headline': 'terrified jeb bush beginning to fade from visible spectrum',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/adele-titanic-30th-birthday_us_5af01716e4b0ab5c3d674608',\n",
       "  'headline': \"adele celebrates 'titanic'-themed 30th birthday\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/changing-weather-inspires-area-conversationalist-1819564896',\n",
       "  'headline': 'changing weather inspires area conversationalist',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/fourth-tool-discovered-1819564068',\n",
       "  'headline': 'fourth tool discovered',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/texas-school-lunches_us_59bc0d4fe4b02da0e141a88a',\n",
       "  'headline': 'texas public school districts may now store, not trash, leftover food',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/hentai-message-board-features-surprisingly-close-knit-1822979571',\n",
       "  'headline': 'hentai message board features surprisingly close-knit, supportive community',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/hootie-and-the-blowfish-breaking-down-racial-barriers-1819586105',\n",
       "  'headline': 'hootie and the blowfish: breaking down racial barriers between black, white pussies',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/study-snapping-three-times-leading-way-to-recall-movie-1819569673',\n",
       "  'headline': 'study: snapping three times leading way to recall movies, actors',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trump-pardon-himself_us_5973587be4b0e79ec1999463',\n",
       "  'headline': \"donald trump insists he has the 'complete power' to pardon, as russia probe persists\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/syria-ceasefire_us_58661b7ae4b0d9a5945aec0d',\n",
       "  'headline': 'syria ceasefire, backed by russia and turkey, holds after initial clashes',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/france-end-of-life_n_6829532.html',\n",
       "  'headline': \"french parliament debates 'deep sleep' bill for end of life\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/reality-risk-reward-is-us_b_6671542.html',\n",
       "  'headline': 'reality, risk & reward: is us kids tv ripe for authenticity and autonomy?',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/harvey-weinstein-ami-rose-mcgowan_us_5a147274e4b09650540dcf2f',\n",
       "  'headline': 'transcript, emails show how tabloid reporters helped harvey weinstein get dirt on women',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/trumps-america--a-not-so_b_14614238.html',\n",
       "  'headline': \"trump's america--a not so shining city on the hill\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/rowan-metzner-in-my-mothers-clothes_us_56f58ee0e4b014d3fe231708',\n",
       "  'headline': \"photographer assumes endless identities in her mother's clothes\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/stripper-surprised-she-only-talked-to-2-homicide-detect-1819576200',\n",
       "  'headline': 'stripper surprised she only talked to 2 homicide detectives today',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/justin-bieber-fan-jealous-of-anne-frank-1819574830',\n",
       "  'headline': 'justin bieber fan jealous of anne frank',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/elevate-your-leadership_b_8905052.html',\n",
       "  'headline': 'elevate your leadership in 2016',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/larethia-haddon-face-down-halloween-dummy-gets-repeated-911-calls_us_561837e9e4b0dbb8000eba35',\n",
       "  'headline': \"woman's face-down halloween dummy gets repeated 911 calls\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/259-new-objects-now-available-in-gummi-form-1819586626',\n",
       "  'headline': '259 new objects now available in gummi form',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/kremlin-agent-not-even-going-to-bother-trying-to-compro-1819579654',\n",
       "  'headline': 'kremlin agent not even going to bother trying to compromise trump staffer who will be forced to resign in few months',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-future-is-blurry_b_5930848.html',\n",
       "  'headline': 'the future is blurry!',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/facegloria-brazil_n_7744966.html',\n",
       "  'headline': \"here's a pg-rated facebook alternative for evangelical christians\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/donald-trump-unqualified-poll_us_57dad724e4b04a1497b2f7da',\n",
       "  'headline': 'donald trump is unqualified to be president, majority of american voters say',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/body-given-false-hope-with-first-piece-of-fruit-in-9-da-1819579359',\n",
       "  'headline': 'body given false hope with first piece of fruit in 9 days',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/missouri-filibuster-lgbt-religious-freedom_us_56e0514be4b065e2e3d45217',\n",
       "  'headline': \"missouri democrats filibuster for 39 hours to stop anti-gay 'religious freedom' bill\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/hilde-lysiak-reporter-book-deal_us_5775758ce4b09b4c43bf89fe',\n",
       "  'headline': \"9-year-old reporter told to just be 'cute' has landed a book deal\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/this-craigslist-missed-connection-is-delightfully-feminist_us_575185a4e4b0c3752dcd50b7',\n",
       "  'headline': 'this craigslist missed connection is delightfully feminist',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/african-nation-not-war-torn-1819563956',\n",
       "  'headline': 'african nation not war-torn',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/three-chinese-tourists-dead-six-people-missing-in-borneo-shipwreck_us_588e0da4e4b017637794f0df',\n",
       "  'headline': 'three chinese tourists dead, six people missing in borneo shipwreck',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/god-announces-successful-test-of-first-category-7-hurri-1819579249',\n",
       "  'headline': 'god announces successful test of first category 7 hurricane',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/lone-geek-sits-off-by-self-reading-the-silmarillion-thr-1819565006',\n",
       "  'headline': 'lone geek sits off by self reading the silmarillion throughout recess',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/arya-stark-yearbook-quote_us_55a42b87e4b0ecec71bcc7ad',\n",
       "  'headline': 'this is what happens when you ask arya stark to write your yearbook quote',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/alex-jones-pleads-with-sandy-hook-parents-to-imagine-pa-1825338170',\n",
       "  'headline': 'alex jones pleads with sandy hook parents to imagine pain an expensive lawsuit would cause him',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/crowdfunding-animal-projects_n_5959980.html',\n",
       "  'headline': 'rescue animals get the help they need thanks to online donations',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/goldfish-cant-stand-bowlmate-1819568161',\n",
       "  'headline': \"goldfish can't stand bowlmate\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-box-of-mass-incarcera_b_6164542.html',\n",
       "  'headline': 'john legend speaks to the crack in the system caused by mass incarceration',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/sfpd-wheelchair-push-man_n_6527510.html',\n",
       "  'headline': 'police launch investigation after video appears to show cop shoving man in wheelchair into street',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/nasa-discovers-distant-planet-located-outside-funding-c-1819579186',\n",
       "  'headline': 'nasa discovers distant planet located outside funding capabilities',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/judge-keep-knees-together-rape-trial-resigns_us_58c2c017e4b0ed71826c2e6c',\n",
       "  'headline': \"judge who asked 'why couldn't you keep knees together?' resigns\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/white-house-rescissions-congress_us_5af0e298e4b0ab5c3d68ed8f',\n",
       "  'headline': 'white house prepares to send congress $15 billion spending cuts package',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttps://www.washingtonpost.com/politics/fiery-republican-race-heads-to-sc-known-for-dirty-tricks-and-brawls/2016/02/10/a99b161e-d024-11e5-b2bc-988409ee911b_story.html?hpid=hp_hp-top-table-main_southcarolina-830pm:homepage/story',\n",
       "  'headline': 'gop race heads to south carolina, known for dirty tricks and brawls',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/kindergartener-allegedly-barred-from-school-because-she-has-two-moms_us_560c0d6be4b0af3706deddca',\n",
       "  'headline': 'kindergartener allegedly barred from school because she has two moms',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttps://www.washingtonpost.com/opinions/bill-oreilly-makes-a-mess-of-history/2015/11/10/03ef0d94-87d9-11e5-be8b-1ae2e4f50f76_story.html',\n",
       "  'headline': \"george will trashes bill o'reilly: 'wise, he is not'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ohio-marijuana-legalization_us_55cbe829e4b064d5910a7ce5',\n",
       "  'headline': 'ohio voters will get to decide on legalizing marijuana',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://lifehacker.com/the-best-ifttt-recipes-to-make-the-most-of-your-vacatio-1778763165?utm_source=of%20a%20kind&utm_medium=referral&utm_campaign=10%20things%20newsletter',\n",
       "  'headline': 'the best ifttt recipes to make the most of your vacation',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/last-living-california-raisin-dies-of-prostate-cancer-1819576366',\n",
       "  'headline': 'last living california raisin dies of prostate cancer',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/microsoft-debuts-surface-book-its-first-laptop-plus-other-new-gizmos_us_5613f946e4b022a4ce5f9a28',\n",
       "  'headline': 'microsoft debuts surface book, its first laptop, plus other new gizmos',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/fda-confirms-psilocybin-reduces-risk-of-mindlessly-foll-1821046978',\n",
       "  'headline': \"fda confirms psilocybin reduces risk of mindlessly following society's rules like fucking lemming\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/t_us_55bd212de4b06363d5a26dc3',\n",
       "  'headline': \"fourth death in new york legionnaire's disease outbreak\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/national-weather-service-to-give-hurricanes-full-names-1819568307',\n",
       "  'headline': 'national weather service to give hurricanes full names',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/william-h-macy-prom-dress-daughter_us_5ae1e001e4b04aa23f2079b9',\n",
       "  'headline': 'william h. macy has ultimate dad moment dancing with his daughter before prom',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.comhttp://pubx.co/Na6Vrd',\n",
       "  'headline': 'warm temperatures bring hot deals on winter gear',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/sarah-huckabee-sanders-flatly-rejects-jim-acostas-asser-1825902692',\n",
       "  'headline': \"sarah huckabee sanders flatly rejects jim acosta's assertion that he's jim acosta\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/reno-orders-investigation-of-u-s-department-of-corrupt-1819565316',\n",
       "  'headline': 'reno orders investigation of u.s. department of corruption',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/billions-of-electric-signals-between-neurons-allow-brai-1819576197',\n",
       "  'headline': 'billions of electric signals between neurons allow brain to imagine what michael imperioli looks like',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/lucy-jones-self-portraits_n_7153684.html',\n",
       "  'headline': 'unapologetic self-portraits that shatter perceptions of disability',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/scientists-discover-eating-serves-function-other-than-e-1819577590',\n",
       "  'headline': 'scientists discover eating serves function other than easing anxiety',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/libraries-burning-from-sarajevo-mosul_b_6749898.html',\n",
       "  'headline': 'libraries burning: from sarajevo to mosul',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/what-i-learned-from-searching-for-soulmate_us_576180afe4b0df4d586ece5b',\n",
       "  'headline': 'what i realized after years of searching for my soulmate',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/astoria-characters-the-ch_b_5262569.html',\n",
       "  'headline': 'astoria characters: the charity stager',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/dolby-theatre-usher-throws-out-matt-damon-for-attemptin-1819579675',\n",
       "  'headline': 'dolby theatre usher throws out matt damon for attempting to film oscars with camcorder',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/600-pound-butter-cow-sculpture-wins-iowa-caucus-1819573170',\n",
       "  'headline': '600-pound butter cow sculpture wins iowa caucus',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/bill-o-reilly-tearfully-packs-up-framed-up-skirt-photos-1819579856',\n",
       "  'headline': \"bill o'reilly tearfully packs up framed up-skirt photos from desk\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/mark-green-gay-gestapo_us_591214f7e4b0a58297e01bd0',\n",
       "  'headline': \"trump's army secretary pick is victim of 'gay gestapo,' right wing activists claim\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://politics.theonion.com/candidates-preparing-for-colorado-debate-conditions-wit-1819578385',\n",
       "  'headline': 'candidates preparing for colorado debate conditions with high-altitude speaking drills',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/the-drama-desks-all-the-w_b_5428063.html',\n",
       "  'headline': \"the drama desks, all the way, m&m's and more\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/magazine-editor-undergoes-sleek-new-redesign-1819568739',\n",
       "  'headline': 'magazine editor undergoes sleek new redesign',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/town-nervously-welcomes-veteran-back-home-1819575479',\n",
       "  'headline': 'town nervously welcomes veteran back home',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/local-student-also-a-poet-1819586162',\n",
       "  'headline': 'local student also a poet',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/its-a-wonderful-life-censored_n_6375120.html',\n",
       "  'headline': \"'it's a wonderful life' was almost too racy for theaters\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/ariana-grande-snl_n_5895624.html',\n",
       "  'headline': \"ariana grande performs 'break free' on 'snl'\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://local.theonion.com/unconditional-love-given-to-15-year-old-who-just-called-1819580304',\n",
       "  'headline': 'unconditional love given to 15-year-old who just called mom a bitch in middle of hollister',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/newborn-prince-of-cambridge-begins-consolidating-power-1825477519',\n",
       "  'headline': 'newborn prince of cambridge begins consolidating power by having family imprisoned in tower of london',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/congress-abandons-wikiconstitution-1819568036',\n",
       "  'headline': 'congress abandons wikiconstitution',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/2016-all-about-the-electorate_b_6449000.html',\n",
       "  'headline': '2016: all about the electorate',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/self-defense-instructor-keeps-a-couple-of-secrets-to-hi-1819568286',\n",
       "  'headline': 'self-defense instructor keeps a couple of secrets to himself',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://local.theonion.com/adorable-puppy-nets-owner-handjob-1819563966',\n",
       "  'headline': 'adorable puppy nets owner handjob',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/can-we-whine-for-a-moment-judgement-free_us_55f06e50e4b002d5c0779abd',\n",
       "  'headline': \"10 super annoying things we can't help but whine about\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/man-bitten-by-radioactive-sloth-does-the-lying-around-a-1819566377',\n",
       "  'headline': 'man bitten by radioactive sloth does the lying-around-all-day of 10 normal men',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://entertainment.theonion.com/quake-claims-500-hours-1819565271',\n",
       "  'headline': 'quake claims 500 hours',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/teen-football-recruit-makes-bold-statement-about-black-lives-at-training-camp_us_594d51f5e4b05c37bb762a0e',\n",
       "  'headline': 'teen football recruit makes bold statement about black lives at training camp',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/10-of-illinois-safest-cit_b_6970346.html',\n",
       "  'headline': \"10 of illinois' safest cities\",\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.theonion.com/unconsciousness-faked-to-make-anesthesiologist-feel-bet-1819588655',\n",
       "  'headline': 'unconsciousness faked to make anesthesiologist feel better',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://politics.theonion.com/trump-hails-gorsuch-as-fierce-protector-of-future-amend-1819579577',\n",
       "  'headline': 'trump hails gorsuch as fierce protector of future amendment allowing president to temporarily suspend right to assemble',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/historical-archives-two-feared-dead-in-near-by-child-b-1819570249',\n",
       "  'headline': 'historical archives: two feared dead in near-by child-birth',\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.theonion.com/prison-warden-vows-to-take-away-el-chapo-s-tunnel-privi-1819577998',\n",
       "  'headline': \"prison warden vows to take away el chapo's tunnel privileges if captured\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/mothers-precious-and-misu_b_7249750.html',\n",
       "  'headline': 'mothers, precious and misunderstood: the many mothers i have met',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://entertainment.theonion.com/new-baby-weinstein-tapes-prepare-infants-for-career-in-1819568631',\n",
       "  'headline': \"new 'baby weinstein' tapes prepare infants for career in entertainment law\",\n",
       "  'is_sarcastic': 1},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/5-steps-to-get-you-from-shy-to-sociable_b_5807752.html',\n",
       "  'headline': '5 steps to get you from shy to sociable',\n",
       "  'is_sarcastic': 0},\n",
       " {'article_link': 'https://www.huffingtonpost.com/entry/who-were-you-on-911_b_11956800.html',\n",
       "  'headline': 'who were you on 9/11?',\n",
       "  'is_sarcastic': 0},\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "data = []\n",
    "with open(\"Sarcasm_Headlines_Dataset.json\", \"r\") as file:\n",
    "    for line in file: \n",
    "        data.append(json.loads(line))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8571d3aa-b859-4668-859a-e5ea048f211e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/american-...</td>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/americas-...</td>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/reparatio...</td>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/israeli-b...</td>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/gourmet-g...</td>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "0      https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1      https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2      https://local.theonion.com/mom-starting-to-fea...   \n",
       "3      https://politics.theonion.com/boehner-just-wan...   \n",
       "4      https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "...                                                  ...   \n",
       "26704  https://www.huffingtonpost.com/entry/american-...   \n",
       "26705  https://www.huffingtonpost.com/entry/americas-...   \n",
       "26706  https://www.huffingtonpost.com/entry/reparatio...   \n",
       "26707  https://www.huffingtonpost.com/entry/israeli-b...   \n",
       "26708  https://www.huffingtonpost.com/entry/gourmet-g...   \n",
       "\n",
       "                                                headline  is_sarcastic  \n",
       "0      former versace store clerk sues over secret 'b...             0  \n",
       "1      the 'roseanne' revival catches up to our thorn...             0  \n",
       "2      mom starting to fear son's web series closest ...             1  \n",
       "3      boehner just wants wife to listen, not come up...             1  \n",
       "4      j.k. rowling wishes snape happy birthday in th...             0  \n",
       "...                                                  ...           ...  \n",
       "26704               american politics in moral free-fall             0  \n",
       "26705                            america's best 20 hikes             0  \n",
       "26706                              reparations and obama             0  \n",
       "26707  israeli ban targeting boycott supporters raise...             0  \n",
       "26708                  gourmet gifts for the foodie 2014             0  \n",
       "\n",
       "[26709 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f99134-a480-4cf6-9259-f652825d852c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sarcastic\n",
       "0    0.561047\n",
       "1    0.438953\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking class balance how many sarcastic vs not \n",
    "df[\"is_sarcastic\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e653f32-9e6f-4876-8413-74d21e00ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"source\"] = \"news_headline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65f9b60-1397-40ed-aff8-53ed9ab000af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"hf://datasets/nikesh66/Sarcasm-dataset/sarcasm_tweets.csv\")\n",
    "df1[\"Sarcasm (yes/no)\"] = df1[\"Sarcasm (yes/no)\"].map({'yes':1, 'no':0})\n",
    "df1[\"source\"] = \"twitter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450b3369-77e7-493f-9d6b-03ddba25d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"train.En.csv\")\n",
    "df2[\"source\"] = \"twitter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5a8279-25a6-4079-bfc9-9db3a094a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "df3[\"source\"] = \"Reddit\"\n",
    "df4 = pd.read_csv(\"task_A_En_test.csv\")\n",
    "df4[\"source\"] = \"twitter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c098ee7-eea6-4a12-98fe-c7073be958f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047398</th>\n",
       "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047399</th>\n",
       "      <td>Omg how an earth is that a pen !!! 🤡</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047400</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047401</th>\n",
       "      <td>I love it when women are referred to as \"girl ...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047402</th>\n",
       "      <td>The fact that people still don't get that you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1047403 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  label  \\\n",
       "0        former versace store clerk sues over secret 'b...      0   \n",
       "1        the 'roseanne' revival catches up to our thorn...      0   \n",
       "2        mom starting to fear son's web series closest ...      1   \n",
       "3        boehner just wants wife to listen, not come up...      1   \n",
       "4        j.k. rowling wishes snape happy birthday in th...      0   \n",
       "...                                                    ...    ...   \n",
       "1047398  I’ve just seen this and felt it deserved a Ret...      0   \n",
       "1047399               Omg how an earth is that a pen !!! 🤡      0   \n",
       "1047400          Bringing Kanye and drake to a tl near you      0   \n",
       "1047401  I love it when women are referred to as \"girl ...      1   \n",
       "1047402  The fact that people still don't get that you ...      1   \n",
       "\n",
       "                source  \n",
       "0        news_headline  \n",
       "1        news_headline  \n",
       "2        news_headline  \n",
       "3        news_headline  \n",
       "4        news_headline  \n",
       "...                ...  \n",
       "1047398        twitter  \n",
       "1047399        twitter  \n",
       "1047400        twitter  \n",
       "1047401        twitter  \n",
       "1047402        twitter  \n",
       "\n",
       "[1047403 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select and rename columns consistently (including source)\n",
    "df1_sub = df[['headline', 'is_sarcastic', 'source']].rename(columns={'headline': 'text', 'is_sarcastic': 'label'})\n",
    "df2_sub = df1[['Tweet', 'Sarcasm (yes/no)', 'source']].rename(columns={'Tweet': 'text', 'Sarcasm (yes/no)': 'label'})\n",
    "df3_sub = df2[['tweet', 'sarcastic', 'source']].rename(columns={'tweet': 'text', 'sarcastic': 'label'})\n",
    "df4_sub = df3[['comment', 'label', 'source']].rename(columns={'comment': 'text', 'label': 'label'})\n",
    "df5_sub = df4[['text', 'sarcastic', 'source']].rename(columns={'text': 'text', 'sarcastic': 'label'})\n",
    "\n",
    "# combine all vertically\n",
    "df_full = pd.concat([df1_sub, df2_sub, df3_sub, df4_sub, df5_sub], axis=0).reset_index(drop=True)\n",
    "df_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e03a1600-a1c0-4d87-8cb5-9cb0a55abeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_text = df_full[df_full.duplicated(subset = 'text', keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bb3d5af-a0f5-4f08-8268-7534dc4563ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicated rows (text-wise): 66249\n"
     ]
    }
   ],
   "source": [
    "num_dup = dup_text.shape[0]\n",
    "print(f\"Total duplicated rows (text-wise): {num_dup}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f663ce0f-10af-4f01-ac25-77b8675b3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_clean = df_full.drop_duplicates(subset = 'text', keep = 'first').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ad4658b-b013-4658-b2e9-c5070d69469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'#flintstones': 444, '#1': 383, '#3232': 183, '#2': 128, '#congaparrot': 56, '#3': 49, '#i': 43, '#4': 40, '#s': 34, '#triggered': 33, '#rekt': 32, '#5': 27, '#you': 26, '#the': 25, '#we': 25, '#this': 20, '#6': 20, '#blacklivesmatter': 19, '#7': 19, '#10': 18, '#yolo': 18, '#notallmen': 15, '#9': 15, '#he': 14, '#feelthebern': 12, '#it': 12, '#no': 12, '#can': 12, '#8': 11, '#notallmuslims': 11, '#that': 11, '#but': 11, '#based': 10, '#a': 10, '#11': 10, '#notbob': 10, '#nofilter': 9, '#12': 9, '#d': 9, '#e': 9, '#notop': 9, '#metoo': 8, '#trump': 8, '#blessed': 8, '#blm': 8, '#racist': 8, '#oh': 8, '#fuck': 8, '#b': 8, '#what': 8, '#define': 8, '#pcmasterrace': 8, '#gamergate': 8, '#yes': 7, '#spoiler': 7, '#g': 7, '#so': 7, '#t': 7, '#get': 7, '#kappa': 7, '#r': 7, '#and': 7, '#k': 7, '#hashtag': 7, '#swag': 7, '#america': 6, '#wow': 6, '#firstworldproblems': 6, '#18': 6, '#neverforget': 6, '#13': 6, '#17': 6, '#f': 6, '#o': 6, '#don': 6, '#0': 6, '#free': 6, '#whitelivesmatter': 6, '#mods': 6, '#h': 6, '#trump2016': 6, '#20': 6, '#haes': 6, '#bbccricket': 5, '#23': 5, '#facts': 5, '#bluelivesmatter': 5, '#to': 5, '#imwithher': 5, '#how': 5, '#stop': 5, '#fire': 5, '#your': 5, '#killallmen': 5, '#19': 5, '#balanced': 5, '#m': 5, '#make': 5, '#relationshipgoals': 5, '#face': 5, '#my': 5, '#millionsofcopies': 5, '#maga': 4, '#brexit': 4, '#cricket': 4, '#ghostoftsushima': 4, '#pizzagate': 4, '#wew': 4, '#lol': 4, '#dicksoutforharambe': 4, '#notallwomen': 4, '#she': 4, '#fucking': 4, '#thank': 4, '#all': 4, '#alllivesmatter': 4, '#please': 4, '#not': 4, '#where': 4, '#when': 4, '#35': 4, '#n': 4, '#24': 4, '#99': 4, '#meta': 4, '#goals': 3, '#wtf1': 3, '#loveisland': 3, '#nffc': 3, '#lineofduty': 3, '#indveng': 3, '#aewallout': 3, '#ps5share': 3, '#art': 3, '#is': 3, '#sorrynotsorry': 3, '#bill': 3, '#praise': 3, '#if': 3, '#well': 3, '#cunt': 3, '#u': 3, '#never': 3, '#god': 3, '#dae': 3, '#believe': 3, '#draintheswamp': 3, '#yeah': 3, '#intel': 3, '#holy': 3, '#sexist': 3, '#now': 3, '#are': 3, '#legend': 3, '#those': 3, '#healthy': 3, '#c': 3, '#damn': 3, '#ted': 3, '#they': 3, '#wutface': 3, '#14': 3, '#15': 3, '#rip': 3, '#just': 3, '#thuglife': 3, '#logic': 3, '#high': 3, '#37': 3, '#cruzmissile': 3, '#effyourbeautystandards': 3, '#usa': 3, '#27': 3, '#30': 3, '#wengerout': 3, '#winning': 3, '#consolemasterrace': 3, '#lewd': 3, '#jesus': 3, '#handsupdontshoot': 3, '#approved': 3, '#icantbreathe': 2, '#talktome': 2, '#notme': 2, '#nevertrump': 2, '#squadgoals': 2, '#oscarssowhite': 2, '#maga2020': 2, '#hokies': 2, '#ncfc': 2, '#covid_19': 2, '#evermorealbum': 2, '#prs2021': 2, '#nancy': 2, '#fabricreview': 2, '#engvsind': 2, '#belgiangp': 2, '#football': 2, '#nfl': 2, '#thanksgiving': 2, '#askingforafriend': 2, '#eurovision': 2, '#coronavirus': 2, '#bidenharris2020': 2, '#unitedforgood': 2, '#fail': 2, '#bbcfootball': 2, '#pride2020': 2, '#snooker': 2, '#econtwitter': 2, '#why': 2, '#disappointed': 2, '#catholic': 2, '#birthday': 2, '#eng': 2, '#holbycity': 2, '#tech': 2, '#matthanock': 2, '#engvind': 2, '#gogglebox': 2, '#olympics': 2, '#socialdistancing': 2, '#money': 2, '#asshole': 2, '#themoreyouknow': 2, '#kids': 2, '#nomorelockdowns': 2, '#til': 2, '#farmersstandingfirm': 2, '#vi': 2, '#currentyear': 2, '#confirmed': 2, '#grit': 2, '#look': 2, '#wrong': 2, '#great': 2, '#exposed': 2, '#shebelongs': 2, '#p': 2, '#amen': 2, '#meme': 2, '#drain': 2, '#spoilers': 2, '#lockherup': 2, '#best': 2, '#did': 2, '#over': 2, '#y': 2, '#fe': 2, '#who': 2, '#head': 2, '#reddit': 2, '#such': 2, '#x': 2, '#denied': 2, '#rigged': 2, '#first': 2, '#whinylittlebitch': 2, '#su': 2, '#wakeupsheeple': 2, '#casual': 2, '#400': 2, '#woke': 2, '#kill': 2, '#had': 2, '#pathetic': 2, '#trumpforprison2016': 2, '#btfo': 2, '#notmypope': 2, '#100': 2, '#op': 2, '#educate': 2, '#feelsbadman': 2, '#gitgud': 2, '#feelthejohnson': 2, '#21': 2, '#doublestandards': 2, '#illuminati': 2, '#downvoted': 2, '#demexit': 2, '#lul': 2, '#more': 2, '#ayy': 2, '#ota': 2, '#olicity': 2, '#progress': 2, '#will': 2, '#marp': 2, '#low': 2, '#white': 2, '#tim': 2, '#only': 2, '#refugeeswelcome': 2, '#000000': 2, '#triggerwarning': 2, '#roasted': 2, '#basta': 2, '#notlikethis': 2, '#w': 2, '#doesn': 2, '#feminism': 2, '#bigdickproblems': 2, '#16': 2, '#oscarsowhite': 2, '#nimble': 2, '#kreygasm': 2, '#notyourshield': 2, '#facepalm': 2, '#burn': 2, '#something': 2, '#sexism': 2, '#packyourshit': 2, '#literally': 2, '#science': 2, '#gt': 2, '#omg': 2, '#ha': 2, '#32': 2, '#38': 2, '#160': 2, '#welcome': 2, '#29': 2, '#gg': 2, '#ravenholdtorriot': 2, '#ayyy': 2, '#420': 2, '#guys': 2, '#do': 2, '#very': 2, '#of': 2, '#vegan': 2, '#worth': 2, '#respect': 2, '#qb': 2, '#trusttheprocess': 2, '#itt': 2, '#78': 2, '#thanksobama': 2, '#heil': 2, '#freedom': 2, '#notallchristians': 2, '#feelinthebern': 2, '#2starswinwars': 2, '#56': 2, '#1s': 2, '#thereturn': 2, '#fuckbullies': 2, '#dipmad': 2, '#for': 2, '#hire': 2, '#obama': 2, '#yosted': 2, '#realtalk': 2, '#basicincome': 2, '#topbantz': 2, '#cheatriots': 2, '#trihard': 2, '#choate': 2, '#dinosaurlivesmatter': 2, '#misandry': 2, '#masterrace': 2, '#blazeit': 2, '#ps4masterrace': 2, '#talk': 2, '#shotsfired': 2, '#creepingsharia': 2, '#2edgy4me': 2, '#pcmrproblems': 2, '#pottoblame': 2, '#hipster': 2, '#54': 2, '#solidarity': 2, '#42': 2, '#9835': 2, '#atheist': 2, '#lvgout': 2, '#34': 2, '#pie': 2, '#shrekt': 2, '#3720': 2, '#boobs': 2, '#40': 2, '#there': 2, '#146': 2, '#holo': 2, '#xmasgiftsfromtrump': 1, '#brownribboncampaign': 1, '#trybeatingmelightly': 1, '#dirtydenier': 1, '#kellyonmymind': 1, '#ridiculousexcusestostayhome': 1, '#prayforpaulgeorge': 1, '#starwarschristmascarols': 1, '#trickortreatin100years': 1, '#trumphair': 1, '#womenboycotttwitter': 1, '#trumpbacktoschooltips': 1, '#alternativefacts': 1, '#mewesyria': 1, '#addcandytoamovie': 1, '#gopsongsaboutethics': 1, '#badpicturemonday': 1, '#teamlogan': 1, '#teamnocancer': 1, '#obamaandkids': 1, '#dropthecover': 1, '#hscc2015': 1, '#napaquake': 1, '#napastrong': 1, '#digitalhealth': 1, '#alohahuffpost': 1, '#surfbort': 1, '#nobannowall': 1, '#millionsmarchsf': 1, '#emojisinthewild': 1, '#staywokeandvote': 1, '#11717': 1, '#iphonefeatures4politicians': 1, '#myroommateisweird': 1, '#youlookdisgusting': 1, '#nspw2017': 1, '#ionceoverheard': 1, '#explainthe90sin4words': 1, '#dearbetsy': 1, '#friendshipgoals': 1, '#missuniverse2015': 1, '#addclimatechangetotv': 1, '#trumpacandy': 1, '#middleagedschmovies': 1, '#thereisaidit': 1, '#dadlife': 1, '#meat14': 1, '#thrive': 1, '#menforchoice': 1, '#donaldtrump': 1, '#imaceleb': 1, '#mbe': 1, '#ruinedchristmas': 1, '#euref': 1, '#findom': 1, '#murderhornets': 1, '#sweatingmyballsoff': 1, '#bollocks': 1, '#sarcastic': 1, '#schoolclosure': 1, '#r4bh': 1, '#istillhaveexams': 1, '#tryingtosleep': 1, '#metaxa': 1, '#thankstotrump': 1, '#valentinesday2021': 1, '#wd40': 1, '#dailybriefing': 1, '#covidisnotover': 1, '#karmaisabitch': 1, '#antitrusthearings': 1, '#apple': 1, '#housejudiciary': 1, '#happybirthdaykeanureeves': 1, '#sports': 1, '#bbcqt': 1, '#quarantinelife': 1, '#fancyamcdonalds': 1, '#ivoryrooms': 1, '#seriously': 1, '#unibowl': 1, '#bbcbodyclock': 1, '#lineofduty6': 1, '#lod6': 1, '#lod': 1, '#spoileralert': 1, '#freezing': 1, '#impeachmentday': 1, '#trumpimpeachment2': 1, '#wycopride': 1, '#yeeyee': 1, '#oktotry': 1, '#gotrump': 1, '#notoeuropeansuperleague': 1, '#coronalockdownuk': 1, '#summerholidays': 1, '#happyvalentinesday': 1, '#thismorning': 1, '#alexa': 1, '#towie': 1, '#coronavirusupdate': 1, '#konami': 1, '#pt': 1, '#silenthills': 1, '#spooky': 1, '#playableteaser': 1, '#kojima': 1, '#76ers': 1, '#atlvsphi': 1, '#hawks': 1, '#nbaplayoffs': 1, '#bez': 1, '#weak': 1, '#testosteronedeficiency': 1, '#efficiency': 1, '#chanukah': 1, '#doughnuts': 1, '#justsaying': 1, '#opportunity': 1, '#mondays': 1, '#debate2020': 1, '#taesup': 1, '#presentation': 1, '#healtheconomics': 1, '#thecrown': 1, '#edexcelbiology': 1, '#gcses2018': 1, '#madrespect': 1, '#iwishwehadallbeenready': 1, '#intothespiderverse': 1, '#twitchtrolls': 1, '#coronavirusuk': 1, '#painful': 1, '#sad': 1, '#supremecourtofindia': 1, '#trumpisanidiot': 1, '#studentathlete': 1, '#trumppressbriefing': 1, '#wewantarefund': 1, '#ranbooemail': 1, '#allbirthdaysmatter': 1, '#lilleejean': 1, '#goptraitors': 1, '#chitty': 1, '#help': 1, '#postanunpopularfoodopinion': 1, '#thegrandtour': 1, '#loudbang': 1, '#eurovisionagain': 1, '#crap': 1, '#samsung': 1, '#w100': 1, '#socceraid': 1, '#dominiccummings': 1, '#euro2016': 1, '#myenglandxi': 1, '#capitalonecup': 1, '#iamaurora': 1, '#skyf1': 1, '#idenainvite': 1, '#20yearsofblue': 1, '#cnoh3': 1, '#imaginarygrandprix': 1, '#clickbaitnetflix': 1, '#nakedformonstax': 1, '#wweraw': 1, '#jwufambingo': 1, '#universalcredit': 1, '#eurofinal': 1, '#euro2021': 1, '#rammstein': 1, '#magic': 1, '#thicc': 1, '#brexitshambles': 1, '#ww111': 1, '#grav3yardgirl': 1, '#newbie': 1, '#woooooo': 1, '#psychtoolbox': 1, '#academictwitter': 1, '#beetwitter': 1, '#ecologytwitter': 1, '#avfc': 1, '#avlars': 1, '#pride': 1, '#spacex': 1, '#us': 1, '#refugeesdetained': 1, '#rt': 1, '#potus': 1, '#greed': 1, '#covidiots': 1, '#antivaxxers': 1, '#appleevent': 1, '#togetherwe': 1, '#stanggang': 1, '#allstarmusicals': 1, '#masks': 1, '#ufc': 1, '#fightnight': 1, '#creeper': 1, '#gigs': 1, '#excited': 1, '#welshopen': 1, '#girlboss': 1, '#needabreak': 1, '#voteleave': 1, '#homewreckermovie': 1, '#grimmfest': 1, '#lebrock': 1, '#billcosby': 1, '#wtf': 1, '#me2': 1, '#fatshaming': 1, '#clothessizing': 1, '#dobetter': 1, '#jeans': 1, '#linkinparkforlife': 1, '#chesterbennington': 1, '#happybirthday': 1, '#linkinpark': 1, '#suicideprevention': 1, '#wewillalwaysloveyou': 1, '#finance': 1, '#catholicism': 1, '#covid19': 1, '#eastersunday': 1, '#billions': 1, '#corona': 1, '#etsy': 1, '#beige': 1, '#bedroom': 1, '#cotton': 1, '#polyester': 1, '#throwp': 1, '#cats': 1, '#catpillow': 1, '#decorativepillow': 1, '#catcushion': 1, '#petpillow': 1, '#throwpillow': 1, '#kittypillow': 1, '#catlovergift': 1, '#homemade': 1, '#ultimatemombrain': 1, '#alsoimapoorteacher': 1, '#enthusiast': 1, '#eattherich': 1, '#iswareimnotobsessedwithcamp': 1, '#reputation': 1, '#didicomplainaboutitthatmuch': 1, '#nashville': 1, '#baileysfantasycamptn': 1, '#fieldofdreamsgame': 1, '#notts': 1, '#raceforlife': 1, '#donate': 1, '#snowden': 1, '#worldwar3': 1, '#australiafires': 1, '#justfloridathings': 1, '#hurricane': 1, '#larry': 1, '#wxgeek': 1, '#wxtwitter': 1, '#superbowl': 1, '#superbowlhalftimeshow': 1, '#depop': 1, '#embroidery': 1, '#totebags': 1, '#tunafortuna': 1, '#whowantsasuperleague': 1, '#sciencejobs': 1, '#researchjobs': 1, '#postdocjobs': 1, '#graduatejobs': 1, '#animalbehaviour': 1, '#universityofportsmouth': 1, '#zebrafish': 1, '#gladeplugins': 1, '#glade': 1, '#gotitfree': 1, '#bonuspoints': 1, '#godsown': 1, '#england': 1, '#bbc': 1, '#postoffice': 1, '#bbcpn': 1, '#orphanblack': 1, '#election': 1, '#txsenatedebate': 1, '#lyinted': 1, '#losecruz': 1, '#pantsonfire': 1, '#womancentredcare': 1, '#allthethanksineed': 1, '#mcflyallaboutus': 1, '#universitychallenge': 1, '#triedforless': 1, '#softwaredevelopment': 1, '#webdevelopment': 1, '#motivated': 1, '#businesses': 1, '#podcasts': 1, '#starwars': 1, '#didyouknow': 1, '#nationalfoodstrategy': 1, '#gamer': 1, '#talesfromtheloop': 1, '#paulritter': 1, '#lineofdutyfinale': 1, '#bringbacktoonami': 1, '#complimentary': 1, '#halloween2019': 1, '#chrislowe': 1, '#happybirthdaychrislowe': 1, '#petshopboys': 1, '#pokemoncenterlondon': 1, '#pokemoncentrelondon': 1, '#disco': 1, '#hbomax': 1, '#mainstreetrenewal': 1, '#womensequalityday': 1, '#nationaldogday': 1, '#ourfirstfeditm': 1, '#bettywhite': 1, '#jamesearljones': 1, '#formula1': 1, '#anniversary': 1, '#happy': 1, '#mixedmedia': 1, '#steampunk': 1, '#myart': 1, '#ufc200': 1, '#therepairshop': 1, '#bromley': 1, '#wembleystadium': 1, '#fleetwoodmac': 1, '#leicesterlockdown': 1, '#tedtalks': 1, '#yesday': 1, '#bodyguard': 1, '#freebritney': 1, '#goheels': 1, '#indvsend': 1, '#3rdtest': 1, '#jmc4412': 1, '#peppapig': 1, '#firstdayofschool': 1, '#hulk': 1, '#thecircle': 1, '#lateishshow': 1, '#iamconfusion': 1, '#snl': 1, '#shopsmall': 1, '#bloggersrequired': 1, '#whitepeoplelovesaying': 1, '#kkw2022': 1, '#littlehouseontheprairie': 1, '#bbcolympics': 1, '#euro2020': 1, '#ratchetclankriftapart': 1, '#amazonwishlists': 1, '#sotrue': 1, '#momproblems': 1, '#letsmakeitawkward': 1, '#carolineflack': 1, '#suicide': 1, '#thisismycrew': 1, '#stimuluscheck': 1, '#ripjeffbezos': 1, '#instagramreels': 1, '#schoolsreopening': 1, '#flyingbeast': 1, '#robinhood': 1, '#robinhoodtraders': 1, '#covid__19': 1, '#mask': 1, '#daytrading': 1, '#ddtg': 1, '#mafsuk': 1, '#raducanu': 1, '#ripcharliewatts': 1, '#charliewatts': 1, '#deathmetal': 1, '#cupcakeatm': 1, '#really': 1, '#ealingbroadway': 1, '#nikehalfmarathontrainingplan': 1, '#week12complete': 1, '#nofilterneeded': 1, '#naturephotography': 1, '#mothernatureneedsnofilter': 1, '#brokenangel': 1, '#itsokaynottobeokay': 1, '#king810': 1, '#goldorchocolate': 1, '#iarr2021': 1, '#psypag2021': 1, '#narcissism': 1, '#lokiepisode2': 1, '#disney': 1, '#marvel': 1, '#soundstoriessoundwages': 1, '#coyb': 1, '#wildlife': 1, '#shielding': 1, '#itfc': 1, '#manchestercity': 1, '#messi': 1, '#manchester': 1, '#poster': 1, '#desperatehousewives': 1, '#moveon': 1, '#bekind': 1, '#inspirationalquote': 1, '#sotalented': 1, '#sprinkled': 1, '#delitteruk': 1, '#coys': 1, '#totmci': 1, '#f1': 1, '#olilogic': 1, '#myjobisdonehere': 1, '#cancersucks': 1, '#3yearsdown': 1, '#blacklove': 1, '#michigan': 1, '#notredame': 1, '#xfinity': 1, '#et': 1, '#holiday': 1, '#jeopardygoat': 1, '#ffxiv': 1, '#wwe': 1, '#iloveyou': 1, '#idm2021': 1, '#idliketogetpaidfor': 1, '#wfh': 1, '#bunnies': 1, '#rabbits': 1, '#pets': 1, '#rallythevalley': 1, '#100t': 1, '#lcs': 1, '#sopranos': 1, '#food': 1, '#newworldmmo': 1, '#themanysaintsofnewark': 1, '#toonami': 1, '#rhainil': 1, '#twill': 1, '#tbcresolutionremix': 1, '#boycotthamilton': 1, '#joanneworldtour': 1, '#adultquestions': 1, '#election2016': 1, '#shecrazy': 1, '#needstogointhehome': 1, '#piersmorgan': 1, '#thereceiptspodcast': 1, '#gbbo': 1, '#masterchef': 1, '#federervsdjokovic': 1, '#killjoys': 1, '#fgm': 1, '#agenttechaway': 1, '#afterlife': 1, '#shocked': 1, '#streamstories': 1, '#twitchstreamer': 1, '#untildawn': 1, '#scared': 1, '#twitch': 1, '#debates2020': 1, '#igotoutofmypjstoday': 1, '#internationalselfcareday': 1, '#tuesdaythoughts': 1, '#followthewheeler': 1, '#dcprotests': 1, '#education': 1, '#byuprobs': 1, '#euro2020final': 1, '#lost': 1, '#disneyplusstar': 1, '#showerthoughts': 1, '#bbchospital': 1, '#muspartans': 1, '#30weds21': 1, '#farmersprotest': 1, '#farmersprotests': 1, '#fb': 1, '#greetingscard': 1, '#easter': 1, '#bignightin': 1, '#fancydressfriday': 1, '#pewithjoe': 1, '#algorand': 1, '#byebye': 1, '#jackstack': 1, '#raducanufernandez': 1, '#usopen2021': 1, '#thebachelor': 1, '#marchforourlives': 1, '#whatwouldoliviapopedo': 1, '#malignant': 1, '#wetlandheroes': 1, '#familyfun': 1, '#visitgloucestershire': 1, '#whatislife': 1, '#wahhh24isold': 1, '#isitsummeryet': 1, '#smokingcat': 1, '#coolestvestever': 1, '#europeansuperleague': 1, '#lufc': 1, '#whoknew': 1, '#bbcsnooker': 1, '#thechase': 1, '#fozzy': 1, '#growingupwithmyname': 1, '#minnvsbama': 1, '#gcseresults2021': 1, '#gcseresults': 1, '#montypythonesquedisses': 1, '#sportscliche': 1, '#literallyhitler': 1, '#circlejerk': 1, '#teampitt': 1, '#256': 1, '#hillareeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee': 1, '#allahu': 1, '#349': 1, '#1758': 1, '#conversion': 1, '#dicksoutforitt': 1, '#somethingnew': 1, '#herturn': 1, '#sigiin': 1, '#caitlyn': 1, '#yuri': 1, '#oppai': 1, '#obamafoundedisis': 1, '#self': 1, '#neverforgetti': 1, '#weekend': 1, '#bernie': 1, '#itjustworks': 1, '#4head': 1, '#balance': 1, '#pool': 1, '#healthatanysize': 1, '#allthingscontainmatter': 1, '#prayforharambe': 1, '#makediegocapitano': 1, '#ing': 1, '#iron': 1, '#womensolazy': 1, '#dicksoutforchodestrangler': 1, '#ifyoucatchmydrift': 1, '#imwithzim': 1, '#kanye2020': 1, '#horrik': 1, '#wronggenerationproblems': 1, '#masc4mascara': 1, '#tibalt4lyfe': 1, '#animalabuse': 1, '#nevershillart': 1, '#hillmissile': 1, '#jiggshate': 1, '#edelman': 1, '#re': 1, '#goddamnit': 1, '#heswithher': 1, '#still': 1, '#also': 1, '#everyone': 1, '#2edgy4allofus': 1, '#bestfansinbaseball': 1, '#americanotgreatanymore': 1, '#announcements': 1, '#unchainthechain': 1, '#ohyeah': 1, '#r3kt': 1, '#blame': 1, '#getswole': 1, '#doyouevenlift': 1, '#multidisplaymasterrace': 1, '#ripharambe': 1, '#cking': 1, '#vase': 1, '#1d': 1, '#whereisthebriefcase': 1, '#notmyjarl': 1, '#truth': 1, '#suckmydickcommies': 1, '#amd': 1, '#saveisis': 1, '#fuckthenucks': 1, '#mvp': 1, '#justubisoftthings': 1, '#notmyhalflife': 1, '#slick': 1, '#poo': 1, '#al': 1, '#yup': 1, '#ascend': 1, '#botlivesmatter': 1, '#somebody': 1, '#maclivesmatter': 1, '#voterfraud': 1, '#ayes': 1, '#calmmedownin3words': 1, '#silence': 1, '#avlogic': 1, '#nothingtodowithislam': 1, '#doyourjob': 1, '#overpaid': 1, '#giant': 1, '#feel': 1, '#black': 1, '#allbadthingsmatter': 1, '#herbullies': 1, '#hillbullies': 1, '#socialjusticebullies': 1, '#socialjusticeterror': 1, '#soedgyithurts': 1, '#sweepgang': 1, '#rootsbloodyroots': 1, '#fweespeech': 1, '#fellow': 1, '#triiiiiiiggered': 1, '#trex': 1, '#teamdani': 1, '#bbott': 1, '#submission': 1, '#closeenough': 1, '#69': 1, '#censorship': 1, '#toodiversefome': 1, '#chickenlover': 1, '#worstlifeever': 1, '#mum': 1, '#news1mple': 1, '#nebrasketball': 1, '#sowhite': 1, '#quiet': 1, '#lugaw': 1, '#bananaslamma': 1, '#freshspawnbuffneededtohappen': 1, '#nts': 1, '#godmustexist': 1, '#osu': 1, '#nothing': 1, '#notanyonespresident': 1, '#stolen': 1, '#prayforparis': 1, '#anythingispossible': 1, '#hes': 1, '#sedlak': 1, '#johneveralone': 1, '#noupvotebecausenogirl': 1, '#penaltypourlyon': 1, '#pinoypride': 1, '#healthyatanysize': 1, '#physics': 1, '#nbahistory': 1, '#liberally': 1, '#unshackled': 1, '#fuckthepuck': 1, '#thebluejacketsareactuallygreat': 1, '#bearbowthuglife': 1, '#soprod': 1, '#notmyelite4': 1, '#confident': 1, '#26': 1, '#naive': 1, '#fakenews': 1, '#aspception': 1, '#deerlivesmatter': 1, '#ba': 1, '#teehee': 1, '#dont': 1, '#call': 1, '#yuge': 1, '#proof': 1, '#iftrue': 1, '#alldicksmatter': 1, '#makeamericagrearagain': 1, '#reasonfortheseason': 1, '#madonline': 1, '#john': 1, '#tweeterinchief': 1, '#blind': 1, '#hongkongrep': 1, '#makebeerusgreatagain': 1, '#happyhalloween': 1, '#thewalkingdead': 1, '#tatawatch': 1, '#apropos': 1, '#reset': 1, '#yeeeeoooooooooo': 1, '#congrats': 1, '#otherwise': 1, '#tedredemption': 1, '#c__linton__': 1, '#n__ews__': 1, '#n__etwork__': 1, '#shes': 1, '#angrybrady2016': 1, '#doglivesmatter': 1, '#tavcock2018': 1, '#pink': 1, '#unique': 1, '#fuck2016': 1, '#design_space': 1, '#trash': 1, '#cucks': 1, '#thingsthatdidn': 1, '#madebygoogle': 1, '#donniethediddler': 1, '#98434232': 1, '#malik': 1, '#fuckfacevonclownstick': 1, '#notmygroundruledouble': 1, '#makelefttacklesgreatagain': 1, '#equalpayday': 1, '#courage': 1, '#safetyfirst': 1, '#ive': 1, '#merry': 1, '#ospreymasterrace': 1, '#tmw': 1, '#reeeeeeeeeeeeeee': 1, '#grenade': 1, '#shittytip': 1, '#thelist': 1, '#berniefan': 1, '#obamageddon': 1, '#kek': 1, '#firetorts': 1, '#leavenocardshafted': 1, '#give': 1, '#notallirish': 1, '#concise': 1, '#endhypeculture': 1, '#het': 1, '#ffffff': 1, '#pivot': 1, '#teddie': 1, '#teaminstinct': 1, '#iwokeuplikethis': 1, '#blessings': 1, '#brownlifematters': 1, '#slipgate': 1, '#weinersout': 1, '#toxicasianmasculinity': 1, '#asianpatriarchy': 1, '#ifjinglescanfindlove': 1, '#mei': 1, '#dylansbetter': 1, '#highpingfield1': 1, '#dickingbimbos': 1, '#till': 1, '#pmurt': 1, '#allnumbersmatter': 1, '#love': 1, '#pepe': 1, '#71': 1, '#drainthatswamp': 1, '#boycottjeffreyloria': 1, '#freestylelibremasterrace': 1, '#smhamirite': 1, '#insulted': 1, '#use': 1, '#hoovesup': 1, '#flythew': 1, '#ows': 1, '#commonsense': 1, '#pollz': 1, '#trumped': 1, '#merchant': 1, '#overruled': 1, '#bachlivesmatter': 1, '#freeassange': 1, '#neverforgetcharizard': 1, '#hillary': 1, '#airhorn': 1, '#dicksoutforx': 1, '#bigly': 1, '#patriots': 1, '#bifm': 1, '#currentvagina': 1, '#obligatory': 1, '#dishonest': 1, '#3dchess': 1, '#boring': 1, '#mandirwahinbanaenge': 1, '#minionlivesmatter': 1, '#maak': 1, '#sure': 1, '#makeamericaguacbowlsagain': 1, '#notanime': 1, '#boxes4frak': 1, '#novemberboys': 1, '#shall': 1, '#makeamericagreatagain': 1, '#ibpmasters': 1, '#trumpforpresident2016': 1, '#stopthepot': 1, '#law': 1, '#lovetrumpshat': 1, '#carlos': 1, '#poped': 1, '#disqualified': 1, '#wrekt': 1, '#china': 1, '#botham': 1, '#makepopheadsstraightagain': 1, '#blameponpon': 1, '#smells': 1, '#mad': 1, '#hedidthat': 1, '#kerrgate': 1, '#notyourpresident': 1, '#ripyohane2k16': 1, '#remoteplay': 1, '#religioniscancer': 1, '#juststormragethings': 1, '#erdoganvoiceoftheoppressed': 1, '#allwomenwereuneducatedmoronsbeforefeminism': 1, '#remember': 1, '#ljyr088': 1, '#notalleuropeans': 1, '#281': 1, '#take': 1, '#number': 1, '#manthewall': 1, '#johnsonrising': 1, '#butseriouslywhysomanymeniamoneandidontunderstandthis': 1, '#doucheandturdsandwich': 1, '#dirksfordays': 1, '#notmypresident': 1, '#datjuice': 1, '#massmurder': 1, '#vegantildeath': 1, '#hillary4hospice': 1, '#hardcore': 1, '#shopped': 1, '#orange': 1, '#xenolifesmatter': 1, '#neither': 1, '#shocking': 1, '#liberals': 1, '#soerectrightnow': 1, '#yelling': 1, '#shh': 1, '#hillaryducksjustice': 1, '#baller': 1, '#rapefugees': 1, '#2593': 1, '#firewolf': 1, '#petsofthemasterrace': 1, '#its': 1, '#bmgtechy': 1, '#sideofbeef': 1, '#thugdokla': 1, '#hillarydidnothingwrong': 1, '#makeblindpickgreat': 1, '#minecraft': 1, '#applehasended': 1, '#stevewouldnotdothat': 1, '#ih8mydonglelife': 1, '#200kperannuum': 1, '#iammovingtowindows': 1, '#sentfrommyiphone7': 1, '#checkyourattitude': 1, '#adapt': 1, '#explainfifamomentum': 1, '#badjournalism': 1, '#wake': 1, '#gapisclosing': 1, '#teamlh': 1, '#federal': 1, '#dicksoutforanokuu': 1, '#almost': 1, '#buildingtheswamp': 1, '#tacotuesday': 1, '#alsocoltsdefensethings': 1, '#pixelmasterrace': 1, '#starteirik': 1, '#demonic': 1, '#gotcha': 1, '#hottake': 1, '#ukraiseitup': 1, '#bikelife': 1, '#cancercelllivesmatter': 1, '#priorities': 1, '#nflrigged': 1, '#dyslexia': 1, '#thingshotshotggdoes': 1, '#dicksout': 1, '#shut': 1, '#participationmedal': 1, '#lastwinner': 1, '#broing': 1, '#eybs': 1, '#danker': 1, '#nsync': 1, '#backstreetboyzzzz': 1, '#387': 1, '#didnotedit': 1, '#throwupthex': 1, '#kd2dc': 1, '#voteforjames': 1, '#mean': 1, '#77003c': 1, '#626262': 1, '#2663': 1, '#juggforprison': 1, '#hard': 1, '#hurry': 1, '#culturalappropriation': 1, '#release': 1, '#mulletforsuperman': 1, '#failfish': 1, '#rad': 1, '#rank': 1, '#shoutyourabortion': 1, '#couched': 1, '#_': 1, '#googlecookies': 1, '#woman': 1, '#hakt': 1, '#sm4shduckhunt': 1, '#sm4shpit': 1, '#sm4shtoonlink': 1, '#chinosmirk': 1, '#harambe4life': 1, '#most': 1, '#warsaw': 1, '#moretrustedthanhillary': 1, '#nintendo': 1, '#fixmccwithhalo3anniversary': 1, '#stopveganprofiling': 1, '#theydontwantyoutowin': 1, '#banalltrucks': 1, '#88': 1, '#playerretention': 1, '#makenaserversgreatagain': 1, '#forharambe': 1, '#hillshill': 1, '#dicksoutforop': 1, '#manly': 1, '#riptom': 1, '#surprise': 1, '#some': 1, '#berned': 1, '#notalldalmations': 1, '#knickstape': 1, '#spaceelevatorsmatter': 1, '#berntheconvention': 1, '#patriarcy': 1, '#swordsoutforphyrra': 1, '#paranoidrcanadian': 1, '#eyebrowgameonfleek': 1, '#crookedtrump': 1, '#notalldogs': 1, '#dramayearofthecentury': 1, '#freebill': 1, '#out': 1, '#kyleraynerhypetrain': 1, '#babbling': 1, '#clinton': 1, '#hordelivesmatter': 1, '#nerfgoobis': 1, '#ifndef': 1, '#endif': 1, '#justbleed': 1, '#girl': 1, '#should': 1, '#dncleaks': 1, '#bratva': 1, '#deflategate': 1, '#gjallarambe': 1, '#braceyourself': 1, '#fuckyounotley': 1, '#givebaseballgunsagain': 1, '#sakhoout': 1, '#verysuccessful': 1, '#decriminalizeindecentexposure': 1, '#babylivesdontmatter': 1, '#bernieorbust': 1, '#russians': 1, '#be': 1, '#fakevet': 1, '#pleasepleaseplease': 1, '#makesunderlandgreatagain': 1, '#keepcardinalsgreat': 1, '#brexitbantz': 1, '#donthatepls': 1, '#manofsmiles': 1, '#prayforamiracle': 1, '#mysteriousways': 1, '#presidentcruz': 1, '#johnscottwwehof': 1, '#notallturks': 1, '#jailher': 1, '#honestpleadings': 1, '#etfo': 1, '#revokeplanets2016': 1, '#onlyinmontreal': 1, '#apparently': 1, '#rodgersforprison': 1, '#farage': 1, '#stancenation': 1, '#fairlivesmatter': 1, '#thedreamboys': 1, '#cloudsbroclouds': 1, '#wildlivesmatter': 1, '#breaking': 1, '#yaybrexit': 1, '#nasty': 1, '#digitalfictionalmonsterslivesmatter': 1, '#bringbackasquith': 1, '#digital': 1, '#basic': 1, '#pokemongotothepolls': 1, '#background': 1, '#secretwin': 1, '#entry': 1, '#gethyped': 1, '#secretnerd': 1, '#bringbackpolio': 1, '#mouout': 1, '#weneedtrump': 1, '#blackops3': 1, '#hey': 1, '#levelgains': 1, '#209': 1, '#ban': 1, '#peta4life': 1, '#morelikeblessedgulls': 1, '#root': 1, '#everything': 1, '#singin': 1, '#roccityproblems': 1, '#wastehertime2016': 1, '#exitstagelefteven': 1, '#gitgudskrub': 1, '#notalllibertarians': 1, '#smalllivesmatter': 1, '#sharpeilivesmatter': 1, '#peepingpaul': 1, '#safespace': 1, '#makefnaticgreatagain': 1, '#royalty': 1, '#2719392': 1, '#buffthevmp': 1, '#controlmasterrace': 1, '#feelingthejohnson': 1, '#dansgame': 1, '#pogchamp': 1, '#ctr': 1, '#50in07': 1, '#52217': 1, '#buyanexus': 1, '#getsomeskin': 1, '#makeelainagreatagain': 1, '#slitforsonorous': 1, '#redditmasterrace': 1, '#changeiscoming': 1, '#sm4shkirby': 1, '#neverpaul': 1, '#neverpaulryan': 1, '#go': 1, '#back': 1, '#toosoon': 1, '#blamethekerbs': 1, '#cheese': 1, '#toobigtojail': 1, '#uptrump': 1, '#thats': 1, '#tfw': 1, '#bias': 1, '#dutertehanggangimpyerno': 1, '#swagcloset': 1, '#whenwillrussianroadragestop': 1, '#cant': 1, '#cottoncandy': 1, '#penis': 1, '#prayformaravenier': 1, '#noot': 1, '#profit': 1, '#cardboardcutoutofhillary2016': 1, '#imgur': 1, '#iactuallydonthaveaproblemwitherayobutjumpedonthehatetrain': 1, '#drumpf': 1, '#suspend': 1, '#aint': 1, '#ryan': 1, '#blackpeppermatters': 1, '#religionofpeace': 1, '#testplsignore': 1, '#adulting': 1, '#notallelectrictypes': 1, '#put': 1, '#smug': 1, '#iamtoothirsty': 1, '#blondesdontspoof': 1, '#blackolivesmatter': 1, '#protectthestreak': 1, '#savepcislot': 1, '#whitewashing': 1, '#icanteven': 1, '#reicht': 1, '#morepocketsmorepussy': 1, '#asiansquadgoals': 1, '#absolutely': 1, '#deadbasenerf': 1, '#herroyalcrook': 1, '#clearly': 1, '#123': 1, '#omnilogic': 1, '#feminismiscancer': 1, '#a11y': 1, '#feelthejohnsonbern': 1, '#trtfandproud': 1, '#facegains': 1, '#neverhillary': 1, '#wall': 1, '#walkingwhileblack': 1, '#yolopulls': 1, '#85': 1, '#allivesmatter': 1, '#instinct4lyf': 1, '#freemilo': 1, '#freechadkuhl': 1, '#notallwolverines': 1, '#2alltime': 1, '#pick': 1, '#fuckstevenadams': 1, '#replacemark': 1, '#hodor': 1, '#broadstreetbullies': 1, '#butnotall': 1, '#workrate': 1, '#watta': 1, '#double': 1, '#dead': 1, '#justnerdthings': 1, '#gop': 1, '#blackdontcrack': 1, '#dicksoutforpluto': 1, '#witchesshouldburn': 1, '#203': 1, '#109': 1, '#outtaherewiththatnoise': 1, '#allhandsmatter': 1, '#ricardosforaporharambe': 1, '#funny': 1, '#standwithnat': 1, '#intagibles': 1, '#fetuslivesmatter': 1, '#islam': 1, '#makefragmentsgreat': 1, '#offended': 1, '#unpopularopinion': 1, '#papyrusischaraconfirmed': 1, '#prayforlgbt': 1, '#nomoarlameships': 1, '#getgood': 1, '#getgoodhafware': 1, '#1200fpskong': 1, '#bringbacklibellaws': 1, '#settleforhillary': 1, '#crookedkevin': 1, '#dongsoutforharambe': 1, '#hahahhahhahaaha': 1, '#notallgoatfuckers': 1, '#mcathype': 1, '#gazidisout': 1, '#okie': 1, '#frist': 1, '#notallcamels': 1, '#frommycolddeadhands': 1, '#wewillnotcomply': 1, '#slumpbuster': 1, '#banchod': 1, '#lock': 1, '#fan': 1, '#pleasegonow': 1, '#illhelpyoupack': 1, '#voteremain': 1, '#nohomo': 1, '#voila': 1, '#du30': 1, '#changiscoming': 1, '#allicematters': 1, '#cowlivesmatter': 1, '#blacklogosmatter': 1, '#thirdfaction4lyfe': 1, '#justpcmrthings': 1, '#1694': 1, '#tacotacoburrito': 1, '#justpsychicthings': 1, '#31': 1, '#sig': 1, '#outplayed': 1, '#reachfell1540': 1, '#keepthechange': 1, '#barkbarkwoof': 1, '#democracy': 1, '#eyes': 1, '#hahahahahahahahahahaha': 1, '#removekebab': 1, '#anyonebuttoronto': 1, '#aquamandidnothingwrong': 1, '#1854': 1, '#realmvp': 1, '#highchairking': 1, '#1437': 1, '#provocative': 1, '#shlonged': 1, '#sounds': 1, '#shovelgate': 1, '#allqbsmatter': 1, '#justswissthings': 1, '#hahaa': 1, '#bb16': 1, '#adds': 1, '#iii': 1, '#ibelieve': 1, '#redditknowsbalance': 1, '#yolocaust': 1, '#bernieortrump': 1, '#qualityshitpost': 1, '#navixsports': 1, '#meninism': 1, '#firemm': 1, '#serverwatch': 1, '#religion': 1, '#writing': 1, '#fatlivesmatter': 1, '#55': 1, '#tendies': 1, '#hotcheetos': 1, '#blunderbrag': 1, '#dreamgreen': 1, '#mild': 1, '#darkjays': 1, '#braundidnothingwrong': 1, '#iseewutudidthere': 1, '#sharklivesmatter': 1, '#impressive': 1, '#wikileaks': 1, '#another': 1, '#headsgone': 1, '#stockgate': 1, '#seize': 1, '#allstatesmatter': 1, '#happening': 1, '#feelthemath': 1, '#fundiealert': 1, '#ibelievesurvivors': 1, '#deadcat': 1, '#bitch': 1, '#normies': 1, '#ripfamas': 1, '#riptwitch': 1, '#pochout': 1, '#kanhaiyakumar': 1, '#newyork': 1, '#mutrewards': 1, '#fanlogic': 1, '#iq': 1, '#blamedenayerandkdb': 1, '#eurocucks': 1, '#under': 1, '#broken': 1, '#gamingsession': 1, '#fartnoises': 1, '#2a': 1, '#justrexburgthings': 1, '#bluehelmetsmatter': 1, '#getbrekt': 1, '#makestupidhurtagain': 1, '#punchtehdumb': 1, '#becauseroadkill': 1, '#banphantomload': 1, '#consume': 1, '#wastehistime2016': 1, '#wiiuforlyfe': 1, '#thisisnotagame': 1, '#removethekaramjashop': 1, '#thathappened': 1, '#hugot': 1, '#todossomosaceventura': 1, '#bisayamasterrace': 1, '#helpmeobiwan': 1, '#alwaysf7': 1, '#peggoals': 1, '#hahahaha': 1, '#alien': 1, '#diaz': 1, '#funtime': 1, '#venezuela': 1, '#bringbackericstein': 1, '#signeder': 1, '#time': 1, '#iamwoman': 1, '#watchmevote': 1, '#nguyening': 1, '#surely': 1, '#sapelikulangpinoy': 1, '#tea': 1, '#2k16': 1, '#finoallafine': 1, '#definitely': 1, '#flintwatercrisis': 1, '#chickentrump': 1, '#clintonshillsbreakthescales': 1, '#blackfemalelivesdontmatter': 1, '#makesoccergreatagain': 1, '#babyrage': 1, '#sober': 1, '#o_o': 1, '#alliancebois': 1, '#teampopplio': 1, '#tank': 1, '#hahahahahahahahahahahahahahaahahahahahahahahahahahahahaahahahahahahahahahahahahahaahahahahahahahahahahahahahaahahahahahahahahahahahahahaahahahahahahahahahahahahahaahahahahahahahahahahahahahaahahahahahahahahahahahahahaahahahahahahahahahahahahaha': 1, '#istandwithacmakerahmed': 1, '#sploosh': 1, '#notmysharia': 1, '#nottheonion': 1, '#lazy': 1, '#du30forchange': 1, '#domreddit': 1, '#noregrets': 1, '#freebnrandbmp': 1, '#bringbackharper': 1, '#whitewashingmatters': 1, '#makehingesgreatagain': 1, '#teamrnc': 1, '#msm': 1, '#hafthorbjornsson2016': 1, '#snakelivesmatter': 1, '#freethepepperoninipple': 1, '#cuck': 1, '#makebaseballfunagain': 1, '#stumped': 1, '#plantlivesmatter': 1, '#optimist': 1, '#fuckthepoor': 1, '#stumpfortrump': 1, '#feelthechafe': 1, '#sanders': 1, '#enoughisenough': 1, '#23940012': 1, '#bernie2016': 1, '#freezepeach': 1, '#iguessi': 1, '#genitalphobic': 1, '#matthews4manitoba': 1, '#stackoverflow': 1, '#tank4matthews': 1, '#nofear': 1, '#sinkingship': 1, '#step6masterrace': 1, '#whoosh': 1, '#intensity': 1, '#ed': 1, '#build': 1, '#grenades4dayz': 1, '#fuckdraymond': 1, '#lyin': 1, '#frankunderwood2016': 1, '#doyouevenvapebro': 1, '#turn': 1, '#notalldudes': 1, '#istandwithjames': 1, '#necrounion': 1, '#eggban': 1, '#stickem': 1, '#429': 1, '#akimbo': 1, '#slayer': 1, '#deus': 1, '#guac': 1, '#coffee': 1, '#coffeeaddict': 1, '#espresso': 1, '#hario': 1, '#chemex': 1, '#latte': 1, '#latteart': 1, '#deathbeforedecaf': 1, '#delicious': 1, '#coffeesesh': 1, '#music': 1, '#sennheiserhd598': 1, '#sennheiser': 1, '#headphones': 1, '#audiophile': 1, '#project365': 1, '#project366': 1, '#canon': 1, '#canon_official': 1, '#nikon_photography': 1, '#nikon': 1, '#buyfilmnotmegapixels': 1, '#getthefuckoffmyscreenbrandi': 1, '#deadlivesmatter': 1, '#flightfeathers': 1, '#crapple': 1, '#gibberish': 1, '#capitalism': 1, '#noheadphonesisthefuture': 1, '#sixseasonsandamovie': 1, '#feelthewar': 1, '#alllivesmattress': 1, '#3edgy5me': 1, '#deepthought': 1, '#freeswag': 1, '#rawwwwwr': 1, '#justsocietythings': 1, '#bugslifematter': 1, '#yoloswag': 1, '#whatswrongwithtodayskids': 1, '#common': 1, '#hillary2016': 1, '#seed': 1, '#badass': 1, '#botmahayudh': 1, '#1c': 1, '#team': 1, '#gear': 1, '#award': 1, '#cylonsdidnothingwrong': 1, '#sellout': 1, '#666': 1, '#perpetual': 1, '#whyineedfeminism': 1, '#cowboysuk': 1, '#victimblaming': 1, '#stormsellbot2': 1, '#roit': 1, '#palestruggles': 1, '#noreos': 1, '#expose': 1, '#savegoatboy': 1, '#felicity': 1, '#renamearrowtofelicity': 1, '#felicityforblackcanary': 1, '#togglescape': 1, '#patriarchyisreal': 1, '#dontacceptpuffsinvite2016': 1, '#bat': 1, '#rizzohatesfun': 1, '#northshoremasterrace': 1, '#omalley2024': 1, '#killallwhitemen': 1, '#thiswhycapslockswasinventedsothepeoplecanreadwhatyouguysaretyping': 1, '#107': 1, '#thestruggle': 1, '#ftfy': 1, '#gimme': 1, '#justarsenalthings': 1, '#lakers': 1, '#pbtmasterrace': 1, '#cultural': 1, '#menarepigs': 1, '#thankshillary': 1, '#tolerant': 1, '#firebevell': 1, '#firecable': 1, '#hillarysoprogressive': 1, '#chickenlivesmatter': 1, '#prankz42k16': 1, '#slaystar': 1, '#0ac1fb': 1, '#08b8f3': 1, '#notallyoungpeople': 1, '#stopageism': 1, '#richlivesmatter': 1, '#bringbackmunchak': 1, '#noonegivesafuck': 1, '#oce': 1, '#shatonleicester': 1, '#squadgoats': 1, '#banweedwhackherhedge': 1, '#freakazoiddidnothingwrong2k16': 1, '#firematheny': 1, '#cutforbeiber': 1, '#sendfrankiedown': 1, '#vaginaeleven': 1, '#deranosebleed': 1, '#refubee': 1, '#300': 1, '#bot': 1, '#mensrights': 1, '#l': 1, '#google': 1, '#feeltheburnout': 1, '#houstonorbust': 1, '#keroyokan': 1, '#justgeorgiathings': 1, '#catlivesmatter': 1, '#fukunami': 1, '#prepping': 1, '#tyt': 1, '#crystal': 1, '#adelaideisskyblue': 1, '#wasitforthis': 1, '#occupyfbi': 1, '#paperboy': 1, '#qualitypost': 1, '#weedenergy': 1, '#cool': 1, '#blackcrimesmatter': 1, '#cake': 1, '#centipede': 1, '#cruzmissiles': 1, '#toummaaaa': 1, '#imo': 1, '#stopsavagery2k15': 1, '#sbejhapatria': 1, '#whoiskimk': 1, '#mlsisred': 1, '#ilovehaters': 1, '#cigargate': 1, '#ableism': 1, '#loosechange': 1, '#both': 1, '#timeforwar': 1, '#nflcoachessowhite': 1, '#usasowhite': 1, '#stancebro': 1, '#greninja': 1, '#newyerkers4lyfe': 1, '#51': 1, '#lapatriaeselotro': 1, '#duck': 1, '#superfight': 1, '#blackgenocide': 1, '#rocketscience': 1, '#lalalalalakers2020': 1, '#blackmarketsmatter': 1, '#zinger': 1, '#bitcoin': 1, '#australians': 1, '#ikwilhelpen': 1, '#beatla': 1, '#justdevilsthings': 1, '#pantsareberning': 1, '#stonersloth': 1, '#bootyforrazihel': 1, '#phonebanking': 1, '#unpopular': 1, '#staywoke': 1, '#haram': 1, '#inibukanbudayamalaysia': 1, '#nerf': 1, '#partylikeits1944': 1, '#dipponder': 1, '#firenelson': 1, '#upforwhatever': 1, '#vintagewade': 1, '#freekesha': 1, '#introducing': 1, '#doubtyourdoubts': 1, '#feelthescott': 1, '#notmeus': 1, '#nr6': 1, '#fukt': 1, '#famasmasterrace': 1, '#every': 1, '#edgy': 1, '#canada': 1, '#hendo': 1, '#forkyle': 1, '#luzon': 1, '#dis': 1, '#ontrack2018': 1, '#xboxorweriot': 1, '#buckfullies': 1, '#stallintactics': 1, '#stopgamergate2014': 1, '#tedscruz': 1, '#westernbias': 1, '#trufacts': 1, '#bringbackthewitchtrials': 1, '#impressed': 1, '#jokes': 1, '#microfractures': 1, '#nuuuuuuuuuuuuuccccccleeeeeeeeeear': 1, '#releasethetranscripts': 1, '#oneseasonwonder': 1, '#scandle': 1, '#1213': 1, '#commentsowhite': 1, '#yeahbabyallwomyn': 1, '#relatable': 1, '#usefulsteve': 1, '#panders': 1, '#freewideman': 1, '#smkmad': 1, '#jetgate': 1, '#saynicethingsaboutdetroit': 1, '#maleprivilege': 1, '#esport': 1, '#godmod': 1, '#blush': 1, '#sjw': 1, '#shrillary': 1, '#favors': 1, '#stopchristianism': 1, '#another1': 1, '#bringbackgrimfandangbros': 1, '#2017': 1, '#kony2012': 1, '#being': 1, '#blamesworth': 1, '#thisiswhyweneedfeminism': 1, '#whisperofadream': 1, '#astonishing': 1, '#freeelchapo2k16': 1, '#gachigasm': 1, '#letsmakeamericagreatagain': 1, '#vivalasvegas': 1, '#lincolnwasatyrant': 1, '#space': 1, '#sexlivesmatters': 1, '#poop': 1, '#makecubagreatagain': 1, '#blackbearlivesmatter': 1, '#fillthestein': 1, '#randallcobblivesmatter': 1, '#keepo': 1, '#lovelyzstagram': 1, '#trytosmile': 1, '#lietillyoudie': 1, '#82': 1, '#good': 1, '#4g': 1, '#2778': 1, '#heswithus': 1, '#penguinlivesmatter': 1, '#saynotothecrimeculture': 1, '#incestiswincest': 1, '#yearonebestyear': 1, '#unoriginal': 1, '#wargate': 1, '#backoffindia': 1, '#patriarchy': 1, '#imoppressed': 1, '#mabelgross': 1, '#noooo': 1, '#solo': 1, '#fingering': 1, '#masturbation': 1, '#pantiestotheside': 1, '#manupted': 1, '#androidmasterrace': 1, '#madman': 1, '#thedream': 1, '#stopteenpregnancy': 1, '#realchange': 1, '#formatmatters': 1, '#wardroomlife': 1, '#right': 1, '#bye': 1, '#transcriptsfortaxes': 1, '#murica': 1, '#hillary4prez': 1, '#firebiello': 1, '#dippyfresh': 1, '#wenger': 1, '#notallorcs': 1, '#partyunity': 1, '#huehuehue': 1, '#youre': 1, '#eatlocal': 1, '#fuckyoubub': 1, '#two': 1, '#gayboi': 1, '#gaisoftwitter': 1, '#instagay': 1, '#gaysofwhatsapp': 1, '#gaysoflinkedin': 1, '#gaycomcastsubscribers': 1, '#gayfansofitalianfood': 1, '#bienvenidoauba': 1, '#bolder': 1, '#comebackinanotherlife': 1, '#popup': 1, '#boop': 1, '#cueeeeeettoooooo': 1, '#the16b': 1, '#notmodernjazz': 1, '#blackexcellence': 1, '#uneducated': 1, '#teamteam': 1, '#bout': 1, '#osr': 1, '#solar': 1, '#iracing': 1, '#notelite': 1, '#fireshapiro': 1, '#wronggeneration': 1, '#136': 1, '#leethaxyoloswagtasticgetonmylevel': 1, '#leakzzzz': 1, '#ipadpromasterrace': 1, '#prayforminnesota': 1, '#thedecline': 1, '#stayhealthy': 1, '#doyouevenhashtagm8': 1, '#banks': 1, '#globesuperplanmasterrace': 1, '#151': 1, '#bringbackcommando': 1, '#shamelessamerica': 1, '#bringphaxback': 1, '#remove': 1, '#betauprising': 1, '#63xxx': 1, '#somemen': 1, '#redpillredemption': 1, '#betterwithnadeshit': 1, '#fight': 1, '#loseformatthews': 1, '#stfu': 1, '#nevercompromise': 1, '#notahipster': 1, '#lpl': 1, '#bushdidthebutton': 1, '#notallpremies': 1, '#ottawaturrises': 1, '#clapback': 1, '#skinsruinednascene': 1, '#hastag': 1, '#nvidia4lif': 1, '#teamqueequeg': 1, '#banallfood': 1, '#disrespectful': 1, '#unchainmahniggalavlvalchain': 1, '#srynotsry': 1, '#policeharrasment': 1, '#notallengineers': 1, '#fuckxjayroox': 1, '#tradeformatthews': 1, '#asifteam': 1, '#sbmasterrace': 1, '#freedevo': 1, '#ps3limitations': 1, '#notallterrorists': 1, '#hype': 1, '#teamcozy': 1, '#voteno4sailing': 1, '#smitesupportmasterrace': 1, '#occupyplex2016': 1, '#zlatan2sj': 1, '#iamamormon': 1, '#banrs': 1, '#silverscrub': 1, '#clench': 1, '#freelg': 1, '#beckenbaueredelpuff': 1, '#insult2injury': 1, '#rubsaltonwound': 1, '#rgvkigaandmeinaag': 1, '#315': 1, '#destroyed': 1, '#faced': 1, '#tildotrott': 1, '#erect': 1, '#jumla': 1, '#justromanianthings': 1, '#joebuckandaikmanforever': 1, '#localnews': 1, '#endredpilldiscrimination': 1, '#manspreading': 1, '#galeleiowasaliar': 1, '#flatearth': 1, '#shipsdontprovethe': 1, '#fucktheivy': 1, '#itsacareernotacostume': 1, '#endcareerapropriation': 1, '#buffthemanguard': 1, '#si': 1, '#downwiththepatriarchy': 1, '#soedgy': 1, '#spoil': 1, '#sidebar': 1, '#freeibp': 1, '#banfnatic': 1, '#lizardlivesmatter': 1, '#unbanunidan': 1, '#doo': 1, '#allegedupdate': 1, '#objection': 1, '#mony': 1, '#bootie': 1, '#nogaysallowed': 1, '#straya': 1, '#fuckthemuslimstoo': 1, '#stoptheboats': 1, '#ruiningthegame': 1, '#justiceforintegratedgraphics': 1, '#bushdid911': 1, '#activism': 1, '#surfboart': 1, '#coalisgoodforhumanity': 1, '#blameowen': 1, '#notppa': 1, '#dh': 1, '#goalsforcory': 1, '#pb2spw': 1, '#baby': 1, '#whitesagainstobama': 1, '#outlaw': 1, '#bahrainiredditeffect': 1, '#gifwarning': 1, '#shitbronzeplayerssay': 1, '#zero': 1, '#jewn': 1, '#just12yearoldthings': 1, '#whoooooooooooooo': 1, '#cheaters': 1, '#savage': 1, '#niunomenos': 1, '#raisethewage': 1, '#gotem': 1, '#sawft': 1, '#noskoped': 1, '#scrumptiouslymoe': 1, '#bearslivesmatter': 1, '#ttrclosing': 1, '#autismmasterrace': 1, '#agianstsavelapis2k15': 1, '#adblockforcube': 1, '#sadpanda': 1, '#brew': 1, '#penguinmasterrace': 1, '#shitpostsundays': 1, '#toxic': 1, '#daddyinourhearts': 1, '#fiorina': 1, '#juicy': 1, '#archthatbackchallenge': 1, '#bestpost2015': 1, '#bangers': 1, '#justgregorbeinggregor': 1, '#banned': 1, '#kickthewick': 1, '#dfa': 1, '#thankyouellenpao': 1, '#lestemmasterrace': 1, '#tipsfedora': 1, '#bernourfamilies': 1, '#treasonerd': 1, '#twittermatters': 1, '#legitblueaudi': 1, '#blackhat': 1, '#nobama': 1, '#weflyasone': 1, '#chortel': 1, '#failtel': 1, '#gadhetel': 1, '#freet2': 1, '#yousaidsomethingdumb': 1, '#amazmath': 1, '#midtierboys': 1, '#skeltalmasterrace': 1, '#moyesin': 1, '#fatprivilege': 1, '#foldequity': 1, '#fixrng': 1, '#dysphoriasofragile': 1, '#whoops': 1, '#comic': 1, '#aamaadmi': 1, '#designeronly': 1, '#trump4prezident': 1, '#8238': 1, '#trending': 1, '#teamfearless': 1, '#keepshawnwatson': 1, '#regret': 1, '#classicnorv': 1, '#sonofskywalker': 1, '#flair': 1, '#muptiboxerlivesmatter': 1, '#murica2015': 1, '#amberbaalmatters': 1, '#notalldicks': 1, '#whinepower': 1, '#polyglobkinlivesmatter': 1, '#potonpot': 1, '#clsurprise': 1, '#not_all_blacks': 1, '#henevercared': 1, '#havenofurry': 1, '#ableist': 1, '#reasons': 1, '#dropgate': 1, '#intensifies': 1, '#question': 1, '#danny': 1, '#brownlabsmatter': 1, '#7429': 1, '#deadzonecontrolorriot': 1, '#kyonfacepalm': 1, '#70': 1, '#120': 1, '#yesalltrekkies': 1, '#givewomenyourmoney': 1, '#failforcardale': 1, '#dirtybulk': 1, '#notallaustralians': 1, '#novotekanter': 1, '#indigenizeou': 1, '#fliplife': 1, '#yay': 1, '#boo': 1, '#inb4': 1, '#pro': 1, '#youevenliftbro': 1, '#openedmyeyez': 1, '#fuckyourculturestandards': 1, '#daretozlatan': 1, '#pedestrianlivesmatter': 1, '#mcstamwillymarncock2018': 1, '#lcsbigplays': 1, '#kaneout': 1, '#grunklemad': 1, '#notacop': 1, '#attention': 1, '#cumbear2015': 1, '#navi': 1, '#ggmu': 1, '#votetrump': 1, '#ring': 1, '#crying': 1, '#47': 1, '#ducktective': 1, '#ripmasterrace': 1, '#feministlogic': 1, '#notallcops': 1, '#fuckitbaylife': 1, '#rbbproduct': 1, '#newsimple': 1, '#forsbetterworld': 1, '#thankyou': 1, '#appreciateit': 1, '#noiwontgiveyoumymother': 1, '#secret': 1, '#notyourdoublestandard': 1, '#016': 1, '#183092': 1, '#blackdriversmatter': 1, '#conservativecommentatorlivesmatter': 1, '#masala': 1, '#aussieeeeeeeeeee': 1, '#green': 1, '#myfaithisstrong': 1, '#rahrahrahfightthepower': 1, '#justswoopesthings': 1, '#occupy': 1, '#crack': 1, '#kda': 1, '#itsadryheat': 1, '#2012': 1, '#justbasketballconferencethings': 1, '#blackpeopleprivilege': 1, '#irannucleardeal': 1, '#8b4513': 1, '#disable': 1, '#today': 1, '#hammerweek': 1, '#consumerism': 1, '#shortlivesmatter': 1, '#hailbernie': 1, '#hailreddit': 1, '#tuckfrump': 1, '#1fansinbaseball': 1, '#chipkellyforfuhrer': 1, '#futurelaker': 1, '#87': 1, '#yesalljews': 1, '#choppergate': 1, '#gginsyd': 1, '#amdproblems': 1, '#yourlifematters': 1, '#exactoknivesmatter': 1, '#fuckkonami': 1, '#fullybaked': 1, '#hyoukawink': 1, '#eternalperspective': 1, '#benchrodgers': 1, '#notallsawcasms': 1, '#pinklivesmatter': 1, '#gategate': 1, '#warispeace': 1, '#freedomisslavery': 1, '#ignoranceisstrength': 1, '#4h': 1, '#puremichigan': 1, '#rapist': 1, '#criminallivesmatter': 1, '#include': 1, '#heroball': 1, '#girlswhosmokeweed': 1, '#fightfor15': 1, '#im': 1, '#nuffsaid': 1, '#stylesheet': 1, '#chubby': 1, '#biggirls': 1, '#size22': 1, '#loud': 1, '#huolmao': 1, '#clockservative': 1, '#christ': 1, '#font': 1, '#ate': 1, '#soul': 1, '#muh': 1, '#technicallycorrect': 1, '#helloaccc': 1, '#noooooo': 1, '#believeinfloro': 1, '#goalbody2k15': 1, '#notacult': 1, '#hesjustnotready': 1, '#maletears': 1, '#unskewthepolls': 1, '#discriminationisdiscrimination': 1, '#thebakingpenetration': 1, '#the300bakers': 1, '#panthdemic': 1, '#135': 1, '#save': 1, '#fake': 1, '#neverhappened': 1, '#hoax': 1, '#area51': 1, '#kansasproblems': 1, '#indisguise': 1, '#ik': 1, '#synergy': 1, '#clgspirit': 1, '#deleteallmen': 1, '#mlgtvmasterrace': 1, '#forg1v3': 1, '#whyigame': 1, '#fransout': 1, '#rougewedding': 1, '#allclonesmustdie': 1, '#343savedhalo': 1, '#denialan': 1, '#identitycrisis2015': 1, '#power2the': 1, '#players': 1, '#yesallwomen': 1, '#dantesux': 1, '#freethejobstownfour': 1, '#pleb': 1, '#justflushathings': 1, '#rinaldolife': 1, '#only50skidswillgetthis': 1, '#yumyum': 1, '#notallescapecharacters': 1, '#soreaf': 1, '#standwithrand': 1, '#yesallmen': 1, '#vacation': 1, '#stillmorethanconsoles': 1, '#fatacceptance': 1, '#intangibles': 1, '#bearsuk': 1, '#moochers': 1, '#bestmanagement': 1, '#pingpong': 1, '#ssdmasterrace': 1, '#dps': 1, '#colemakmasterrace': 1, '#teampropaneandpropaneaccesories': 1, '#cease': 1, '#sidelongboards2015': 1, '#somelivesdontmatter': 1, '#fiveguys': 1, '#insane': 1, '#lore': 1, '#trolling': 1, '#cirveesisters4life': 1, '#yesallfurries': 1, '#cm12masterrace': 1, '#eflategate': 1, '#tru': 1, '#turnup': 1, '#fuckwhitefolks': 1, '#asianlivesdon': 1, '#drmfreeproblems': 1, '#randy': 1, '#und': 1, '#toomanyhashtags': 1, '#faithage': 1, '#whatstheconspiracy': 1, '#stajcicout': 1, '#firstworldhoonproblems': 1, '#99problemsbutmycarisnotoneofthem': 1, '#thisisn': 1, '#curry4mvp': 1, '#stopallcispeople2015': 1, '#fag_wannabe': 1, '#faggot_please': 1, '#peace_out': 1, '#not_understanding_how_hashtags_work': 1, '#underscore': 1, '#benghazi': 1, '#totalfootball': 1, '#teampie': 1, '#shurima': 1, '#forever29': 1, '#wachrbi': 1, '#notallconservatives': 1, '#notallnuts': 1, '#452': 1, '#pointergate': 1, '#passion': 1, '#givedivasachance': 1, '#cheatgate': 1, '#illridwithyou': 1, '#ksal': 1, '#le': 1, '#andamovie': 1, '#decision': 1, '#notallvaginas': 1, '#alldaysmatter': 1, '#ineedhaestojudgepeoplehealthierthanme': 1, '#luchoout': 1, '#shakawatch': 1, '#yesallcops': 1, '#realmenhavecurves': 1, '#even': 1, '#bayleeflivesmatter': 1, '#ownthefuture': 1, '#ps4ultragraphics': 1, '#oneunit': 1, '#ohnohedidnt': 1, '#67': 1, '#impeachrecker': 1, '#gnomechild': 1, '#yorokobe': 1, '#pot2blame': 1, '#cmon': 1, '#car': 1, '#night': 1, '#theoneinthehole': 1, '#80': 1, '#my_ps4_has_more_teraflops_than_your_xbox': 1, '#ezscape': 1, '#proge': 1, '#idontliketradingwithpeople': 1, '#noplanb': 1, '#keepkeith': 1, '#camelback': 1, '#8255': 1, '#8636': 1, '#bronzethings': 1, '#rule34': 1, '#salmon': 1, '#whiteout': 1, '#tbt': 1, '#bring': 1, '#eatyourwheaties': 1, '#100k': 1, '#fireterrypls': 1, '#just_use_underscore_already': 1, '#nerftheliberator': 1, '#gtfo': 1, '#drill': 1, '#arablivesmatter': 1, '#voteukip': 1, '#notallbodies': 1, '#notallamericans': 1, '#nextgen': 1, '#aloisiout': 1, '#73203': 1, '#apocalypsenow': 1, '#undead4lyfe': 1, '#ineedammo': 1, '#jkimvs': 1, '#nerfbetel': 1, '#plsbringkpback': 1, '#675': 1, '#enduro': 1, '#81': 1, '#iamaknife': 1, '#conspiracy': 1, '#toilet': 1, '#actuallyreadabletumblrgifs': 1, '#killallproducers': 1, '#iseenoproblemwiththat': 1, '#firstworldanarchists': 1, '#str8pride': 1, '#offlinegate': 1, '#slipisshowing': 1, '#coreyterryspaint': 1, '#killallbigots': 1, '#yololifestyle': 1, '#paythepipe': 1, '#freedoms': 1, '#fact': 1, '#joe': 1, '#huntthetruth': 1, '#justsocialjusticethings': 1, '#blackoutboyz': 1, '#foxnewsfacts': 1, '#checkyourprivilage': 1, '#awarriorforjustice': 1, '#thenetwork': 1, '#jumponthephaneufhatebandwagon': 1, '#smashgate': 1, '#notalllegionaries': 1, '#fullmcintosh': 1, '#263': 1, '#hoxtonrevengegate': 1, '#eliteteamfanproblems': 1, '#jealousofthebulls': 1, '#nevar4getskyrim': 1, '#teampepsi': 1, '#banter': 1, '#blackout': 1, '#pantsupdontloot': 1, '#breatheasydontbreakthelaw': 1, '#teamlightskin': 1, '#90skidswouldunderstand': 1, '#theskyisfalling': 1, '#bust2015': 1, '#blazeit420fggts': 1, '#robertson': 1, '#prayforwarner': 1, '#megaduh': 1, '#accepted': 1, '#understandingwhythatmastersdidntpayoff': 1, '#michaeljackson': 1, '#amddreams': 1, '#fashionablemisogyny': 1, '#killalltranswomen': 1, '#whitemalecispride': 1, '#road2ge': 1, '#teamspringdale': 1, '#tankedforphilly': 1, '#journalism': 1, '#cloudmasterrace': 1, '#butdontsupportfaker': 1, '#bundaberggingerbeermasterrace': 1, '#holliday': 1, '#shipmasterrace': 1, '#yearrekt': 1, '#lolmets': 1, '#overbeforeitstarts': 1, '#beefban': 1, '#thereturnofmerk': 1, '#wakeup': 1, '#notallmormons': 1, '#goldenage': 1, '#dramaalert': 1, '#comicsgate': 1, '#notallfeminists': 1, '#appstorebillionaire': 1, '#noerapenal': 1, '#cantbelievetheyvotedforme': 1, '#flackoseason': 1, '#watchthegame': 1, '#capstable': 1, '#justmasterracethings': 1, '#fallofoptic': 1, '#immortal': 1, '#anarchy': 1, '#b1as': 1, '#crunzhitizemecaptain': 1, '#voteyestoge': 1, '#heyguyswegotanewone': 1, '#justoklahomathings': 1, '#stopthegayagenda': 1, '#lamergate': 1, '#opticfamily': 1, '#waybackwednesday': 1, '#nwts': 1, '#teampopey': 1, '#jerusalem': 1, '#lirikh': 1, '#neverforgt': 1, '#glutenfree': 1, '#fairtrade': 1, '#freerange': 1, '#pretentious': 1, '#annoying': 1, '#susanalbumparty': 1, '#heavyassaultmasterrace': 1, '#notbarbaric': 1, '#gopfiscalresponsibility': 1, '#omgdefect': 1, '#notalltackyshirts': 1, '#collusion': 1, '#mix2sj': 1, '#diecisscum': 1, '#darkesttimeline': 1, '#justflush': 1, '#justskyttenthings': 1, '#vibe': 1, '#cancelcolbert2electricboogaloo': 1, '#nosleeptiltaipeithedream': 1, '#scumbagkootra': 1, '#iinnnnteeellll': 1, '#fansthatskipedtoact5': 1, '#stuntheworld': 1, '#cancelcyanide': 1, '#sohipster': 1, '#ibestbuy': 1, '#weaponizedchristmas': 1, '#osteoporosisisnojoke': 1, '#epic': 1, '#jhonny': 1, '#justdishwasherthings': 1, '#killallhumans': 1, '#justhibbertthings': 1, '#xp': 1, '#ehp': 1, '#xboxandweed4life': 1, '#mountainjizz': 1, '#ffde00': 1, '#00ffde00': 1, '#45': 1, '#jonny': 1, '#gosox': 1, '#thrillho': 1, '#notallgeneralizations': 1, '#baseball': 1, '#httr': 1, '#tinfoilhat': 1, '#saveodato': 1, '#selfies': 1, '#blackfisheffect': 1, '#evidence': 1, '#hashtagswork': 1, '#fatasexualdouches': 1, '#squad': 1, '#192': 1, '#215': 1, '#420blazeit': 1, '#mennotboys': 1, '#notallhominids': 1, '#freshmanfuntimes': 1, '#voluntary': 1, '#buckets': 1, '#spiritofchildren': 1, '#spirithalloween': 1, '#unity': 1, '#realcheese': 1, '#yesallfeminists': 1, '#wondout': 1, '#133': 1, '#truefanz': 1, '#stealthefalcons': 1, '#ovi4selke': 1, '#sufferingfromsuccess': 1, '#metalgate': 1, '#justredditthings': 1, '#freak7208in': 1, '#teamcookie': 1, '#oohkillem': 1, '#murktd': 1, '#groom': 1, '#icare': 1, '#caad10': 1, '#australiaissobackwardsorshouldisayupsidedownlolololol': 1, '#sancristobal': 1, '#fingerscrossed': 1, '#chemtrails': 1, '#avbin': 1, '#lower': 1, '#flynning': 1, '#aaptriestowedcongress': 1, '#teamchaos': 1, '#bringbackourdinosaurs': 1, '#whensteamcaredforpc': 1, '#putspacebeforethehashtag': 1, '#redditflippitydo': 1, '#amiright': 1, '#adolf': 1, '#sickburnbro': 1, '#centeroftheuniverse': 1, '#mcguinnessout': 1, '#yoloifoundit': 1, '#nicetryvolvo': 1, '#shit': 1, '#nbaposterboy': 1, '#jedfest2014': 1, '#high4': 1, '#salim2014': 1, '#mod': 1, '#wtfdoyouevenlift': 1, '#ck': 1, '#rapgametaylorswift': 1, '#yugiohrulezmagicdroolz': 1, '#bars': 1, '#loveforintelhd4000': 1, '#escapesequencesftw': 1, '#atleasthedidsomething': 1, '#notyournotyourshield': 1, '#tw': 1, '#notallredditors': 1, '#yesalltotalitarianmindsets': 1, '#hashtags': 1, '#whataboutmrsklaus': 1, '#whatagaem': 1, '#iwantarefund': 1, '#dayzscumbagdevs': 1, '#lies': 1, '#thisisthinprivilege': 1, '#loominarty': 1, '#freeedom': 1, '#dd': 1, '#boylesosoft': 1, '#yesallparticles': 1, '#44': 1, '#notinmyspecies': 1, '#class': 1, '#avgrenadesarebalanced': 1, '#282': 1, '#2021': 1, '#tsmattentionwhoresen': 1, '#teamaustralia': 1, '#need4embiid': 1, '#perfectlygoodsidewalk': 1, '#frat': 1, '#justprogamerthings': 1, '#justgirlythings': 1, '#nofilters': 1, '#doomedtoloseagain': 1, '#thanksalothayne': 1, '#hopecronkbreakshisarmagain': 1, '#yesyougotrekt': 1, '#suey': 1, '#cancelcolbert': 1, '#mambamentality': 1, '#firerickremender': 1, '#justexilethings': 1, '#rethinkinternalcombustionengines': 1, '#hossa4everything': 1, '#nerdjoke': 1, '#getonmylevel': 1, '#2nerdy4u': 1, '#the47': 1, '#notallwhitepeople': 1, '#spygate': 1, '#finalsolution': 1, '#draftednotbought': 1, '#someoneisnew': 1, '#haxor': 1, '#1337': 1, '#pwnn00bs': 1, '#yesallredditors': 1, '#temdininite': 1, '#alwayswithaap': 1, '#istandwitharvind': 1, '#kranti': 1, '#revolution': 1, '#greenwall': 1, '#frontrangeisbestrange': 1, '#baben': 1, '#moyesout': 1, '#wsgy': 1, '#creeperunclejokes': 1, '#freeshinji': 1, '#ps4potatorace': 1, '#cancelled': 1, '#12541': 1, '#3900': 1, '#1604': 1, '#860': 1, '#3901': 1, '#65417': 1, '#justformattingthings': 1, '#fuckyourbeautystandards': 1, '#fuckyourbodystandards': 1, '#tanksandstuffbro': 1, '#leanbulksystem': 1, '#pcgiveaway': 1, '#baryouspeakof': 1, '#fabulousssss': 1, '#dontcrosstheline': 1, '#titis': 1, '#notyourgoodfatty': 1, '#octothorpe': 1, '#baylife': 1, '#lifehack': 1, '#nes4lyfe': 1, '#juststopwhining': 1, '#freedomfighters': 1, '#walloftext': 1, '#mw': 1, '#potatoes': 1, '#thants': 1, '#thedeparture': 1, '#bringbackourjerbs': 1, '#fossilswag': 1, '#noparity': 1, '#22': 1, '#notaclone': 1, '#prayers': 1, '#bluecollargoldswag': 1, '#hickfromfrenchlick': 1, '#hoosierstate': 1, '#sicthesycsonem': 1, '#bestsuggest2014': 1, '#justfinishedlol': 1, '#murcia': 1, '#sootlifebro': 1, '#standbymalthouse': 1, '#ormr': 1, '#blametonyabbot': 1, '#bancancer': 1, '#suga': 1, '#darkskin': 1, '#positiveliving': 1, '#yoloswag9000over': 1, '#ironyateverysize': 1, '#2deepforme': 1, '#420swag': 1, '#losers': 1, '#cashmanfailed': 1, '#crowhoenation': 1, '#throwdempizzalunchables': 1, '#420blazeitogkushgod': 1, '#justtitanthings': 1, '#cubs': 1, '#justwindows7things': 1, '#woah': 1, '#longlimbedandslim': 1, '#yesallblocks': 1, '#12345': 1, '#mittromney2016': 1, '#wincest': 1, '#tradedurant': 1, '#fuckthepolice': 1, '#offtarget': 1, '#men': 1, '#thinprivilege': 1, '#signhimuphodgson': 1, '#nodaysoff': 1, '#top': 1, '#dumbasses': 1, '#freesiggy': 1, '#b1gpride': 1, '#ignance': 1, '#savelin': 1, '#deb887': 1, '#stupid': 1, '#lamesauce': 1, '#5846': 1, '#exactly': 1, '#4e20': 1, '#scumbag': 1, '#makes': 1, '#koolkidzklub': 1, '#nosebleed': 1, '#yep': 1, '#1767': 1, '#funeral': 1, '#getpissed': 1, '#startedatthebottomnowwehere': 1, '#drinkthekoolaid': 1, '#illustratorpentoolicontattoo': 1, '#thumbsup': 1, '#airen': 1, '#bwarm': 1, '#yadi': 1, '#longboarders420blazeitfagets': 1, '#blackface': 1, '#yoloswaglawlz': 1, '#misandryinvideogames': 1, '#osteen': 1, '#sigh': 1, '#33b5e5swag': 1, '#think': 1, '#with': 1, '#keyboard': 1, '#faded': 1, '#poor': 1, '#25': 1, '#ratchet': 1, '#metime': 1, '#feku': 1, '#occupywallstreet': 1, '#478': 1, '#992': 1, '#nomakeup': 1, '#crislife': 1, '#realraponly': 1, '#1188': 1, '#savage2013': 1, '#smh': 1, '#high7': 1, '#bringbacksnoopeh': 1, '#modelstatus': 1, '#truechristian': 1, '#binghawks': 1, '#nerd': 1, '#nerdygirl': 1, '#nnnnnnnnnnnnnnnnnnnnnnnnnooooooooooooooooooooo': 1, '#shitthatdidn': 1, '#bloodycyclists': 1, '#joseout': 1, '#eh': 1, '#country': 1, '#killthehype': 1, '#kicktheautistic': 1, '#occupyindyracing': 1, '#10003': 1, '#517': 1, '#minutes': 1, '#beautiful': 1, '#sketchbook': 1, '#women': 1, '#2748262': 1, '#sixseaonsandamovie': 1, '#trillblackskinhead': 1, '#f1chilton': 1, '#justiceforshaaliver': 1, '#upsagans': 1, '#deepthoughtswithdrunk': 1, '#idunwannaliveondisplanetanymore': 1, '#burnitwithfire': 1, '#holoyolo': 1, '#117': 1, '#bostonstrong': 1, '#53': 1, '#respectthebrodie': 1, '#brodiesalute': 1, '#sickplays': 1, '#curseordie': 1, '#cop': 1, '#4583': 1, '#161': 1, '#stressingaboutcookingchristmasdinner': 1})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# extracting hashtags from a string\n",
    "def extract_hashtags(text):\n",
    "    return re.findall(r'#\\w+', text.lower()) if isinstance(text, str) else []\n",
    "\n",
    "# 2. Apply to entire column\n",
    "all_hashtags = df_full['text'].apply(extract_hashtags)\n",
    "\n",
    "# 3. Flatten the list of lists into one big list\n",
    "flattened = [tag for sublist in all_hashtags for tag in sublist]\n",
    "\n",
    "# 4. Get unique hashtags and their counts\n",
    "from collections import Counter\n",
    "hashtag_counts = Counter(flattened)\n",
    "\n",
    "# 5. Display the top N hashtags\n",
    "print(hashtag_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f006439a-fcea-4a87-ac05-bf46c6ae1878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/v2v1xtnj6b90rtp9nqmdrwvm0000gn/T/ipykernel_69395/1590669406.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  repeating_rows = df_full_clean['text'].str.contains(r'(.)\\1{2,}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with repeated characters (3+): 86016\n"
     ]
    }
   ],
   "source": [
    "repeating_rows = df_full_clean['text'].str.contains(r'(.)\\1{2,}')\n",
    "repeating_count = repeating_rows.sum()\n",
    "print(\"Rows with repeated characters (3+):\", repeating_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a771e33c-1967-4a38-bc4b-1d84e2773238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                               u\"\\U00002700-\\U000027BF\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)\n",
    "\n",
    "    # Remove hashtags like #sarcasm and the hashtag symbol\n",
    "    text = re.sub(r\"#sarcasm|#sarcastic|#irony\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "\n",
    "\n",
    "    # Remove non-standard symbols (except ? !)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\?\\!]\", \"\", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df_full_clean = df_full_clean.copy()\n",
    "df_full_clean['text'] = df_full_clean['text'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "616cc801-abee-40f0-b2fa-faf8cf2487da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_repeats(text):\n",
    "    pattern = re.compile(r'(.)\\1{2,}')\n",
    "    while pattern.search(text):\n",
    "        text = pattern.sub(r'\\1\\1', text)\n",
    "    return text\n",
    "\n",
    "df_full_clean = df_full_clean.copy()\n",
    "df_full_clean['text'] = df_full_clean['text'].apply(reduce_repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5549d10-2a70-4179-9286-9244a534b4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993974</th>\n",
       "      <td>Ive just seen this and felt it deserved a Retw...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993975</th>\n",
       "      <td>Omg how an earth is that a pen !!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993976</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993977</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993978</th>\n",
       "      <td>The fact that people still dont get that you n...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993979 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       former versace store clerk sues over secret bl...      0   \n",
       "1       the roseanne revival catches up to our thorny ...      0   \n",
       "2       mom starting to fear sons web series closest t...      1   \n",
       "3       boehner just wants wife to listen not come up ...      1   \n",
       "4       jk rowling wishes snape happy birthday in the ...      0   \n",
       "...                                                   ...    ...   \n",
       "993974  Ive just seen this and felt it deserved a Retw...      0   \n",
       "993975                  Omg how an earth is that a pen !!      0   \n",
       "993976          Bringing Kanye and drake to a tl near you      0   \n",
       "993977  I love it when women are referred to as girl b...      1   \n",
       "993978  The fact that people still dont get that you n...      1   \n",
       "\n",
       "               source  \n",
       "0       news_headline  \n",
       "1       news_headline  \n",
       "2       news_headline  \n",
       "3       news_headline  \n",
       "4       news_headline  \n",
       "...               ...  \n",
       "993974        twitter  \n",
       "993975        twitter  \n",
       "993976        twitter  \n",
       "993977        twitter  \n",
       "993978        twitter  \n",
       "\n",
       "[993979 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0119f58d-0391-46e5-a135-fd6a553d22c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    0.50005\n",
       "1    0.49995\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fb38a-68bb-442a-afaf-aab5cb2010a3",
   "metadata": {},
   "source": [
    "A perfectly balanced dataset is 50/50.\n",
    "\n",
    "Your dataset is less than 1% off — this is excellent for training a binary classifier.\n",
    "\n",
    "You do not need to oversample or undersample at this stage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30890e9-b683-402d-ab96-ce0a55e16e05",
   "metadata": {},
   "source": [
    "Check:\n",
    " No missing or empty texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1f01309-d3e7-4224-8663-0c101e9d9ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    993979.000000\n",
       "mean         57.160459\n",
       "std          57.577855\n",
       "min           0.000000\n",
       "25%          29.000000\n",
       "50%          47.000000\n",
       "75%          74.000000\n",
       "max       10000.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9bb4092-db3c-4644-bfb6-594e15584c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      object\n",
       "label      int64\n",
       "source    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f631e04e-6f4d-43cc-9496-683f65d1e03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing texts (NaN): 0\n",
      "Empty texts (''): 183\n",
      "Total invalid: 183\n"
     ]
    }
   ],
   "source": [
    "missing_count = df_full_clean[\"text\"].isna().sum()\n",
    "empty_count = (df_full_clean['text'].str.strip() == '').sum()\n",
    "print(f\"Missing texts (NaN): {missing_count}\")\n",
    "print(f\"Empty texts (''): {empty_count}\")\n",
    "print(f\"Total invalid: {missing_count + empty_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ed9c4cc-0a29-46bb-ab44-e376c630bcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27898</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28021</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28389</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29403</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30699</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949248</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952359</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958236</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962595</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989742</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  label   source\n",
       "27898            0  twitter\n",
       "28021            0  twitter\n",
       "28389            0  twitter\n",
       "29403            0  twitter\n",
       "30699            0   Reddit\n",
       "...     ...    ...      ...\n",
       "949248           0   Reddit\n",
       "952359           0   Reddit\n",
       "958236           1   Reddit\n",
       "962595           0   Reddit\n",
       "989742           0   Reddit\n",
       "\n",
       "[183 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (df_full_clean['text'].str.strip() == '')\n",
    "df_full_clean[a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efa5c860-9768-43c5-af62-330ec0dcdaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_clean['text'].replace('', pd.NA, inplace=True)\n",
    "df_full_clean.dropna(subset= ['text'], inplace= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08c23824-87ac-4d6b-be4e-9091deb9074e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    993796.000000\n",
       "mean         57.170985\n",
       "std          57.577930\n",
       "min           1.000000\n",
       "25%          29.000000\n",
       "50%          47.000000\n",
       "75%          74.000000\n",
       "max       10000.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b29cec84-be61-4142-9117-8de1cb5e0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30417</th>\n",
       "      <td>L</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30664</th>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30686</th>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31514</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31813</th>\n",
       "      <td>K</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979053</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980755</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983025</th>\n",
       "      <td>o</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988379</th>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992022</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       text  label  source\n",
       "30417     L      0  Reddit\n",
       "30664     P      0  Reddit\n",
       "30686     E      0  Reddit\n",
       "31514     4      0  Reddit\n",
       "31813     K      0  Reddit\n",
       "...     ...    ...     ...\n",
       "979053    0      1  Reddit\n",
       "980755    A      0  Reddit\n",
       "983025    o      0  Reddit\n",
       "988379    m      0  Reddit\n",
       "992022    a      1  Reddit\n",
       "\n",
       "[258 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = (df_full_clean['text'].str.len() == 1)\n",
    "df_full_clean[b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22202e78-8c18-48ae-9b83-20ee19048b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_clean = df_full_clean[~b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34794c15-1ae7-4239-ace0-2c7c5db313d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    993538.000000\n",
       "mean         57.185571\n",
       "std          57.578289\n",
       "min           2.000000\n",
       "25%          29.000000\n",
       "50%          47.000000\n",
       "75%          74.000000\n",
       "max       10000.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e05085a-d19b-49f7-9568-fdffd59eb56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (df_full_clean['text'].str.len() == 2)\n",
    "df_full_clean[b]\n",
    "df_full_clean = df_full_clean[~b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66c51f00-0a11-4c44-aaaf-7bace3936c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (df_full_clean['text'].str.len() == 3)\n",
    "df_full_clean[b]\n",
    "df_full_clean = df_full_clean[~b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0360875-065e-415a-a2e2-ca9c751d7ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28382</th>\n",
       "      <td>FCAS</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30602</th>\n",
       "      <td>Rekt</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30622</th>\n",
       "      <td>Same</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30673</th>\n",
       "      <td>Yup!</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30832</th>\n",
       "      <td>Said</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991538</th>\n",
       "      <td>Dick</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991603</th>\n",
       "      <td>Moto</td>\n",
       "      <td>0</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991837</th>\n",
       "      <td>FIFY</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992472</th>\n",
       "      <td>Pray</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992876</th>\n",
       "      <td>This</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3185 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  label   source\n",
       "28382   FCAS      0  twitter\n",
       "30602   Rekt      0   Reddit\n",
       "30622   Same      0   Reddit\n",
       "30673   Yup!      0   Reddit\n",
       "30832   Said      0   Reddit\n",
       "...      ...    ...      ...\n",
       "991538  Dick      1   Reddit\n",
       "991603  Moto      0   Reddit\n",
       "991837  FIFY      1   Reddit\n",
       "992472  Pray      1   Reddit\n",
       "992876  This      0  twitter\n",
       "\n",
       "[3185 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = (df_full_clean['text'].str.len() == 4)\n",
    "df_full_clean[b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47aba1a0-f158-49d2-9f9b-2162839e6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_clean = df_full_clean[df_full_clean['text'].str.contains(r'[a-zA-Z]', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac2d7432-df1d-4d5f-8f5d-3db8201fdcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, label, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean[df_full_clean['text'].str.contains(r'\\*[\\w]+|[\\w]+\\*', regex=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db0c35b0-2985-46c2-a909-58b6d2636bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where text is ONLY one word that starts or ends with an asterisk\n",
    "df_full_clean = df_full_clean[~df_full_clean['text'].str.match(r'^\\*?\\w+\\*?$', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cadb875-fa48-4e08-a581-9cc77c9b22f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993974</th>\n",
       "      <td>Ive just seen this and felt it deserved a Retw...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993975</th>\n",
       "      <td>Omg how an earth is that a pen !!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993976</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993977</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993978</th>\n",
       "      <td>The fact that people still dont get that you n...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974515 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       former versace store clerk sues over secret bl...      0   \n",
       "1       the roseanne revival catches up to our thorny ...      0   \n",
       "2       mom starting to fear sons web series closest t...      1   \n",
       "3       boehner just wants wife to listen not come up ...      1   \n",
       "4       jk rowling wishes snape happy birthday in the ...      0   \n",
       "...                                                   ...    ...   \n",
       "993974  Ive just seen this and felt it deserved a Retw...      0   \n",
       "993975                  Omg how an earth is that a pen !!      0   \n",
       "993976          Bringing Kanye and drake to a tl near you      0   \n",
       "993977  I love it when women are referred to as girl b...      1   \n",
       "993978  The fact that people still dont get that you n...      1   \n",
       "\n",
       "               source  \n",
       "0       news_headline  \n",
       "1       news_headline  \n",
       "2       news_headline  \n",
       "3       news_headline  \n",
       "4       news_headline  \n",
       "...               ...  \n",
       "993974        twitter  \n",
       "993975        twitter  \n",
       "993976        twitter  \n",
       "993977        twitter  \n",
       "993978        twitter  \n",
       "\n",
       "[974515 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6aa04e73-afa5-4d38-af44-b1fcedce70e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = (df_full_clean['text'].str.len() > 500).sum()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3ccb2f0-db63-465e-b1fe-185ec54e3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_clean = df_full_clean[df_full_clean['text'].str.len() <= 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c44a63d0-382c-49b5-be8b-7b17b519043c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    974277.000000\n",
       "mean         57.813970\n",
       "std          40.995867\n",
       "min           4.000000\n",
       "25%          30.000000\n",
       "50%          48.000000\n",
       "75%          74.000000\n",
       "max         499.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean['text'].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f143a519-fddd-4d22-b518-f127dab517d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, label, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_mask = df_full_clean['text'].str.contains(r'http\\S+|www.\\S+', regex=True)\n",
    "df_full_clean[url_mask]  # Show first 20 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a855571d-1da6-4297-8cd0-9dd4b2de99d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, label, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maybe_foreign = df_full_clean[~df_full_clean['text'].str.contains(r'[a-zA-Z]', regex=True)]\n",
    "maybe_foreign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e927cf70-f2cc-4409-908b-2b507648b645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993974</th>\n",
       "      <td>Ive just seen this and felt it deserved a Retw...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993975</th>\n",
       "      <td>Omg how an earth is that a pen !!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993976</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993977</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993978</th>\n",
       "      <td>The fact that people still dont get that you n...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       former versace store clerk sues over secret bl...      0   \n",
       "1       the roseanne revival catches up to our thorny ...      0   \n",
       "2       mom starting to fear sons web series closest t...      1   \n",
       "3       boehner just wants wife to listen not come up ...      1   \n",
       "4       jk rowling wishes snape happy birthday in the ...      0   \n",
       "...                                                   ...    ...   \n",
       "993974  Ive just seen this and felt it deserved a Retw...      0   \n",
       "993975                  Omg how an earth is that a pen !!      0   \n",
       "993976          Bringing Kanye and drake to a tl near you      0   \n",
       "993977  I love it when women are referred to as girl b...      1   \n",
       "993978  The fact that people still dont get that you n...      1   \n",
       "\n",
       "               source  \n",
       "0       news_headline  \n",
       "1       news_headline  \n",
       "2       news_headline  \n",
       "3       news_headline  \n",
       "4       news_headline  \n",
       "...               ...  \n",
       "993974        twitter  \n",
       "993975        twitter  \n",
       "993976        twitter  \n",
       "993977        twitter  \n",
       "993978        twitter  \n",
       "\n",
       "[974277 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "169ce31e-8140-42f6-a8e5-b2c1d687d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/v2v1xtnj6b90rtp9nqmdrwvm0000gn/T/ipykernel_69395/1366557371.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  repeating_rows = df_full_clean['text'].str.contains(r'(.)\\1{2,}').sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with repeated characters (3+): 0\n"
     ]
    }
   ],
   "source": [
    "repeating_rows = df_full_clean['text'].str.contains(r'(.)\\1{2,}').sum()\n",
    "print(\"Rows with repeated characters (3+):\", repeating_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "85a74ce6-15c2-4483-b0ca-31ee7f679cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/v2v1xtnj6b90rtp9nqmdrwvm0000gn/T/ipykernel_69395/3386088755.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_full_clean[df_full_clean['text'].str.contains(r'(.)\\1{2,}')].sum()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "label     0\n",
       "source    0\n",
       "dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean[df_full_clean['text'].str.contains(r'(.)\\1{2,}')].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "090c0a3b-a35e-4efe-a57d-d934888e0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "df_full_clean = df_full_clean.copy()\n",
    "df_full_clean['text'] = df_full_clean['text'].apply(expand_contractions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "908df688-8231-40ec-b538-3a1b77403a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993974</th>\n",
       "      <td>I Have just seen this and felt it deserved a R...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993975</th>\n",
       "      <td>Omg how an earth is that a pen !!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993976</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993977</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993978</th>\n",
       "      <td>The fact that people still do not get that you...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       former versace store clerk sues over secret bl...      0   \n",
       "1       the roseanne revival catches up to our thorny ...      0   \n",
       "2       mom starting to fear sons web series closest t...      1   \n",
       "3       boehner just wants wife to listen not come up ...      1   \n",
       "4       jk rowling wishes snape happy birthday in the ...      0   \n",
       "...                                                   ...    ...   \n",
       "993974  I Have just seen this and felt it deserved a R...      0   \n",
       "993975                  Omg how an earth is that a pen !!      0   \n",
       "993976          Bringing Kanye and drake to a tl near you      0   \n",
       "993977  I love it when women are referred to as girl b...      1   \n",
       "993978  The fact that people still do not get that you...      1   \n",
       "\n",
       "               source  \n",
       "0       news_headline  \n",
       "1       news_headline  \n",
       "2       news_headline  \n",
       "3       news_headline  \n",
       "4       news_headline  \n",
       "...               ...  \n",
       "993974        twitter  \n",
       "993975        twitter  \n",
       "993976        twitter  \n",
       "993977        twitter  \n",
       "993978        twitter  \n",
       "\n",
       "[974277 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59c4c730-def1-48a8-9c2b-026f01cf04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_words = [\n",
    "    \"not\", \"no\", \"never\", \"none\", \"nobody\", \"nothing\",\n",
    "    \"nowhere\", \"neither\", \"nor\", \"isn't\", \"aren't\", \"wasn't\",\n",
    "    \"weren't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\",\n",
    "    \"can't\", \"couldn't\", \"shouldn't\", \"haven't\", \"hasn't\", \"hadn't\",\n",
    "    \"cannot\", \"ain't\"\n",
    "]\n",
    "negation_words = set([...])  # from the list above\n",
    "\n",
    "def mark_negation(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    negate = False\n",
    "    for word in words:\n",
    "        word_clean = word.lower().strip(\".,!?\")\n",
    "        if word_clean in negation_words:\n",
    "            negate = True\n",
    "        if negate:\n",
    "            new_words.append(word + \"_NEG\")\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "        if word.endswith('.') or word.endswith('!') or word.endswith('?'):\n",
    "            negate = False\n",
    "    return ' '.join(new_words)\n",
    "    \n",
    "df_full_clean = df_full_clean.copy()\n",
    "df_full_clean['text'] = df_full_clean['text'].apply(mark_negation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "485af8b5-d926-46f8-ada1-032a2b5bd360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974272</th>\n",
       "      <td>I Have just seen this and felt it deserved a R...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974273</th>\n",
       "      <td>Omg how an earth is that a pen !!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974274</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974275</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974276</th>\n",
       "      <td>The fact that people still do not get that you...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974277 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       former versace store clerk sues over secret bl...      0   \n",
       "1       the roseanne revival catches up to our thorny ...      0   \n",
       "2       mom starting to fear sons web series closest t...      1   \n",
       "3       boehner just wants wife to listen not come up ...      1   \n",
       "4       jk rowling wishes snape happy birthday in the ...      0   \n",
       "...                                                   ...    ...   \n",
       "974272  I Have just seen this and felt it deserved a R...      0   \n",
       "974273                  Omg how an earth is that a pen !!      0   \n",
       "974274          Bringing Kanye and drake to a tl near you      0   \n",
       "974275  I love it when women are referred to as girl b...      1   \n",
       "974276  The fact that people still do not get that you...      1   \n",
       "\n",
       "               source  \n",
       "0       news_headline  \n",
       "1       news_headline  \n",
       "2       news_headline  \n",
       "3       news_headline  \n",
       "4       news_headline  \n",
       "...               ...  \n",
       "974272        twitter  \n",
       "974273        twitter  \n",
       "974274        twitter  \n",
       "974275        twitter  \n",
       "974276        twitter  \n",
       "\n",
       "[974277 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean = df_full_clean.reset_index(drop=True)\n",
    "df_full_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1acb49-455f-40cd-a4c4-904ce7db8287",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before training any models, it was essential to conduct an exploratory data analysis (EDA) to understand the structure, distribution, and linguistic properties of the dataset. This helped in making decisions about model architecture, preprocessing strategies, and feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Dataset Overview\n",
    "\n",
    "The cleaned dataset contains **972,891 text samples** with two columns:\n",
    "- `text`: the cleaned input text (tweets, headlines, or comments)\n",
    "- `label`: binary target variable (1 = sarcastic, 0 = not sarcastic)\n",
    "\n",
    "Descriptive statistics were used to inspect the structure of both columns. The `text` column had over 960,000 unique values, with the most frequent one appearing 16 times. The `label` column showed a perfectly balanced class distribution (50.05% sarcastic, 49.95% non-sarcastic).\n",
    "\n",
    "No missing values were present in the cleaned dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Class Balance\n",
    "\n",
    "To confirm the dataset was balanced, a bar chart was generated showing the distribution of sarcastic and non-sarcastic labels.\n",
    "\n",
    "> A balanced dataset is essential for training unbiased binary classifiers. Since the labels were almost perfectly split (≈50/50), no oversampling or undersampling was required.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Text Length Distribution (Word Count)\n",
    "\n",
    "The length of each text entry was computed by counting the number of words. A histogram was created to visualize this distribution.\n",
    "\n",
    "Text lengths were categorized as follows:\n",
    "- **Short**: 1–5 words\n",
    "- **Medium**: 10–20 words\n",
    "- **Long**: 30+ words\n",
    "\n",
    "Most entries were between 5 and 30 words long. The 90th, 95th, and 99th percentiles of text length were 21, 26, and 39 words respectively.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 Sequence Length Thresholding\n",
    "\n",
    "Because transformer-based models like BERT or BiLSTM require input sequences of fixed length, it was necessary to choose a maximum sequence length (`max_len`). Based on the distribution:\n",
    "\n",
    "- 99% of all texts were ≤ 39 words\n",
    "- A threshold of `max_len = 40` was selected to cover nearly all data\n",
    "- Only 0.6% of texts (5,890 out of 972k) were longer and excluded to avoid excessive padding or truncation\n",
    "\n",
    "This ensured efficient memory usage and minimized the risk of cutting off meaningful sarcastic cues.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 BERT Token Length Distribution\n",
    "\n",
    "To evaluate how many tokens BERT would generate for each text, the `bert-base-uncased` tokenizer was used. Most texts resulted in 15–30 tokens after tokenization.\n",
    "\n",
    "> This analysis confirmed that `max_len = 40` tokens would be a reasonable choice for BERT-based models as well.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.6 Most Common Words\n",
    "\n",
    "To explore linguistic patterns, the most frequent words in sarcastic and non-sarcastic texts were extracted separately. Before analysis, stopwords such as \"the\", \"is\", and \"to\" were removed using NLTK.\n",
    "\n",
    "Among sarcastic texts, frequently occurring words included:\n",
    "- `\"great\"`, `\"sure\"`, `\"awesome\"`\n",
    "\n",
    "These findings suggest a pattern of **positive-sounding words** being used sarcastically, which is consistent with prior sarcasm detection research.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.7 Sentiment Analysis with VADER\n",
    "\n",
    "The VADER sentiment analyzer was used to examine the polarity distribution of texts.\n",
    "\n",
    "- **Sarcastic texts** often had neutral or slightly positive compound sentiment scores.\n",
    "- **Non-sarcastic texts** showed a similar overall distribution, but with slightly more polarity (both positive and negative).\n",
    "\n",
    "This supports the hypothesis that sarcasm often appears **emotionally ambiguous**, and may not always be detectable through sentiment alone.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.8 Punctuation Features: Exclamation & Question Marks\n",
    "\n",
    "The frequency of `!` and `?` in each text was computed. Results showed:\n",
    "\n",
    "- `!` appeared in **9.61%** of texts, and among those, **72.7%** were sarcastic\n",
    "- `?` appeared in **11.88%** of texts, with only **47.2%** being sarcastic\n",
    "\n",
    "> This indicates that **exclamation marks may serve as a strong feature** for sarcasm due to their use in exaggeration or emphasis. In contrast, question marks were more evenly distributed.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.9 Negation Frequency\n",
    "\n",
    "Negation terms (e.g., \"not\", \"never\", \"can't\") were counted in each text.\n",
    "\n",
    "- Sarcastic texts containing negation: 60,842\n",
    "- Non-sarcastic texts containing negation: 62,705\n",
    "\n",
    "Negation was common across both classes but slightly more prevalent in sarcastic examples when normalized. This supports the idea that **negation cues can play a role in reversing literal sentiment** — a key trait of sarcasm.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd3a00aa-5638-4e2f-9ae2-b913023b5662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>974277</td>\n",
       "      <td>974277.000000</td>\n",
       "      <td>974277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>965902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>You dropped this</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>942607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.504463</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.499980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text          label  source\n",
       "count             974277  974277.000000  974277\n",
       "unique            965902            NaN       3\n",
       "top     You dropped this            NaN  Reddit\n",
       "freq                  16            NaN  942607\n",
       "mean                 NaN       0.504463     NaN\n",
       "std                  NaN       0.499980     NaN\n",
       "min                  NaN       0.000000     NaN\n",
       "25%                  NaN       0.000000     NaN\n",
       "50%                  NaN       1.000000     NaN\n",
       "75%                  NaN       1.000000     NaN\n",
       "max                  NaN       1.000000     NaN"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8ee8b57e-3760-4506-b208-3ad2e793dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_link    0\n",
       "headline        0\n",
       "is_sarcastic    0\n",
       "source          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46100889-cccf-4486-a782-29a57b874800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_full_clean['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8d7e0da8-eb7c-4568-b03e-419247544751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.504463\n",
       "0    0.495537\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_clean['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbe122-0a05-4a6b-98d7-a89af1f428d0",
   "metadata": {},
   "source": [
    "This creates a histogram — a bar chart showing how often texts of different lengths occur.\n",
    "\n",
    "Short texts (e.g., 1–5 words)\n",
    "\n",
    "Medium texts (e.g., 10–20 words)\n",
    "\n",
    "Long texts (e.g., 30+ words)\n",
    "\n",
    "🎯 Why This Is Important (for LSTM/BERT etc.)\n",
    "When using deep learning models like BiLSTM or BERT, you need to pad or truncate each input to a fixed length:\n",
    "\n",
    "Too short, and you might cut off meaningful sarcastic cues.\n",
    "\n",
    "Too long, and you waste memory and slow down training.\n",
    "\n",
    "By plotting the distribution of text lengths, you can:\n",
    "\n",
    "Choose a smart max sequence length (e.g., 95% of texts are ≤ 50 words → set max_len=50)\n",
    "\n",
    "Avoid over-padding or excessive truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a6b3f29-6017-4631-a81d-c61783eb5393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_full_clean['text_length'] = df_full_clean['text'].apply(lambda x: len(x.split()))\n",
    "df_full_clean['text_length'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a7d769f1-ddaf-4520-b837-97baf09a4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th percentile: 21.0\n",
      "95th percentile: 26.0\n",
      "99th percentile: 39.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "percentiles = np.percentile(df_full_clean['text_length'], [90, 95,99])\n",
    "print(f\"90th percentile: {percentiles[0]}\")\n",
    "print(f\"95th percentile: {percentiles[1]}\")\n",
    "print(f\"99th percentile: {percentiles[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7047ae94-a4c9-4c1c-9d8c-91660d856347",
   "metadata": {},
   "source": [
    "Given that sarcasm often depends on cues that occur at the end of a statement, truncating longer texts posed a risk of removing crucial signal. Therefore, texts exceeding 39 words—accounting for only 1% of the data—were excluded from the dataset. This decision preserved the semantic integrity of the remaining data and aligned with the short-form nature of sarcastic communication commonly found on social media.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4bbae0be-ca4b-4e3a-bbb9-ddcbe4b33882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th percentile: 20.0\n",
      "95th percentile: 25.0\n",
      "99th percentile: 33.0\n"
     ]
    }
   ],
   "source": [
    "# Filter out texts longer than 39 words\n",
    "df_full_clean['text_length'] = df_full_clean['text'].apply(lambda x: len(x.split()))\n",
    "df_full_clean = df_full_clean[df_full_clean['text_length'] <= 39]\n",
    "percentiles = np.percentile(df_full_clean['text_length'], [90, 95,99])\n",
    "print(f\"90th percentile: {percentiles[0]}\")\n",
    "print(f\"95th percentile: {percentiles[1]}\")\n",
    "print(f\"99th percentile: {percentiles[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be83e9b3-fc04-4ea5-95fb-83dcf56b22ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in df_full_clean['text']]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(token_lengths, bins=50)\n",
    "plt.title(\"BERT Token Length Distribution\")\n",
    "plt.xlabel(\"Number of tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "205c36af-2010-4b06-ace6-35936721a7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51fdc8ed-9da9-4913-aa8c-825e1fb19fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining samples: 959069\n",
      "Removed: 5914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/v2v1xtnj6b90rtp9nqmdrwvm0000gn/T/ipykernel_69395/1800864935.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_full_clean['bert_token_length'] = df_full_clean['text'].apply(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get BERT token length for each text\n",
    "df_full_clean['bert_token_length'] = df_full_clean['text'].apply(\n",
    "    lambda x: len(tokenizer.encode(x, add_special_tokens=True))\n",
    ")\n",
    "\n",
    "# Step 2: Filter out samples longer than 40 tokens\n",
    "df_filtered = df_full_clean[df_full_clean['bert_token_length'] <= 40].reset_index(drop=True)\n",
    "\n",
    "print(f\"Remaining samples: {len(df_filtered)}\")\n",
    "print(f\"Removed: {len(df_full_clean) - len(df_filtered)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a14c14b7-1687-4d29-a632-6327bf0941f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoding = tokenizer(\n",
    "    list(df_filtered['text']),\n",
    "    padding='max_length',\n",
    "    truncation=False,  # ✅ No need to truncate now!\n",
    "    max_length=40,\n",
    "    return_tensors='pt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "20ae718a-68cf-4b72-9dee-72cb7393708d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 168367),\n",
       " ('to', 118182),\n",
       " ('a', 109146),\n",
       " ('is', 94230),\n",
       " ('you', 79291),\n",
       " ('not', 79246),\n",
       " ('I', 77231),\n",
       " ('and', 73969),\n",
       " ('that', 69749),\n",
       " ('of', 68979),\n",
       " ('are', 51640),\n",
       " ('in', 50352),\n",
       " ('it', 47076),\n",
       " ('for', 45244),\n",
       " ('they', 36123),\n",
       " ('have', 36080),\n",
       " ('be', 35333),\n",
       " ('do', 31285),\n",
       " ('on', 30174),\n",
       " ('so', 29315)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sarcastic_words = ' '.join(df_filtered[df_filtered['label']==1]['text']).split()\n",
    "non_sarcastic_words = ' '.join(df_filtered[df_filtered['label']==0]['text']).split()\n",
    "\n",
    "Counter(sarcastic_words).most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c7bfb-2756-436a-ad3a-14d82c6ad0cb",
   "metadata": {},
   "source": [
    "\"As expected, the most frequent words across both sarcastic and non-sarcastic texts were common stopwords such as 'the', 'is', and 'to'. These were removed prior to lexical analysis to highlight sarcasm-relevant vocabulary. After filtering, frequent sarcastic cues such as 'great', 'sure', and 'awesome' emerged as dominant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1c47af5-734a-43ef-bf8f-8f0c37e0135e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/evelinaivanova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Yeah', 26256),\n",
       " ('like', 24239),\n",
       " ('would', 19485),\n",
       " ('people', 18858),\n",
       " ('get', 15583),\n",
       " ('know', 13881),\n",
       " ('one', 12079),\n",
       " ('Well', 11259),\n",
       " ('sure', 10966),\n",
       " ('cannot', 10526),\n",
       " ('good', 9784),\n",
       " ('Yes', 8982),\n",
       " ('make', 8640),\n",
       " ('really', 8461),\n",
       " ('think', 8164),\n",
       " ('right', 8130),\n",
       " ('see', 8018),\n",
       " ('going', 8016),\n",
       " ('man', 7869),\n",
       " ('never', 7785)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "sarcastic_words = ' '.join(df_filtered[df_filtered['label']==1]['text']).split()\n",
    "sarcastic_words_clean = [word for word in sarcastic_words if word.lower() not in stop_words]\n",
    "\n",
    "from collections import Counter\n",
    "Counter(sarcastic_words_clean).most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "56ed77ad-73e3-42c3-99c0-5d9f9835a8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text_length</th>\n",
       "      <th>bert_token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>959069</td>\n",
       "      <td>959069.000000</td>\n",
       "      <td>959069</td>\n",
       "      <td>959069.000000</td>\n",
       "      <td>959069.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>950694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>You dropped this</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>927846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.507560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.753029</td>\n",
       "      <td>14.04884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.499943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.605641</td>\n",
       "      <td>7.04271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text          label  source    text_length  \\\n",
       "count             959069  959069.000000  959069  959069.000000   \n",
       "unique            950694            NaN       3            NaN   \n",
       "top     You dropped this            NaN  Reddit            NaN   \n",
       "freq                  16            NaN  927846            NaN   \n",
       "mean                 NaN       0.507560     NaN      10.753029   \n",
       "std                  NaN       0.499943     NaN       6.605641   \n",
       "min                  NaN       0.000000     NaN       1.000000   \n",
       "25%                  NaN       0.000000     NaN       6.000000   \n",
       "50%                  NaN       1.000000     NaN       9.000000   \n",
       "75%                  NaN       1.000000     NaN      14.000000   \n",
       "max                  NaN       1.000000     NaN      38.000000   \n",
       "\n",
       "        bert_token_length  \n",
       "count        959069.00000  \n",
       "unique                NaN  \n",
       "top                   NaN  \n",
       "freq                  NaN  \n",
       "mean             14.04884  \n",
       "std               7.04271  \n",
       "min               4.00000  \n",
       "25%               9.00000  \n",
       "50%              13.00000  \n",
       "75%              18.00000  \n",
       "max              40.00000  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "99794947-948e-4ad9-a2f1-6b56dc56cae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                 0\n",
       "label                0\n",
       "source               0\n",
       "text_length          0\n",
       "bert_token_length    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a6f1b552-2b4b-4717-81ff-47c13bbe6fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text_length</th>\n",
       "      <th>bert_token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>bad to the bone to be used in film</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5131</th>\n",
       "      <td>bad to the bone to be used in film</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>fool me once</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28906</th>\n",
       "      <td>GhostOfTsushima PS5Share GhostofTsushima</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28907</th>\n",
       "      <td>GhostOfTsushima PS5Share GhostofTsushima</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957560</th>\n",
       "      <td>Oh yeah totally</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957696</th>\n",
       "      <td>I was waiting for the</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957715</th>\n",
       "      <td>Wow that was quick</td>\n",
       "      <td>1</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958519</th>\n",
       "      <td>so sorry for your loss</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958951</th>\n",
       "      <td>this!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14539 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text  label         source  \\\n",
       "1678          bad to the bone to be used in film      1  news_headline   \n",
       "5131          bad to the bone to be used in film      1  news_headline   \n",
       "13769                               fool me once      0  news_headline   \n",
       "28906   GhostOfTsushima PS5Share GhostofTsushima      0        twitter   \n",
       "28907   GhostOfTsushima PS5Share GhostofTsushima      0        twitter   \n",
       "...                                          ...    ...            ...   \n",
       "957560                           Oh yeah totally      1         Reddit   \n",
       "957696                     I was waiting for the      1         Reddit   \n",
       "957715                        Wow that was quick      1         Reddit   \n",
       "958519                    so sorry for your loss      0        twitter   \n",
       "958951                                     this!      0        twitter   \n",
       "\n",
       "        text_length  bert_token_length  \n",
       "1678              9                 11  \n",
       "5131              9                 11  \n",
       "13769             3                  5  \n",
       "28906             3                 14  \n",
       "28907             3                 14  \n",
       "...             ...                ...  \n",
       "957560            3                  5  \n",
       "957696            5                  7  \n",
       "957715            4                  6  \n",
       "958519            5                  7  \n",
       "958951            1                  4  \n",
       "\n",
       "[14539 rows x 5 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_texts = df_filtered[df_filtered.duplicated(subset='text', keep=False)]\n",
    "duplicate_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cbd81167-0154-4915-a805-ba5f18785c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop_duplicates(subset='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "23c16392-d39e-497b-9c06-ed1199c8c327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text_length</th>\n",
       "      <th>bert_token_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>950694</td>\n",
       "      <td>950694.000000</td>\n",
       "      <td>950694</td>\n",
       "      <td>950694.000000</td>\n",
       "      <td>950694.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>950694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>919475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.507983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.816620</td>\n",
       "      <td>14.121312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.499937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.597763</td>\n",
       "      <td>7.029071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text          label  \\\n",
       "count                                              950694  950694.000000   \n",
       "unique                                             950694            NaN   \n",
       "top     former versace store clerk sues over secret bl...            NaN   \n",
       "freq                                                    1            NaN   \n",
       "mean                                                  NaN       0.507983   \n",
       "std                                                   NaN       0.499937   \n",
       "min                                                   NaN       0.000000   \n",
       "25%                                                   NaN       0.000000   \n",
       "50%                                                   NaN       1.000000   \n",
       "75%                                                   NaN       1.000000   \n",
       "max                                                   NaN       1.000000   \n",
       "\n",
       "        source    text_length  bert_token_length  \n",
       "count   950694  950694.000000      950694.000000  \n",
       "unique       3            NaN                NaN  \n",
       "top     Reddit            NaN                NaN  \n",
       "freq    919475            NaN                NaN  \n",
       "mean       NaN      10.816620          14.121312  \n",
       "std        NaN       6.597763           7.029071  \n",
       "min        NaN       1.000000           4.000000  \n",
       "25%        NaN       6.000000           9.000000  \n",
       "50%        NaN       9.000000          13.000000  \n",
       "75%        NaN      14.000000          18.000000  \n",
       "max        NaN      38.000000          40.000000  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844b5ac-14f4-4868-a41c-843fcaf64e61",
   "metadata": {},
   "source": [
    "VADER sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "93180f0c-2280-47e3-b3df-8da704e85880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Generate sentiment scores WITHOUT modifying df_filtered\n",
    "vader_scores = df_filtered['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "vader_scores.hist(bins=50, edgecolor='black')\n",
    "plt.title(\"VADER Compound Sentiment Score Distribution\")\n",
    "plt.xlabel(\"Sentiment Score (Compound: -1 = Negative, +1 = Positive)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b378689a-9ba0-4646-9732-1a22d3541600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarcastic_only = df_filtered[df_filtered['label'] == 1]\n",
    "\n",
    "vader_scores_sarcastic = sarcastic_only['text'].apply(\n",
    "    lambda x: analyzer.polarity_scores(x)['compound']\n",
    ")\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 5))\n",
    "vader_scores_sarcastic.hist(bins=50, edgecolor='black')\n",
    "plt.title(\"VADER Sentiment Distribution (Sarcastic Texts Only)\")\n",
    "plt.xlabel(\"Compound Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "594e0d3f-997f-46bc-a484-c7784e2a9cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarcastic_only = df_filtered[df_filtered['label'] == 0]\n",
    "\n",
    "vader_scores_sarcastic = sarcastic_only['text'].apply(\n",
    "    lambda x: analyzer.polarity_scores(x)['compound']\n",
    ")\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 5))\n",
    "vader_scores_sarcastic.hist(bins=50, edgecolor='black')\n",
    "plt.title(\"VADER Sentiment Distribution (Non-Sarcastic Texts Only)\")\n",
    "plt.xlabel(\"Compound Sentiment Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb55c635-6a4f-4f99-9c75-f848b2b980a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered['exclamations'] = df_filtered['text'].apply(lambda x: x.count('!'))\n",
    "df_filtered['question_marks'] = df_filtered['text'].apply(lambda x: x.count('?'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9c05bf18-d703-4ec3-b3c7-28d2e8e3085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text_length</th>\n",
       "      <th>bert_token_length</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>question_marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959064</th>\n",
       "      <td>I would imply that this friend has a tendency ...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959065</th>\n",
       "      <td>I Have just seen this and felt it deserved a R...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959066</th>\n",
       "      <td>Omg how an earth is that a pen !!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959067</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959068</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>950694 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       former versace store clerk sues over secret bl...      0   \n",
       "1       the roseanne revival catches up to our thorny ...      0   \n",
       "2       mom starting to fear sons web series closest t...      1   \n",
       "3       boehner just wants wife to listen not come up ...      1   \n",
       "4       jk rowling wishes snape happy birthday in the ...      0   \n",
       "...                                                   ...    ...   \n",
       "959064  I would imply that this friend has a tendency ...      0   \n",
       "959065  I Have just seen this and felt it deserved a R...      0   \n",
       "959066                  Omg how an earth is that a pen !!      0   \n",
       "959067          Bringing Kanye and drake to a tl near you      0   \n",
       "959068  I love it when women are referred to as girl b...      1   \n",
       "\n",
       "               source  text_length  bert_token_length  exclamations  \\\n",
       "0       news_headline           12                 17             0   \n",
       "1       news_headline           14                 18             0   \n",
       "2       news_headline           14                 17             0   \n",
       "3       news_headline           13                 20             0   \n",
       "4       news_headline           11                 16             0   \n",
       "...               ...          ...                ...           ...   \n",
       "959064        twitter           19                 27             0   \n",
       "959065        twitter           15                 20             0   \n",
       "959066        twitter            9                 13             2   \n",
       "959067        twitter            9                 12             0   \n",
       "959068        twitter           17                 20             1   \n",
       "\n",
       "        question_marks  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "959064               0  \n",
       "959065               0  \n",
       "959066               0  \n",
       "959067               0  \n",
       "959068               0  \n",
       "\n",
       "[950694 rows x 7 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "025d582c-5bd2-4c14-aff0-fe59d31b85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_filtered[df_filtered['exclamations'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "39b95e9d-490a-4a75-96a7-2eb91f4eb545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text_length</th>\n",
       "      <th>bert_token_length</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>question_marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>the source of donald trumps military expertise...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>god humbled to be the answer to jeopardy! clue</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>vegas baby! well minus our babies</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>meet the other baldwin brother james!</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>whoops! selfie snapper smashes sculpture days ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959048</th>\n",
       "      <td>Thank you my lovely friend for mentioning Heel...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959050</th>\n",
       "      <td>Thank you so much for this lovely review Mandy...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959053</th>\n",
       "      <td>Ugh! Cannot believe my back has gone at the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959059</th>\n",
       "      <td>Proof of unjustified victimisation!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959068</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90866 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "125     the source of donald trumps military expertise...      0   \n",
       "185        god humbled to be the answer to jeopardy! clue      1   \n",
       "314                     vegas baby! well minus our babies      0   \n",
       "371                 meet the other baldwin brother james!      1   \n",
       "523     whoops! selfie snapper smashes sculpture days ...      0   \n",
       "...                                                   ...    ...   \n",
       "959048  Thank you my lovely friend for mentioning Heel...      0   \n",
       "959050  Thank you so much for this lovely review Mandy...      0   \n",
       "959053  Ugh! Cannot believe my back has gone at the st...      0   \n",
       "959059                Proof of unjustified victimisation!      0   \n",
       "959068  I love it when women are referred to as girl b...      1   \n",
       "\n",
       "               source  text_length  bert_token_length  exclamations  \\\n",
       "125     news_headline            9                 13             1   \n",
       "185     news_headline            9                 13             1   \n",
       "314     news_headline            6                  9             1   \n",
       "371     news_headline            6                  9             1   \n",
       "523     news_headline            9                 16             1   \n",
       "...               ...          ...                ...           ...   \n",
       "959048        twitter           22                 26             1   \n",
       "959050        twitter           15                 18             1   \n",
       "959053        twitter           22                 26             1   \n",
       "959059        twitter            4                 10             1   \n",
       "959068        twitter           17                 20             1   \n",
       "\n",
       "        question_marks  \n",
       "125                  0  \n",
       "185                  0  \n",
       "314                  0  \n",
       "371                  0  \n",
       "523                  0  \n",
       "...                ...  \n",
       "959048               0  \n",
       "959050               0  \n",
       "959053               0  \n",
       "959059               0  \n",
       "959068               0  \n",
       "\n",
       "[90866 rows x 7 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "21976d2f-067d-4471-8516-8d8ea10aeb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts with '!': 91457 (9.62%)\n",
      "Texts with '?': 112880 (11.87%)\n"
     ]
    }
   ],
   "source": [
    "has_excl = df_filtered['exclamations'] > 0\n",
    "has_quest = df_filtered['question_marks'] > 0\n",
    "\n",
    "num_excl = has_excl.sum()\n",
    "num_quest = has_quest.sum()\n",
    "\n",
    "print(f\"Texts with '!': {num_excl} ({num_excl / len(df_filtered):.2%})\")\n",
    "print(f\"Texts with '?': {num_quest} ({num_quest / len(df_filtered):.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77e6a6fe-abaa-4806-8878-2b45e4e1ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Among texts with '!':\n",
      "label\n",
      "1    0.72621\n",
      "0    0.27379\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Among texts with '?':\n",
      "label\n",
      "0    0.528614\n",
      "1    0.471386\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sarcastic = 1\n",
    "sarcastic_excl = df_filtered[has_excl]['label'].value_counts(normalize=True)\n",
    "sarcastic_quest = df_filtered[has_quest]['label'].value_counts(normalize=True)\n",
    "\n",
    "print(\"\\nAmong texts with '!':\")\n",
    "print(sarcastic_excl)\n",
    "\n",
    "print(\"\\nAmong texts with '?':\")\n",
    "print(sarcastic_quest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01042c3d-d6f9-463a-abf4-ef1db34cbf8b",
   "metadata": {},
   "source": [
    "Exploratory analysis revealed that exclamation marks (!) were present in 9.61% of texts, and among those, 72.7% were sarcastic. This suggests that ! is a strong cue for exaggerated or emphatic tone common in sarcastic expression. In contrast, question marks (?) appeared in 11.88% of texts but did not show a strong skew toward sarcasm, with only 47.2% of those texts being sarcastic. These findings suggest that exclamation marks may serve as a valuable linguistic feature for sarcasm detection models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "86d4c3a1-7ad4-4251-8fea-3d777fddea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "negations = [\"not\", \"no\", \"never\", \"n't\"]\n",
    "df_filtered['negation_count'] = df_filtered['text'].apply(lambda x: sum(word in x.lower() for word in negations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4f1722a6-1719-4776-ad20-c208e22db23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text_length</th>\n",
       "      <th>bert_token_length</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>question_marks</th>\n",
       "      <th>negation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret bl...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the roseanne revival catches up to our thorny ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear sons web series closest t...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen not come up ...</td>\n",
       "      <td>1</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jk rowling wishes snape happy birthday in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>news_headline</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959064</th>\n",
       "      <td>I would imply that this friend has a tendency ...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959065</th>\n",
       "      <td>I Have just seen this and felt it deserved a R...</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959066</th>\n",
       "      <td>Omg how an earth is that a pen !!</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959067</th>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>twitter</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959068</th>\n",
       "      <td>I love it when women are referred to as girl b...</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>950694 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  \\\n",
       "0       former versace store clerk sues over secret bl...      0   \n",
       "1       the roseanne revival catches up to our thorny ...      0   \n",
       "2       mom starting to fear sons web series closest t...      1   \n",
       "3       boehner just wants wife to listen not come up ...      1   \n",
       "4       jk rowling wishes snape happy birthday in the ...      0   \n",
       "...                                                   ...    ...   \n",
       "959064  I would imply that this friend has a tendency ...      0   \n",
       "959065  I Have just seen this and felt it deserved a R...      0   \n",
       "959066                  Omg how an earth is that a pen !!      0   \n",
       "959067          Bringing Kanye and drake to a tl near you      0   \n",
       "959068  I love it when women are referred to as girl b...      1   \n",
       "\n",
       "               source  text_length  bert_token_length  exclamations  \\\n",
       "0       news_headline           12                 17             0   \n",
       "1       news_headline           14                 18             0   \n",
       "2       news_headline           14                 17             0   \n",
       "3       news_headline           13                 20             0   \n",
       "4       news_headline           11                 16             0   \n",
       "...               ...          ...                ...           ...   \n",
       "959064        twitter           19                 27             0   \n",
       "959065        twitter           15                 20             0   \n",
       "959066        twitter            9                 13             2   \n",
       "959067        twitter            9                 12             0   \n",
       "959068        twitter           17                 20             1   \n",
       "\n",
       "        question_marks  negation_count  \n",
       "0                    0               1  \n",
       "1                    0               0  \n",
       "2                    0               0  \n",
       "3                    0               2  \n",
       "4                    0               0  \n",
       "...                ...             ...  \n",
       "959064               0               1  \n",
       "959065               0               0  \n",
       "959066               0               0  \n",
       "959067               0               0  \n",
       "959068               0               0  \n",
       "\n",
       "[950694 rows x 8 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d5524782-b58b-4a82-aa95-f369b4023811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic texts with negation: 158288\n"
     ]
    }
   ],
   "source": [
    "Neg = df_filtered[(df_filtered['negation_count'] > 0) & (df_filtered['label'] == 1)]\n",
    "num_negated_sarcastic = len(df_filtered[(df_filtered['negation_count'] > 0) & (df_filtered['label'] == 1)])\n",
    "print(f\"Sarcastic texts with negation: {num_negated_sarcastic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "317d804e-c991-4608-8f2b-a88f9a7dbea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486170"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['negation_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "74233f23-03fe-4c9a-b840-96442e858298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-sarcastic texts with negation: 134093\n"
     ]
    }
   ],
   "source": [
    "num_negated_sarcastic = len(df_filtered[(df_filtered['negation_count'] > 0) & (df_filtered['label'] == 0)])\n",
    "print(f\"Non-sarcastic texts with negation: {num_negated_sarcastic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d65ebc-1eee-4163-8be9-bef6d4384c7a",
   "metadata": {},
   "source": [
    "to do 9. Topic Modeling (LDA or BERTopic)\n",
    "To explore common topics in sarcastic vs. non-sarcastic text.\n",
    "\n",
    "10. Emotion Detection\n",
    "Use an emotional lexicon (e.g., NRC Emotion Lexicon) or a model to tag emotions like joy, anger, sadness, etc.\n",
    "✔️ Useful if you're incorporating emotional embeddings later.\n",
    "\n",
    "11. N-gram Collocations\n",
    "See if sarcastic phrases appear as bigram/trigram combinations (\"oh great\", \"yeah right\").\n",
    "\n",
    "12. Dataset Comparison\n",
    "If combining datasets (News, Twitter, Reddit), analyze:\n",
    "\n",
    "Avg. length per source.\n",
    "\n",
    "Label distribution per source.\n",
    "\n",
    "Text style differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26b9f1-ab0f-4417-89cc-f8b239c9f234",
   "metadata": {},
   "source": [
    "## Dataset Audit: Before vs After Cleaning\n",
    "\n",
    "\n",
    "Headline findings:\n",
    "\n",
    "Size reduced: 1.05M → 0.95M rows (−9.23% overall)\n",
    "\n",
    "Biggest drop: Twitter (−53%), Reddit (−9%), News (<1%)\n",
    "\n",
    "Class balance: stays near even (before: 50/50, after: 49% vs 51%)\n",
    "\n",
    "Length distributions: extremes removed (tokens max 2222→38; chars max 10k→472)\n",
    "\n",
    "Markers: emojis, hashtags, and excessive quotes largely eliminated\n",
    "\n",
    "Language signal: key words/phrases (e.g., “looks like”, “sounds like”, “pretty sure”) remain consistent across labels\n",
    "\n",
    "Takeaway: cleaning removed noise and outliers, stabilized text lengths, and reduced reliance on superficial cues while keeping the semantic patterns intact — making the dataset more reliable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "313002f0-e7ed-4998-9435-7ef7a6f2d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Dataset Analysis: BEFORE vs AFTER cleaning\n",
    "# - Works with df_full (raw) and df_filtered (cleaned)\n",
    "# - Grouping column is `source`\n",
    "# - Outputs CSVs & PNGs into data_reports/\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Config ----------\n",
    "OUTPUT_DIR = \"data_reports\"      # all CSVs/plots will go here\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"              # 0/1\n",
    "SOURCE_COL = \"source\"            # <- your grouping column\n",
    "TOP_K = 25                       # top n-grams to list\n",
    "NGRAM_MAX_FEATURES = 50000       # cap for vectorizer\n",
    "LENGTH_HIST_BINS = 40\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Helper: robust emoji detection ----------\n",
    "EMOJI_RE = re.compile(\n",
    "    \"[\"                                  # start char group\n",
    "    \"\\U0001F300-\\U0001F5FF\"              # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"              # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"              # transport & map\n",
    "    \"\\U0001F700-\\U0001F77F\"\n",
    "    \"\\U0001F780-\\U0001F7FF\"\n",
    "    \"\\U0001F800-\\U0001F8FF\"\n",
    "    \"\\U0001F900-\\U0001F9FF\"              # supplemental symbols & pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"\n",
    "    \"\\U0001FA70-\\U0001FAFF\"\n",
    "    \"\\U00002600-\\U000026FF\"              # misc symbols\n",
    "    \"\\U00002700-\\U000027BF\"              # dingbats\n",
    "    \"]\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if TEXT_COL not in df.columns:\n",
    "        raise ValueError(f\"Expected a '{TEXT_COL}' column.\")\n",
    "    if LABEL_COL not in df.columns:\n",
    "        df[LABEL_COL] = np.nan\n",
    "    else:\n",
    "        df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors=\"coerce\")\n",
    "    if SOURCE_COL not in df.columns:\n",
    "        df[SOURCE_COL] = \"unknown\"\n",
    "    return df\n",
    "\n",
    "def basic_clean_for_token_len(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    t = re.sub(r\"[^A-Za-z0-9\\s']\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def compute_marker_counts(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return 0,0,0,0,0,0\n",
    "    emojis = len(EMOJI_RE.findall(text))\n",
    "    hashtags = text.count(\"#\")\n",
    "    qmarks = text.count(\"?\")\n",
    "    excls = text.count(\"!\")\n",
    "    quotes = text.count('\"') + text.count(\"'\")\n",
    "    slash_s = 1 if \"/s\" in text.lower() else 0\n",
    "    return emojis, hashtags, qmarks, excls, quotes, slash_s\n",
    "\n",
    "def class_distribution(df: pd.DataFrame):\n",
    "    counts = df[LABEL_COL].value_counts(dropna=False)\n",
    "    total = len(df)\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"n_label_0\": int(counts.get(0, 0)),\n",
    "        \"n_label_1\": int(counts.get(1, 0)),\n",
    "        \"pct_label_0\": round(100*counts.get(0,0)/total, 2) if total else 0.0,\n",
    "        \"pct_label_1\": round(100*counts.get(1,0)/total, 2) if total else 0.0,\n",
    "        \"has_nan_labels\": int(counts.get(np.nan, 0)),\n",
    "    }\n",
    "\n",
    "def describe_series(x: pd.Series):\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return {\n",
    "        \"count\": int(x.shape[0]),\n",
    "        \"mean\": round(float(np.nanmean(x)), 2) if x.shape[0] else np.nan,\n",
    "        \"std\": round(float(np.nanstd(x)), 2) if x.shape[0] else np.nan,\n",
    "        \"min\": int(np.nanmin(x)) if x.shape[0] else np.nan,\n",
    "        \"p25\": int(np.nanpercentile(x, 25)) if x.shape[0] else np.nan,\n",
    "        \"median\": int(np.nanmedian(x)) if x.shape[0] else np.nan,\n",
    "        \"p75\": int(np.nanpercentile(x, 75)) if x.shape[0] else np.nan,\n",
    "        \"max\": int(np.nanmax(x)) if x.shape[0] else np.nan,\n",
    "    }\n",
    "\n",
    "def plot_length_hist(lengths: pd.Series, title: str, outfile: str):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(lengths.dropna(), bins=LENGTH_HIST_BINS)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Length\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "def top_ngrams(texts, ngram_range=(1,1), top_k=TOP_K, stop_words=\"english\"):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range,\n",
    "                                 stop_words=stop_words,\n",
    "                                 max_features=NGRAM_MAX_FEATURES)\n",
    "    try:\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "    except ValueError:\n",
    "        return []\n",
    "    sums = np.asarray(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "    order = np.argsort(-sums)[:top_k]\n",
    "    return list(zip(vocab[order], sums[order].tolist()))\n",
    "\n",
    "def run_dataset_analysis(df: pd.DataFrame, name_tag=\"before\"):\n",
    "    \"\"\"\n",
    "    df: DataFrame with columns: text, label, source\n",
    "    name_tag: 'before' (df_full) or 'after' (df_filtered)\n",
    "    Outputs CSV + PNG files into OUTPUT_DIR\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running analysis for: {name_tag.upper()} ===\")\n",
    "    df = ensure_columns(df.copy())\n",
    "    # drop empty/whitespace-only text\n",
    "    df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "    df = df[~df[TEXT_COL].str.strip().eq(\"\")]\n",
    "\n",
    "    # ---- OVERALL size & class distribution\n",
    "    overall_dist = class_distribution(df)\n",
    "    pd.DataFrame([overall_dist]).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_overall_sizes.csv\"), index=False)\n",
    "\n",
    "    # ---- PER-SOURCE size & class distribution\n",
    "    per_source_rows = []\n",
    "    for src, df_s in df.groupby(SOURCE_COL):\n",
    "        d = class_distribution(df_s)\n",
    "        d.update({SOURCE_COL: src})\n",
    "        per_source_rows.append(d)\n",
    "    pd.DataFrame(per_source_rows).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_per_source_sizes.csv\"), index=False)\n",
    "\n",
    "    # ---- LENGTH STATS (chars/tokens) overall and per source\n",
    "    lengths_chars = df[TEXT_COL].astype(str).str.len()\n",
    "    lengths_tokens = df[TEXT_COL].astype(str).map(basic_clean_for_token_len).str.split().map(len)\n",
    "\n",
    "    rows = [{\n",
    "        \"scope\": \"overall\",\n",
    "        \"chars\": json.dumps(describe_series(lengths_chars)),\n",
    "        \"tokens\": json.dumps(describe_series(lengths_tokens)),\n",
    "    }]\n",
    "    for src, df_s in df.groupby(SOURCE_COL):\n",
    "        c = df_s[TEXT_COL].astype(str).str.len()\n",
    "        t = df_s[TEXT_COL].astype(str).map(basic_clean_for_token_len).str.split().map(len)\n",
    "        rows.append({\n",
    "            \"scope\": f\"source::{src}\",\n",
    "            \"chars\": json.dumps(describe_series(c)),\n",
    "            \"tokens\": json.dumps(describe_series(t)),\n",
    "        })\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_length_stats.csv\"), index=False)\n",
    "\n",
    "    # ---- LENGTH HISTOGRAMS\n",
    "    plot_length_hist(lengths_chars, f\"Character length distribution ({name_tag})\", os.path.join(OUTPUT_DIR, f\"{name_tag}_char_length_hist.png\"))\n",
    "    plot_length_hist(lengths_tokens, f\"Token length distribution ({name_tag})\", os.path.join(OUTPUT_DIR, f\"{name_tag}_token_length_hist.png\"))\n",
    "    for src, df_s in df.groupby(SOURCE_COL):\n",
    "        c = df_s[TEXT_COL].astype(str).str.len()\n",
    "        t = df_s[TEXT_COL].astype(str).map(basic_clean_for_token_len).str.split().map(len)\n",
    "        plot_length_hist(c, f\"Char length ({src}, {name_tag})\", os.path.join(OUTPUT_DIR, f\"{name_tag}_{src}_char_length_hist.png\"))\n",
    "        plot_length_hist(t, f\"Token length ({src}, {name_tag})\", os.path.join(OUTPUT_DIR, f\"{name_tag}_{src}_token_length_hist.png\"))\n",
    "\n",
    "    # ---- MARKERS: emoji, hashtags, punctuation, quotes, '/s'\n",
    "    marker_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        emojis, hashtags, qmarks, excls, quotes, slash_s = compute_marker_counts(row[TEXT_COL])\n",
    "        marker_rows.append({\n",
    "            SOURCE_COL: row[SOURCE_COL],\n",
    "            \"label\": row[LABEL_COL],\n",
    "            \"emojis\": emojis,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"question_marks\": qmarks,\n",
    "            \"exclamation_marks\": excls,\n",
    "            \"quotes\": quotes,\n",
    "            \"slash_s\": slash_s,\n",
    "        })\n",
    "    markers_df = pd.DataFrame(marker_rows)\n",
    "    agg = markers_df.groupby([SOURCE_COL, \"label\"]).agg(\n",
    "        n=(\"emojis\", \"size\"),\n",
    "        emojis_mean=(\"emojis\", \"mean\"),\n",
    "        hashtags_mean=(\"hashtags\", \"mean\"),\n",
    "        qmarks_mean=(\"question_marks\", \"mean\"),\n",
    "        excls_mean=(\"exclamation_marks\", \"mean\"),\n",
    "        quotes_mean=(\"quotes\", \"mean\"),\n",
    "        slash_s_rate=(\"slash_s\", \"mean\")\n",
    "    ).reset_index()\n",
    "    agg.to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_marker_stats_by_source_class.csv\"), index=False)\n",
    "\n",
    "    # ---- TOP N-GRAMS (overall, by class, by source)\n",
    "    texts = df[TEXT_COL].astype(str).tolist()\n",
    "    uni_all = top_ngrams(texts, (1,1), TOP_K)\n",
    "    bi_all  = top_ngrams(texts, (2,2), TOP_K)\n",
    "    pd.DataFrame(uni_all, columns=[\"ngram\",\"count\"]).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_top_unigrams_overall.csv\"), index=False)\n",
    "    pd.DataFrame(bi_all,  columns=[\"ngram\",\"count\"]).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_top_bigrams_overall.csv\"), index=False)\n",
    "\n",
    "    for target in [0, 1]:\n",
    "        subset = df[df[LABEL_COL] == target]\n",
    "        uni_c = top_ngrams(subset[TEXT_COL].astype(str).tolist(), (1,1), TOP_K)\n",
    "        bi_c  = top_ngrams(subset[TEXT_COL].astype(str).tolist(), (2,2), TOP_K)\n",
    "        pd.DataFrame(uni_c, columns=[\"ngram\",\"count\"]).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_top_unigrams_label{target}.csv\"), index=False)\n",
    "        pd.DataFrame(bi_c,  columns=[\"ngram\",\"count\"]).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_top_bigrams_label{target}.csv\"), index=False)\n",
    "\n",
    "    for src, df_s in df.groupby(SOURCE_COL):\n",
    "        uni_s = top_ngrams(df_s[TEXT_COL].astype(str).tolist(), (1,1), TOP_K)\n",
    "        bi_s  = top_ngrams(df_s[TEXT_COL].astype(str).tolist(), (2,2), TOP_K)\n",
    "        pd.DataFrame(uni_s, columns=[\"ngram\",\"count\"]).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_top_unigrams_{src}.csv\"), index=False)\n",
    "        pd.DataFrame(bi_s,  columns=[\"ngram\",\"count\"]).to_csv(os.path.join(OUTPUT_DIR, f\"{name_tag}_top_bigrams_{src}.csv\"), index=False)\n",
    "\n",
    "    print(f\"Finished analytics for: {name_tag}. CSVs/PNGs saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# ---------- Optional: removal summary (how much was dropped) ----------\n",
    "def removal_summary(df_before: pd.DataFrame, df_after: pd.DataFrame):\n",
    "    df_before = ensure_columns(df_before.copy())\n",
    "    df_after  = ensure_columns(df_after.copy())\n",
    "    before_n, after_n = len(df_before), len(df_after)\n",
    "    removed = before_n - after_n\n",
    "    pct = round(100 * removed / before_n, 2) if before_n else 0.0\n",
    "\n",
    "    def counts_by_source(df):\n",
    "        return df[SOURCE_COL].value_counts().to_dict()\n",
    "\n",
    "    src_before = counts_by_source(df_before)\n",
    "    src_after  = counts_by_source(df_after)\n",
    "    rows = []\n",
    "    all_srcs = sorted(set(src_before) | set(src_after))\n",
    "    for s in all_srcs:\n",
    "        b = src_before.get(s, 0)\n",
    "        a = src_after.get(s, 0)\n",
    "        r = b - a\n",
    "        p = round(100 * r / b, 2) if b else 0.0\n",
    "        rows.append({SOURCE_COL: s, \"before\": b, \"after\": a, \"removed\": r, \"removed_pct\": p})\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(OUTPUT_DIR, \"removal_summary_per_source.csv\"), index=False)\n",
    "    pd.DataFrame([{\"before\": before_n, \"after\": after_n, \"removed\": removed, \"removed_pct\": pct}]) \\\n",
    "        .to_csv(os.path.join(OUTPUT_DIR, \"removal_summary_overall.csv\"), index=False)\n",
    "    print(\"Saved removal summaries to data_reports/\")\n",
    "    return {\"overall\": {\"before\": before_n, \"after\": after_n, \"removed\": removed, \"removed_pct\": pct},\n",
    "            \"per_source\": rows}\n",
    "\n",
    "# ================================\n",
    "# USAGE (run these after defining df_full and df_filtered):\n",
    "# run_dataset_analysis(df_full,    name_tag=\"before\")\n",
    "# run_dataset_analysis(df_filtered, name_tag=\"after\")\n",
    "# removal_summary(df_full, df_filtered)\n",
    "# ================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cdc23afd-9b67-4bb8-8153-4bfb1b3a5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Dataset Analysis: BEFORE vs AFTER cleaning\n",
    "# - Works with df_full (raw) and df_filtered (cleaned)\n",
    "# - Grouping column is `source`\n",
    "# - Prints everything to console instead of saving\n",
    "# ================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ---------- Config ----------\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"              # 0/1\n",
    "SOURCE_COL = \"source\"            # grouping column\n",
    "TOP_K = 25                       # top n-grams to list\n",
    "NGRAM_MAX_FEATURES = 50000       # cap for vectorizer\n",
    "\n",
    "# ---------- Helper: robust emoji detection ----------\n",
    "EMOJI_RE = re.compile(\n",
    "    \"[\\U0001F300-\\U0001F5FF\"   # symbols & pictographs\n",
    "    \"\\U0001F600-\\U0001F64F\"    # emoticons\n",
    "    \"\\U0001F680-\\U0001F6FF\"    # transport & map\n",
    "    \"\\U0001F700-\\U0001F77F\"\n",
    "    \"\\U0001F780-\\U0001F7FF\"\n",
    "    \"\\U0001F800-\\U0001F8FF\"\n",
    "    \"\\U0001F900-\\U0001F9FF\"    # supplemental symbols & pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"\n",
    "    \"\\U0001FA70-\\U0001FAFF\"\n",
    "    \"\\u2600-\\u26FF\"            # misc symbols\n",
    "    \"\\u2700-\\u27BF]\",          # dingbats\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if TEXT_COL not in df.columns:\n",
    "        raise ValueError(f\"Expected a '{TEXT_COL}' column.\")\n",
    "    if LABEL_COL not in df.columns:\n",
    "        df[LABEL_COL] = np.nan\n",
    "    else:\n",
    "        df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors=\"coerce\")\n",
    "    if SOURCE_COL not in df.columns:\n",
    "        df[SOURCE_COL] = \"unknown\"\n",
    "    return df\n",
    "\n",
    "def basic_clean_for_token_len(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    t = re.sub(r\"[^A-Za-z0-9\\s']\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def compute_marker_counts(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        return 0,0,0,0,0,0\n",
    "    emojis = len(EMOJI_RE.findall(text))\n",
    "    hashtags = text.count(\"#\")\n",
    "    qmarks = text.count(\"?\")\n",
    "    excls = text.count(\"!\")\n",
    "    quotes = text.count('\"') + text.count(\"'\")\n",
    "    slash_s = 1 if \"/s\" in text.lower() else 0\n",
    "    return emojis, hashtags, qmarks, excls, quotes, slash_s\n",
    "\n",
    "def class_distribution(df: pd.DataFrame):\n",
    "    counts = df[LABEL_COL].value_counts(dropna=False)\n",
    "    total = len(df)\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"n_label_0\": int(counts.get(0, 0)),\n",
    "        \"n_label_1\": int(counts.get(1, 0)),\n",
    "        \"pct_label_0\": round(100*counts.get(0,0)/total, 2) if total else 0.0,\n",
    "        \"pct_label_1\": round(100*counts.get(1,0)/total, 2) if total else 0.0,\n",
    "        \"has_nan_labels\": int(counts.get(np.nan, 0)),\n",
    "    }\n",
    "\n",
    "def describe_series(x: pd.Series):\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    return {\n",
    "        \"count\": int(x.shape[0]),\n",
    "        \"mean\": round(float(np.nanmean(x)), 2) if x.shape[0] else np.nan,\n",
    "        \"std\": round(float(np.nanstd(x)), 2) if x.shape[0] else np.nan,\n",
    "        \"min\": int(np.nanmin(x)) if x.shape[0] else np.nan,\n",
    "        \"p25\": int(np.nanpercentile(x, 25)) if x.shape[0] else np.nan,\n",
    "        \"median\": int(np.nanmedian(x)) if x.shape[0] else np.nan,\n",
    "        \"p75\": int(np.nanpercentile(x, 75)) if x.shape[0] else np.nan,\n",
    "        \"max\": int(np.nanmax(x)) if x.shape[0] else np.nan,\n",
    "    }\n",
    "\n",
    "def top_ngrams(texts, ngram_range=(1,1), top_k=TOP_K, stop_words=\"english\"):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range,\n",
    "                                 stop_words=stop_words,\n",
    "                                 max_features=NGRAM_MAX_FEATURES)\n",
    "    try:\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "    except ValueError:\n",
    "        return []\n",
    "    sums = np.asarray(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "    order = np.argsort(-sums)[:top_k]\n",
    "    return list(zip(vocab[order], sums[order].tolist()))\n",
    "\n",
    "def run_dataset_analysis(df: pd.DataFrame, name_tag=\"before\"):\n",
    "    print(f\"\\n=== ANALYSIS FOR: {name_tag.upper()} ===\")\n",
    "    df = ensure_columns(df.copy())\n",
    "    df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "    df = df[~df[TEXT_COL].str.strip().eq(\"\")]\n",
    "\n",
    "    # ---- OVERALL size & class distribution\n",
    "    print(\"\\n[Overall class distribution]\")\n",
    "    print(class_distribution(df))\n",
    "\n",
    "    # ---- PER-SOURCE size & class distribution\n",
    "    print(\"\\n[Per-source class distribution]\")\n",
    "    for src, df_s in df.groupby(SOURCE_COL):\n",
    "        print(src, \"->\", class_distribution(df_s))\n",
    "\n",
    "    # ---- LENGTH STATS\n",
    "    lengths_chars = df[TEXT_COL].astype(str).str.len()\n",
    "    lengths_tokens = df[TEXT_COL].astype(str).map(basic_clean_for_token_len).str.split().map(len)\n",
    "    print(\"\\n[Length stats overall]\")\n",
    "    print(\"Chars:\", describe_series(lengths_chars))\n",
    "    print(\"Tokens:\", describe_series(lengths_tokens))\n",
    "\n",
    "    for src, df_s in df.groupby(SOURCE_COL):\n",
    "        c = df_s[TEXT_COL].astype(str).str.len()\n",
    "        t = df_s[TEXT_COL].astype(str).map(basic_clean_for_token_len).str.split().map(len)\n",
    "        print(f\"\\nLength stats for {src}\")\n",
    "        print(\"Chars:\", describe_series(c))\n",
    "        print(\"Tokens:\", describe_series(t))\n",
    "\n",
    "    # ---- MARKERS\n",
    "    print(\"\\n[Markers (emoji, hashtags, punctuation, quotes, /s)]\")\n",
    "    markers = []\n",
    "    for _, row in df.iterrows():\n",
    "        emojis, hashtags, qmarks, excls, quotes, slash_s = compute_marker_counts(row[TEXT_COL])\n",
    "        markers.append({\n",
    "            SOURCE_COL: row[SOURCE_COL],\n",
    "            \"label\": row[LABEL_COL],\n",
    "            \"emojis\": emojis,\n",
    "            \"hashtags\": hashtags,\n",
    "            \"question_marks\": qmarks,\n",
    "            \"exclamation_marks\": excls,\n",
    "            \"quotes\": quotes,\n",
    "            \"slash_s\": slash_s,\n",
    "        })\n",
    "    markers_df = pd.DataFrame(markers)\n",
    "    print(markers_df.groupby([SOURCE_COL, \"label\"]).mean().round(3))\n",
    "\n",
    "    # ---- TOP N-GRAMS\n",
    "    print(\"\\n[Top unigrams overall]\")\n",
    "    print(top_ngrams(df[TEXT_COL].tolist(), (1,1), TOP_K))\n",
    "    print(\"\\n[Top bigrams overall]\")\n",
    "    print(top_ngrams(df[TEXT_COL].tolist(), (2,2), TOP_K))\n",
    "\n",
    "    for target in [0,1]:\n",
    "        subset = df[df[LABEL_COL] == target]\n",
    "        print(f\"\\nTop unigrams label={target}\")\n",
    "        print(top_ngrams(subset[TEXT_COL].tolist(), (1,1), TOP_K))\n",
    "        print(f\"\\nTop bigrams label={target}\")\n",
    "        print(top_ngrams(subset[TEXT_COL].tolist(), (2,2), TOP_K))\n",
    "\n",
    "    for src, df_s in df.groupby(SOURCE_COL):\n",
    "        print(f\"\\nTop unigrams {src}\")\n",
    "        print(top_ngrams(df_s[TEXT_COL].tolist(), (1,1), TOP_K))\n",
    "        print(f\"\\nTop bigrams {src}\")\n",
    "        print(top_ngrams(df_s[TEXT_COL].tolist(), (2,2), TOP_K))\n",
    "\n",
    "def removal_summary(df_before: pd.DataFrame, df_after: pd.DataFrame):\n",
    "    df_before = ensure_columns(df_before.copy())\n",
    "    df_after  = ensure_columns(df_after.copy())\n",
    "    before_n, after_n = len(df_before), len(df_after)\n",
    "    removed = before_n - after_n\n",
    "    pct = round(100 * removed / before_n, 2) if before_n else 0.0\n",
    "\n",
    "    print(\"\\n=== REMOVAL SUMMARY ===\")\n",
    "    print(f\"Before: {before_n}, After: {after_n}, Removed: {removed} ({pct}%)\")\n",
    "\n",
    "    src_before = df_before[SOURCE_COL].value_counts().to_dict()\n",
    "    src_after  = df_after[SOURCE_COL].value_counts().to_dict()\n",
    "    for s in sorted(set(src_before) | set(src_after)):\n",
    "        b = src_before.get(s, 0)\n",
    "        a = src_after.get(s, 0)\n",
    "        r = b - a\n",
    "        p = round(100 * r / b, 2) if b else 0.0\n",
    "        print(f\"{s}: before={b}, after={a}, removed={r}, removed%={p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4c834f45-7417-4ece-a9c9-31e8b16336c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANALYSIS FOR: BEFORE ===\n",
      "\n",
      "[Overall class distribution]\n",
      "{'total': 1047403, 'n_label_0': 526678, 'n_label_1': 520725, 'pct_label_0': 50.28, 'pct_label_1': 49.72, 'has_nan_labels': 0}\n",
      "\n",
      "[Per-source class distribution]\n",
      "Reddit -> {'total': 1010826, 'n_label_0': 505413, 'n_label_1': 505413, 'pct_label_0': 50.0, 'pct_label_1': 50.0, 'has_nan_labels': 0}\n",
      "news_headline -> {'total': 26709, 'n_label_0': 14985, 'n_label_1': 11724, 'pct_label_0': 56.1, 'pct_label_1': 43.9, 'has_nan_labels': 0}\n",
      "twitter -> {'total': 9868, 'n_label_0': 6280, 'n_label_1': 3588, 'pct_label_0': 63.64, 'pct_label_1': 36.36, 'has_nan_labels': 0}\n",
      "\n",
      "[Length stats overall]\n",
      "Chars: {'count': 1047403, 'mean': 56.9, 'std': 61.04, 'min': 1, 'p25': 27, 'median': 47, 'p75': 74, 'max': 10000}\n",
      "Tokens: {'count': 1047403, 'mean': 10.52, 'std': 10.44, 'min': 0, 'p25': 5, 'median': 9, 'p75': 14, 'max': 2222}\n",
      "\n",
      "Length stats for Reddit\n",
      "Chars: {'count': 1010826, 'mean': 56.69, 'std': 61.82, 'min': 1, 'p25': 27, 'median': 46, 'p75': 74, 'max': 10000}\n",
      "Tokens: {'count': 1010826, 'mean': 10.51, 'std': 10.57, 'min': 0, 'p25': 5, 'median': 9, 'p75': 14, 'max': 2222}\n",
      "\n",
      "Length stats for news_headline\n",
      "Chars: {'count': 26709, 'mean': 60.91, 'std': 19.18, 'min': 7, 'p25': 48, 'median': 61, 'p75': 73, 'max': 254}\n",
      "Tokens: {'count': 26709, 'mean': 10.06, 'std': 3.21, 'min': 2, 'p25': 8, 'median': 10, 'p75': 12, 'max': 40}\n",
      "\n",
      "Length stats for twitter\n",
      "Chars: {'count': 9868, 'mean': 67.63, 'std': 53.41, 'min': 2, 'p25': 33, 'median': 45, 'p75': 85, 'max': 613}\n",
      "Tokens: {'count': 9868, 'mean': 12.2, 'std': 9.91, 'min': 0, 'p25': 6, 'median': 8, 'p75': 16, 'max': 128}\n",
      "\n",
      "[Markers (emoji, hashtags, punctuation, quotes, /s)]\n",
      "                     emojis  hashtags  question_marks  exclamation_marks  \\\n",
      "source        label                                                        \n",
      "Reddit        0       0.000     0.007           0.125              0.056   \n",
      "              1       0.000     0.004           0.109              0.138   \n",
      "news_headline 0       0.000     0.005           0.049              0.011   \n",
      "              1       0.000     0.001           0.008              0.003   \n",
      "twitter       0       0.219     0.077           0.083              0.123   \n",
      "              1       0.105     0.035           0.155              0.077   \n",
      "\n",
      "                     quotes  slash_s  \n",
      "source        label                   \n",
      "Reddit        0       0.464    0.000  \n",
      "              1       0.471    0.000  \n",
      "news_headline 0       0.644    0.000  \n",
      "              1       0.392    0.000  \n",
      "twitter       0       0.226    0.003  \n",
      "              1       0.188    0.001  \n",
      "\n",
      "[Top unigrams overall]\n",
      "[('just', 62884), ('like', 56496), ('yeah', 41133), ('don', 40734), ('people', 37464), ('know', 26444), ('think', 25275), ('good', 24816), ('right', 24038), ('really', 20326), ('time', 19867), ('sure', 19597), ('make', 18468), ('yes', 17918), ('game', 16636), ('man', 15827), ('fuck', 15681), ('better', 15413), ('way', 14839), ('need', 14748), ('want', 14476), ('oh', 14389), ('did', 14317), ('didn', 14118), ('ll', 13742)]\n",
      "\n",
      "[Top bigrams overall]\n",
      "[('fuck fuck', 5296), ('don know', 5055), ('looks like', 4101), ('sounds like', 3595), ('don think', 2811), ('just like', 2695), ('comcast comcast', 2675), ('pretty sure', 2300), ('good thing', 2048), ('don want', 1838), ('ve seen', 1751), ('money money', 1732), ('didn know', 1666), ('donald trump', 1662), ('year old', 1583), ('don worry', 1567), ('feel like', 1512), ('look like', 1499), ('white people', 1469), ('don like', 1439), ('jerry jerry', 1341), ('oh yeah', 1316), ('makes sense', 1276), ('make sure', 1275), ('don forget', 1274)]\n",
      "\n",
      "Top unigrams label=0\n",
      "[('like', 30113), ('just', 28971), ('don', 19392), ('people', 16033), ('think', 15612), ('good', 12024), ('know', 11811), ('time', 11052), ('fuck', 10672), ('really', 10649), ('yeah', 8811), ('make', 8658), ('did', 8349), ('right', 8291), ('ve', 8101), ('game', 7940), ('got', 7862), ('want', 7468), ('going', 7121), ('way', 7093), ('sure', 7059), ('ll', 7055), ('say', 6978), ('pretty', 6964), ('actually', 6713)]\n",
      "\n",
      "Top bigrams label=0\n",
      "[('fuck fuck', 5268), ('comcast comcast', 2675), ('looks like', 2644), ('don know', 2226), ('don think', 1992), ('money money', 1708), ('sounds like', 1612), ('donald trump', 1421), ('pretty sure', 1358), ('jerry jerry', 1341), ('feel like', 1173), ('ve seen', 1116), ('iphone iphone', 1111), ('liar liar', 1075), ('raving raving', 1017), ('fake news', 948), ('look like', 941), ('copy pasta', 922), ('don want', 909), ('pasta copy', 908), ('news fake', 899), ('just like', 860), ('trump donald', 784), ('year old', 725), ('years ago', 631)]\n",
      "\n",
      "Top unigrams label=1\n",
      "[('just', 33913), ('yeah', 32322), ('like', 26383), ('people', 21431), ('don', 21342), ('right', 15747), ('know', 14633), ('good', 12792), ('sure', 12538), ('yes', 12081), ('man', 10320), ('oh', 9862), ('make', 9810), ('really', 9677), ('think', 9663), ('obviously', 9220), ('time', 8815), ('better', 8802), ('game', 8696), ('need', 8341), ('totally', 8067), ('forgot', 8008), ('way', 7746), ('didn', 7713), ('want', 7008)]\n",
      "\n",
      "Top bigrams label=1\n",
      "[('don know', 2829), ('sounds like', 1983), ('just like', 1835), ('good thing', 1655), ('looks like', 1457), ('white people', 1243), ('don worry', 1170), ('didn know', 1134), ('oh yeah', 1097), ('black people', 1020), ('pretty sure', 942), ('don want', 929), ('year old', 858), ('don like', 843), ('don think', 819), ('yeah sure', 797), ('yeah totally', 783), ('don understand', 769), ('yeah fuck', 765), ('yeah like', 762), ('don forget', 723), ('make sure', 707), ('yeah just', 684), ('makes sense', 677), ('people don', 652)]\n",
      "\n",
      "Top unigrams Reddit\n",
      "[('just', 61092), ('like', 55746), ('yeah', 41103), ('don', 40317), ('people', 36783), ('know', 26071), ('think', 24984), ('good', 24398), ('right', 23427), ('really', 19559), ('sure', 19452), ('time', 18864), ('yes', 17838), ('make', 17710), ('game', 16118), ('fuck', 15606), ('better', 14582), ('way', 14488), ('need', 14424), ('man', 14352), ('want', 14190), ('did', 14161), ('oh', 14006), ('didn', 13962), ('ll', 13557)]\n",
      "\n",
      "Top bigrams Reddit\n",
      "[('fuck fuck', 5296), ('don know', 5016), ('looks like', 4068), ('sounds like', 3592), ('don think', 2788), ('comcast comcast', 2675), ('just like', 2669), ('pretty sure', 2281), ('good thing', 2043), ('don want', 1813), ('ve seen', 1733), ('money money', 1732), ('didn know', 1645), ('don worry', 1562), ('feel like', 1477), ('look like', 1464), ('white people', 1455), ('don like', 1426), ('jerry jerry', 1341), ('oh yeah', 1315), ('year old', 1306), ('makes sense', 1270), ('don forget', 1268), ('make sure', 1257), ('donald trump', 1200)]\n",
      "\n",
      "Top unigrams news_headline\n",
      "[('trump', 1684), ('new', 1523), ('man', 1395), ('just', 579), ('year', 552), ('report', 515), ('area', 491), ('woman', 481), ('donald', 474), ('day', 471), ('says', 453), ('time', 439), ('obama', 420), ('like', 407), ('women', 402), ('people', 399), ('old', 393), ('world', 383), ('life', 379), ('nation', 362), ('house', 356), ('clinton', 337), ('make', 333), ('white', 329), ('family', 290)]\n",
      "\n",
      "Top bigrams news_headline\n",
      "[('donald trump', 460), ('area man', 257), ('year old', 256), ('white house', 181), ('hillary clinton', 178), ('new york', 117), ('supreme court', 113), ('study finds', 110), ('bernie sanders', 78), ('high school', 78), ('climate change', 73), ('health care', 68), ('introduces new', 63), ('pope francis', 63), ('north korea', 62), ('ted cruz', 61), ('morning email', 56), ('paul ryan', 54), ('need know', 52), ('area woman', 49), ('new study', 44), ('taylor swift', 44), ('mike pence', 42), ('star wars', 41), ('trump administration', 40)]\n",
      "\n",
      "Top unigrams twitter\n",
      "[('just', 1213), ('great', 962), ('love', 937), ('better', 707), ('wait', 697), ('genuinely', 685), ('wonderful', 612), ('time', 564), ('really', 536), ('best', 442), ('new', 440), ('make', 425), ('right', 413), ('amazing', 397), ('excited', 377), ('game', 366), ('oh', 365), ('said', 360), ('concert', 348), ('phone', 347), ('experience', 345), ('artists', 344), ('like', 343), ('meal', 343), ('happens', 341)]\n",
      "\n",
      "Top bigrams twitter\n",
      "[('genuinely amazing', 356), ('just love', 346), ('best experience', 335), ('genuinely love', 319), ('make better', 318), ('really make', 316), ('oh great', 312), ('just needed', 310), ('wonderful said', 307), ('new phone', 304), ('truly inspiring', 302), ('great time', 302), ('love concert', 51), ('wait artists', 49), ('book wonderful', 48), ('wait musicians', 48), ('love artists', 46), ('love new', 45), ('love doctors', 45), ('wait writers', 43), ('love meal', 43), ('wait doctors', 43), ('love athletes', 43), ('love vegetarians', 42), ('conference wonderful', 42)]\n",
      "\n",
      "=== ANALYSIS FOR: AFTER ===\n",
      "\n",
      "[Overall class distribution]\n",
      "{'total': 950694, 'n_label_0': 467758, 'n_label_1': 482936, 'pct_label_0': 49.2, 'pct_label_1': 50.8, 'has_nan_labels': 0}\n",
      "\n",
      "[Per-source class distribution]\n",
      "Reddit -> {'total': 919475, 'n_label_0': 449274, 'n_label_1': 470201, 'pct_label_0': 48.86, 'pct_label_1': 51.14, 'has_nan_labels': 0}\n",
      "news_headline -> {'total': 26597, 'n_label_0': 14950, 'n_label_1': 11647, 'pct_label_0': 56.21, 'pct_label_1': 43.79, 'has_nan_labels': 0}\n",
      "twitter -> {'total': 4622, 'n_label_0': 3534, 'n_label_1': 1088, 'pct_label_0': 76.46, 'pct_label_1': 23.54, 'has_nan_labels': 0}\n",
      "\n",
      "[Length stats overall]\n",
      "Chars: {'count': 950694, 'mean': 56.14, 'std': 34.67, 'min': 4, 'p25': 30, 'median': 49, 'p75': 74, 'max': 472}\n",
      "Tokens: {'count': 950694, 'mean': 10.81, 'std': 6.6, 'min': 1, 'p25': 6, 'median': 9, 'p75': 14, 'max': 38}\n",
      "\n",
      "Length stats for Reddit\n",
      "Chars: {'count': 919475, 'mean': 55.91, 'std': 34.94, 'min': 4, 'p25': 30, 'median': 48, 'p75': 74, 'max': 472}\n",
      "Tokens: {'count': 919475, 'mean': 10.82, 'std': 6.65, 'min': 1, 'p25': 6, 'median': 9, 'p75': 14, 'max': 38}\n",
      "\n",
      "Length stats for news_headline\n",
      "Chars: {'count': 26597, 'mean': 60.0, 'std': 18.78, 'min': 6, 'p25': 47, 'median': 60, 'p75': 72, 'max': 209}\n",
      "Tokens: {'count': 26597, 'mean': 9.91, 'std': 3.19, 'min': 2, 'p25': 8, 'median': 10, 'p75': 12, 'max': 31}\n",
      "\n",
      "Length stats for twitter\n",
      "Chars: {'count': 4622, 'mean': 80.19, 'std': 40.78, 'min': 5, 'p25': 47, 'median': 74, 'p75': 108, 'max': 228}\n",
      "Tokens: {'count': 4622, 'mean': 15.33, 'std': 7.81, 'min': 1, 'p25': 9, 'median': 14, 'p75': 21, 'max': 38}\n",
      "\n",
      "[Markers (emoji, hashtags, punctuation, quotes, /s)]\n",
      "                     emojis  hashtags  question_marks  exclamation_marks  \\\n",
      "source        label                                                        \n",
      "Reddit        0         0.0       0.0           0.131              0.055   \n",
      "              1         0.0       0.0           0.113              0.142   \n",
      "news_headline 0         0.0       0.0           0.049              0.011   \n",
      "              1         0.0       0.0           0.008              0.003   \n",
      "twitter       0         0.0       0.0           0.110              0.158   \n",
      "              1         0.0       0.0           0.177              0.177   \n",
      "\n",
      "                     quotes  slash_s  \n",
      "source        label                   \n",
      "Reddit        0         0.0      0.0  \n",
      "              1         0.0      0.0  \n",
      "news_headline 0         0.0      0.0  \n",
      "              1         0.0      0.0  \n",
      "twitter       0         0.0      0.0  \n",
      "              1         0.0      0.0  \n",
      "\n",
      "[Top unigrams overall]\n",
      "[('just', 58305), ('like', 52686), ('yeah', 39361), ('people', 34370), ('did', 27671), ('know', 26017), ('good', 22806), ('think', 22720), ('does', 22469), ('right', 22148), ('really', 18573), ('sure', 18302), ('time', 17703), ('going', 17572), ('make', 16913), ('yes', 15686), ('got', 15017), ('game', 14777), ('want', 14288), ('man', 14231), ('hes', 14157), ('better', 13908), ('need', 13906), ('way', 13893), ('oh', 12902)]\n",
      "\n",
      "[Top bigrams overall]\n",
      "[('looks like', 3998), ('sounds like', 3466), ('just like', 2599), ('pretty sure', 2184), ('did know', 2055), ('good thing', 2012), ('look like', 1425), ('feel like', 1392), ('white people', 1357), ('oh yeah', 1248), ('does mean', 1204), ('make sure', 1192), ('black people', 1123), ('year old', 1108), ('makes sense', 1042), ('years ago', 984), ('yeah like', 903), ('yeah just', 898), ('sound like', 893), ('yeah sure', 879), ('does matter', 833), ('yeah fuck', 817), ('yeah totally', 800), ('people like', 789), ('ah yes', 770)]\n",
      "\n",
      "Top unigrams label=0\n",
      "[('like', 27458), ('just', 26296), ('think', 14278), ('people', 14179), ('did', 14153), ('does', 11686), ('know', 11306), ('good', 10593), ('really', 9586), ('time', 9483), ('going', 9289), ('yeah', 8020), ('got', 7957), ('make', 7829), ('right', 7453), ('want', 7233), ('game', 6796), ('way', 6482), ('pretty', 6441), ('sure', 6401), ('say', 6252), ('actually', 6170), ('better', 6012), ('need', 5848), ('probably', 5758)]\n",
      "\n",
      "Top bigrams label=0\n",
      "[('looks like', 2573), ('sounds like', 1565), ('pretty sure', 1285), ('feel like', 1068), ('look like', 884), ('just like', 803), ('did know', 682), ('does mean', 679), ('years ago', 560), ('year old', 525), ('make sure', 520), ('id say', 512), ('donald trump', 506), ('makes sense', 489), ('holy shit', 481), ('pretty good', 478), ('does make', 433), ('just got', 425), ('just want', 405), ('oh god', 391), ('people like', 383), ('really good', 380), ('good thing', 380), ('high school', 357), ('did say', 356)]\n",
      "\n",
      "Top unigrams label=1\n",
      "[('just', 32009), ('yeah', 31341), ('like', 25228), ('people', 20191), ('know', 14711), ('right', 14695), ('did', 13518), ('good', 12213), ('sure', 11901), ('yes', 11512), ('does', 10783), ('man', 9476), ('make', 9084), ('oh', 9058), ('really', 8987), ('obviously', 8950), ('hes', 8512), ('think', 8442), ('going', 8283), ('time', 8220), ('need', 8058), ('game', 7981), ('better', 7896), ('totally', 7704), ('way', 7411)]\n",
      "\n",
      "Top bigrams label=1\n",
      "[('sounds like', 1901), ('just like', 1796), ('good thing', 1632), ('looks like', 1425), ('did know', 1373), ('white people', 1158), ('oh yeah', 1056), ('black people', 975), ('pretty sure', 899), ('yeah sure', 802), ('yeah totally', 769), ('yeah like', 763), ('yeah fuck', 753), ('yeah just', 704), ('make sure', 672), ('nah man', 631), ('ah yes', 627), ('year old', 583), ('thank god', 581), ('sound like', 561), ('let just', 557), ('makes sense', 553), ('yeah man', 552), ('does matter', 547), ('look like', 541)]\n",
      "\n",
      "Top unigrams Reddit\n",
      "[('just', 57222), ('like', 52027), ('yeah', 39336), ('people', 33777), ('did', 27384), ('know', 25659), ('think', 22473), ('good', 22417), ('does', 22081), ('right', 21855), ('sure', 18172), ('really', 18138), ('going', 17187), ('time', 17064), ('make', 16483), ('yes', 15615), ('got', 14743), ('game', 14564), ('want', 13998), ('hes', 13976), ('better', 13690), ('need', 13609), ('way', 13580), ('man', 12930), ('oh', 12824)]\n",
      "\n",
      "Top bigrams Reddit\n",
      "[('looks like', 3967), ('sounds like', 3463), ('just like', 2575), ('pretty sure', 2166), ('did know', 2032), ('good thing', 2007), ('look like', 1391), ('feel like', 1364), ('white people', 1342), ('oh yeah', 1247), ('does mean', 1191), ('make sure', 1177), ('black people', 1115), ('year old', 1093), ('makes sense', 1037), ('years ago', 965), ('yeah like', 903), ('yeah just', 898), ('sound like', 890), ('yeah sure', 879), ('does matter', 829), ('yeah fuck', 816), ('yeah totally', 799), ('people like', 783), ('ah yes', 764)]\n",
      "\n",
      "Top unigrams news_headline\n",
      "[('new', 1513), ('trump', 1257), ('man', 1240), ('just', 576), ('report', 515), ('area', 478), ('donald', 471), ('says', 446), ('woman', 440), ('day', 428), ('time', 402), ('like', 398), ('trumps', 391), ('people', 374), ('obama', 358), ('house', 347), ('life', 342), ('make', 330), ('white', 317), ('women', 312), ('clinton', 306), ('world', 282), ('years', 281), ('does', 278), ('family', 271)]\n",
      "\n",
      "Top bigrams news_headline\n",
      "[('donald trump', 362), ('area man', 222), ('white house', 175), ('hillary clinton', 162), ('supreme court', 111), ('new york', 111), ('study finds', 110), ('donald trumps', 97), ('bernie sanders', 78), ('climate change', 72), ('high school', 68), ('health care', 67), ('introduces new', 63), ('pope francis', 62), ('north korea', 56), ('morning email', 55), ('ted cruz', 55), ('need know', 52), ('paul ryan', 47), ('new study', 44), ('area woman', 41), ('star wars', 41), ('taylor swift', 37), ('trump administration', 37), ('unveils new', 37)]\n",
      "\n",
      "Top unigrams twitter\n",
      "[('just', 507), ('love', 276), ('like', 261), ('time', 237), ('people', 219), ('really', 207), ('day', 198), ('today', 182), ('good', 173), ('going', 173), ('think', 143), ('know', 135), ('got', 128), ('new', 126), ('great', 125), ('want', 119), ('did', 115), ('right', 114), ('life', 113), ('does', 110), ('best', 109), ('need', 102), ('year', 102), ('make', 100), ('work', 100)]\n",
      "\n",
      "Top bigrams twitter\n",
      "[('just love', 30), ('just want', 22), ('great time', 21), ('feel like', 20), ('make better', 18), ('oh great', 18), ('rain today', 17), ('wonderful said', 16), ('truly inspiring', 16), ('genuinely amazing', 16), ('genuinely love', 16), ('best experience', 16), ('new phone', 16), ('boris johnson', 16), ('just needed', 16), ('looking forward', 16), ('really make', 16), ('year old', 14), ('happy birthday', 13), ('just got', 13), ('high school', 11), ('mental health', 11), ('looks like', 10), ('christmas party', 10), ('prime minister', 9)]\n",
      "\n",
      "=== REMOVAL SUMMARY ===\n",
      "Before: 1047403, After: 950694, Removed: 96709 (9.23%)\n",
      "Reddit: before=1010826, after=919475, removed=91351, removed%=9.04\n",
      "news_headline: before=26709, after=26597, removed=112, removed%=0.42\n",
      "twitter: before=9868, after=4622, removed=5246, removed%=53.16\n"
     ]
    }
   ],
   "source": [
    "# Run for raw (before cleaning)\n",
    "run_dataset_analysis(df_full, name_tag=\"before\")\n",
    "\n",
    "# Run for cleaned\n",
    "run_dataset_analysis(df_filtered, name_tag=\"after\")\n",
    "\n",
    "# Compare removal\n",
    "removal_summary(df_full, df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8109ff1e-788f-41c0-b396-effe3db48095",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = df_filtered[df_filtered['source'] == 'news_headline']\n",
    "twitter_data = df_filtered[df_filtered['source'] == 'twitter']\n",
    "reddit_data = df_filtered[df_filtered['source'] == 'reddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896a3f9-e705-4c1e-a2f7-7bf82d4b9b36",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Linear Baseline: Logistic Regression with TF-IDF\n",
    "\n",
    "To establish a strong baseline, I first tested a **Logistic Regression** model using TF-IDF text features combined with a small set of engineered features. This provided a fast, interpretable benchmark for sarcasm detection, and served as a comparison point for later deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Feature Representation\n",
    "\n",
    "Two types of features were used:\n",
    "\n",
    "* **TF-IDF vectors**:\n",
    "\n",
    "  * 1–2 grams\n",
    "  * Vocabulary capped at 10,000 features\n",
    "  * Sparse matrix representation\n",
    "\n",
    "* **Engineered features**:\n",
    "\n",
    "  * `negation_count` (number of negation cues)\n",
    "  * `exclamations` (number of `!`)\n",
    "  * `question_marks` (number of `?`)\n",
    "\n",
    "These features were concatenated with the TF-IDF vectors to give the model both surface lexical patterns and simple punctuation/negation cues often associated with sarcasm.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Data Splits\n",
    "\n",
    "The cleaned dataset (`df_filtered`) was split into three sets:\n",
    "\n",
    "* **Training set**: 72% of the data\n",
    "* **Validation set**: 8% of the data\n",
    "* **Test set**: 20% of the data\n",
    "\n",
    "Stratified sampling was applied to preserve the balanced distribution between sarcastic and non-sarcastic examples across all splits.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Training the Model\n",
    "\n",
    "* Logistic Regression was trained with `max_iter=1000` to ensure convergence.\n",
    "* Training time was **\\~6.5 seconds** on the full training set.\n",
    "* Inference was highly efficient: less than **0.05s for 190k test samples**.\n",
    "\n",
    "This confirmed that linear models can be deployed at scale with negligible latency.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Results\n",
    "\n",
    "**Performance across splits** showed strong consistency, with no evidence of overfitting:\n",
    "\n",
    "| Split      |  Accuracy | Precision |    Recall |        F1 |\n",
    "| ---------- | --------: | --------: | --------: | --------: |\n",
    "| Train      |     0.713 |     0.734 |     0.684 |     0.708 |\n",
    "| Validation |     0.702 |     0.722 |     0.673 |     0.696 |\n",
    "| Test       | **0.703** | **0.722** | **0.674** | **0.697** |\n",
    "\n",
    "* **Precision > Recall**: the model is slightly conservative, correctly flagging sarcasm when predicted but missing some subtle cases.\n",
    "* The balanced F1 score (\\~0.70) demonstrates that TF-IDF + simple features capture much of the signal despite being lightweight.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Error Analysis\n",
    "\n",
    "* **False negatives**: often subtle sarcasm requiring context or world knowledge (e.g., ironic praise).\n",
    "* **False positives**: enthusiastic or exaggerated statements sometimes misread as sarcasm.\n",
    "\n",
    "Example misclassifications:\n",
    "\n",
    "* *True: sarcastic (1), Pred: not sarcastic (0)* → “Denial is typically a coping device”\n",
    "* *True: non-sarcastic (0), Pred: sarcastic (1)* → “Yeah I cannot use shadow play on a gtx 260”\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Summary\n",
    "\n",
    "Logistic Regression with TF-IDF provides:\n",
    "\n",
    "* **Fast training and inference** (sub-second for hundreds of thousands of samples).\n",
    "* **Stable performance** across splits, with Test F1 ≈ 0.70.\n",
    "* A strong, interpretable **baseline model** to benchmark against transformer and deep learning architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dec44b3f-f3fa-463e-8a92-0d50edec8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# =========================\n",
    "# 1. Prepare data\n",
    "# =========================\n",
    "X_text_all = df_filtered['text'].astype(str)\n",
    "y_all = df_filtered['label']\n",
    "extra_features = df_filtered[['negation_count', 'exclamations', 'question_marks']]\n",
    "\n",
    "# Split raw text and labels first\n",
    "X_text_trainval, X_text_test, y_trainval, y_test = train_test_split(\n",
    "    X_text_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "X_text_train, X_text_val, y_train, y_val = train_test_split(\n",
    "    X_text_trainval, y_trainval, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Split extra features accordingly\n",
    "features_trainval = extra_features.loc[X_text_trainval.index]\n",
    "features_test = extra_features.loc[X_text_test.index]\n",
    "\n",
    "features_train = features_trainval.loc[X_text_train.index]\n",
    "features_val = features_trainval.loc[X_text_val.index]\n",
    "\n",
    "# =========================\n",
    "# 2. Vectorize TF-IDF\n",
    "# =========================\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_tfidf_train = vectorizer.fit_transform(X_text_train)\n",
    "X_tfidf_val = vectorizer.transform(X_text_val)\n",
    "X_tfidf_test = vectorizer.transform(X_text_test)\n",
    "\n",
    "# =========================\n",
    "# 3. Combine TF-IDF + extra features\n",
    "# =========================\n",
    "X_combined_train = hstack([X_tfidf_train, features_train])\n",
    "X_combined_val = hstack([X_tfidf_val, features_val])\n",
    "X_combined_test = hstack([X_tfidf_test, features_test])\n",
    "\n",
    "# =========================\n",
    "# 4. Evaluation Function\n",
    "# =========================\n",
    "def evaluate_model(model, X, y_true, X_text, split_name=\"Validation\", model_name=\"Model\"):\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X)\n",
    "    infer_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {split_name} Set - {model_name} ===\")\n",
    "    print(f\"Inference Time: {infer_time:.4f}s for {len(y_true)} samples\")\n",
    "    print(f\"  Accuracy : {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall   : {rec:.3f}\")\n",
    "    print(f\"  F1 Score : {f1:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "    misclassified_idx = (y_true != y_pred)\n",
    "    print(f\"Misclassified: {np.sum(misclassified_idx)} / {len(y_true)}\")\n",
    "\n",
    "    misclassified_df = pd.DataFrame({\n",
    "        \"text\": np.array(X_text)[misclassified_idx],\n",
    "        \"true_label\": np.array(y_true)[misclassified_idx],\n",
    "        \"pred_label\": y_pred[misclassified_idx]\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nFirst 5 Misclassified Examples ({split_name} - {model_name}):\")\n",
    "    for i in range(min(5, len(misclassified_df))):\n",
    "        row = misclassified_df.iloc[i]\n",
    "        print(f\"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}\")\n",
    "\n",
    "    return y_pred, misclassified_df, acc, prec, rec, f1, infer_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5b3a3f16-69b3-42e5-bce0-0e6e32c2b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Logistic Regression trained in 6.47s\n",
      "\n",
      "=== Train Set - Logistic Regression ===\n",
      "Inference Time: 0.2551s for 684499 samples\n",
      "  Accuracy : 0.713\n",
      "  Precision: 0.734\n",
      "  Recall   : 0.684\n",
      "  F1 Score : 0.708\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.695     0.743     0.718    336758\n",
      "           1      0.734     0.684     0.708    347741\n",
      "\n",
      "    accuracy                          0.713    684499\n",
      "   macro avg      0.714     0.714     0.713    684499\n",
      "weighted avg      0.715     0.713     0.713    684499\n",
      "\n",
      "Misclassified: 196243 / 684499\n",
      "\n",
      "First 5 Misclassified Examples (Train - Logistic Regression):\n",
      "[1] True: 0 | Pred: 1 | Text: Its great i get to play casual edh between mordern matches\n",
      "[2] True: 1 | Pred: 0 | Text: What a hard struggle being able to make so many friends easily\n",
      "[3] True: 1 | Pred: 0 | Text: Not wanting a threesome\n",
      "[4] True: 1 | Pred: 0 | Text: ohh now I get it\n",
      "[5] True: 1 | Pred: 0 | Text: Its fairness when they agree with me not the other way around\n",
      "\n",
      "=== Validation Set - Logistic Regression ===\n",
      "Inference Time: 0.0140s for 76056 samples\n",
      "  Accuracy : 0.702\n",
      "  Precision: 0.722\n",
      "  Recall   : 0.673\n",
      "  F1 Score : 0.696\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.685     0.732     0.708     37448\n",
      "           1      0.722     0.673     0.696     38608\n",
      "\n",
      "    accuracy                          0.702     76056\n",
      "   macro avg      0.703     0.703     0.702     76056\n",
      "weighted avg      0.703     0.702     0.702     76056\n",
      "\n",
      "Misclassified: 22654 / 76056\n",
      "\n",
      "First 5 Misclassified Examples (Validation - Logistic Regression):\n",
      "[1] True: 0 | Pred: 1 | Text: There are a bunch of pictures of scions that people got badges for so yes\n",
      "[2] True: 1 | Pred: 0 | Text: Well at least I can work out without distractions and with avaliable machines everywhere this month\n",
      "[3] True: 1 | Pred: 0 | Text: I remember using those in 3rd grade and that was only a few weeks ago\n",
      "[4] True: 0 | Pred: 1 | Text: More critique work on your fake names\n",
      "[5] True: 0 | Pred: 1 | Text: Yeah I cannot use shadow play on a gtx 260\n",
      "\n",
      "=== Test Set - Logistic Regression ===\n",
      "Inference Time: 0.0431s for 190139 samples\n",
      "  Accuracy : 0.703\n",
      "  Precision: 0.722\n",
      "  Recall   : 0.674\n",
      "  F1 Score : 0.697\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.685     0.732     0.708     93552\n",
      "           1      0.722     0.674     0.697     96587\n",
      "\n",
      "    accuracy                          0.703    190139\n",
      "   macro avg      0.704     0.703     0.703    190139\n",
      "weighted avg      0.704     0.703     0.703    190139\n",
      "\n",
      "Misclassified: 56526 / 190139\n",
      "\n",
      "First 5 Misclassified Examples (Test - Logistic Regression):\n",
      "[1] True: 1 | Pred: 0 | Text: But Twitch chat killed the pro streams\n",
      "[2] True: 1 | Pred: 0 | Text: Denial is typically a coping device\n",
      "[3] True: 1 | Pred: 0 | Text: I disagree with the results of this game and will now burn shit and demand the NCAA officials change the outcome\n",
      "[4] True: 1 | Pred: 0 | Text: Its a turbo storch if only Hitler had one of these he might have made it out of berlin\n",
      "[5] True: 1 | Pred: 0 | Text: Is Not Ansem the Wise the normal counterpart to iAnsem?\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5. Logistic Regression\n",
    "# =========================\n",
    "start_lr = time.time()\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_combined_train, y_train)\n",
    "train_time_lr = time.time() - start_lr\n",
    "print(f\"\\n🧠 Logistic Regression trained in {train_time_lr:.2f}s\")\n",
    "\n",
    "y_train_pred_lr, misclassified_train_df_lr, acc_train_lr, prec_train_lr, rec_train_lr, f1_train_lr, infer_train_lr = evaluate_model(\n",
    "    lr_model, X_combined_train, y_train, X_text_train, \"Train\", \"Logistic Regression\")\n",
    "\n",
    "y_val_pred_lr, misclassified_val_df_lr, acc_val_lr, prec_val_lr, rec_val_lr, f1_val_lr, infer_val_lr = evaluate_model(\n",
    "    lr_model, X_combined_val, y_val, X_text_val, \"Validation\", \"Logistic Regression\")\n",
    "\n",
    "y_test_pred_lr, misclassified_test_df_lr, acc_test_lr, prec_test_lr, rec_test_lr, f1_test_lr, infer_test_lr = evaluate_model(\n",
    "    lr_model, X_combined_test, y_test, X_text_test, \"Test\", \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d2209-2a3e-4038-91f0-1342da2e8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. SVC\n",
    "# =========================\n",
    "start_svc = time.time()\n",
    "svc_model = SVC(kernel=\"linear\", C=1.0)\n",
    "svc_model.fit(X_combined_train, y_train)\n",
    "train_time_svc = time.time() - start_svc\n",
    "print(f\"\\n🧠 SVC trained in {train_time_svc:.2f}s\")\n",
    "\n",
    "y_train_pred_svc, misclassified_train_df_svc, acc_train_svc, prec_train_svc, rec_train_svc, f1_train_svc, infer_train_svc = evaluate_model(\n",
    "    svc_model, X_combined_train, y_train, X_text_train, \"Train\", \"SVC\")\n",
    "\n",
    "y_val_pred_svc, misclassified_val_df_svc, acc_val_svc, prec_val_svc, rec_val_svc, f1_val_svc, infer_val_svc = evaluate_model(\n",
    "    svc_model, X_combined_val, y_val, X_text_val, \"Validation\", \"SVC\")\n",
    "\n",
    "y_test_pred_svc, misclassified_test_df_svc, acc_test_svc, prec_test_svc, rec_test_svc, f1_test_svc, infer_test_svc = evaluate_model(\n",
    "    svc_model, X_combined_test, y_test, X_text_test, \"Test\", \"SVC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed944ae-001a-4e75-9cea-012b38309d5b",
   "metadata": {},
   "source": [
    "As part of the baseline experimentation, several traditional machine learning models were evaluated using TF-IDF vectorized features extracted from the cleaned sarcasm dataset. Both Logistic Regression and Linear Support Vector Machine (SVM) demonstrated strong and consistent performance, achieving an F1-score of 0.70 and maintaining balanced precision and recall across both sarcastic and non-sarcastic classes. These results established reliable baselines for comparison with deep learning models.\n",
    "\n",
    "In contrast, Random Forest was found to be computationally impractical for this task. Due to the combination of a very large dataset (approximately 950,000 samples) and a high-dimensional sparse feature space (5,000+ TF-IDF features), the Random Forest classifier required excessive processing time and memory. Each decision tree in the ensemble had to evaluate numerous splits over thousands of features, making the model inefficient to train at this scale. Although Random Forest has shown success on smaller or lower-dimensional text datasets, it was not a suitable option for this high-volume, high-dimensional sarcasm detection task. Consequently, the model was excluded from final evaluation in favor of more scalable and context-aware deep learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af88907-da95-4745-a863-d959ff8bdaf8",
   "metadata": {},
   "source": [
    "# 4. Deep Learning Models — Initial Evaluation\n",
    "\n",
    "To go beyond linear baselines, I trained several **RNN/CNN architectures with pretrained GloVe embeddings**. The goal here was to see how much lift I can get over TF-IDF + Logistic Regression (Test F1 ≈ **0.697**) while keeping models reasonably efficient for downstream deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1 Architectures Evaluated\n",
    "\n",
    "* **BiLSTM + GloVe (100d)**\n",
    "* **BiLSTM + GloVe (300d)**\n",
    "* **BiLSTM + GloVe (100d) + Attention**\n",
    "* **BiLSTM + GloVe (300d) + Attention**\n",
    "* **CNN + GloVe (100d)** (Conv1D + GlobalMaxPool)\n",
    "\n",
    "**Input setup:**\n",
    "Texts tokenized with Keras, padded to **max\\_len = 40** tokens.\n",
    "**Embeddings:** GloVe 6B (100d or 300d), loaded into a **frozen** embedding layer.\n",
    "**Heads:** Dense(32, ReLU) → Dropout → Sigmoid.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Training & Evaluation Protocol\n",
    "\n",
    "* **Splits:** 81% train, 9% validation, 10% test (random\\_state=42).\n",
    "* **Loss/Opt:** Binary cross-entropy, Adam.\n",
    "* **Epochs/Batch:** 5 epochs, batch size 128.\n",
    "* **Threshold:** Fixed at 0.5 for all models.\n",
    "* **Metrics:** Accuracy, Precision, Recall, **F1** on Train/Val/Test.\n",
    "* **Latency:** End-to-end inference time measured for the full split.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Results (Test Set)\n",
    "\n",
    "| Model                            |   Acc |  Prec |   Rec |    **F1** | Inference (≈95k samples) |\n",
    "| -------------------------------- | ----: | ----: | ----: | --------: | -----------------------: |\n",
    "| **BiLSTM + GloVe (100d)**        | 0.709 | 0.763 | 0.624 | **0.686** |                 \\~15.6 s |\n",
    "| **BiLSTM + GloVe (300d)**        | 0.717 | 0.723 | 0.719 | **0.721** |                 \\~31.6 s |\n",
    "| **BiLSTM + GloVe (100d) + Attn** | 0.713 | 0.718 | 0.717 | **0.718** |                 \\~51.2 s |\n",
    "| **BiLSTM + GloVe (300d) + Attn** | 0.721 | 0.737 | 0.705 | **0.721** |                 \\~67.3 s |\n",
    "| **CNN + GloVe (100d)**           | 0.678 | 0.710 | 0.621 | **0.663** |                  \\~9.4 s |\n",
    "\n",
    "**Headline:** The best-performing classical DL variants are **BiLSTM + GloVe (300d)** and **BiLSTM + GloVe (300d + Attention)**, both at **F1 ≈ 0.721**, giving a **+0.02–0.03 F1** lift over Logistic Regression.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 What the numbers say\n",
    "\n",
    "* **Embedding size matters:** Moving from **100d → 300d** consistently improves F1 (e.g., BiLSTM 0.686 → **0.721**).\n",
    "* **Attention helps, but not always:** Attention with 100d improves over vanilla 100d (0.686 → **0.718**). With 300d, attention roughly **ties** the non-attention model but shifts the **precision–recall trade-off** (300d: Prec 0.723 / Rec 0.719 vs 300d+Attn: **Prec 0.737** / Rec 0.705).\n",
    "* **CNN underperforms here:** The simple Conv1D + max-pool is **fastest** among DL models but trails in F1 (**0.663**), likely due to limited context vs. BiLSTM sequence modeling.\n",
    "* **Latency considerations:** BiLSTMs are **much slower** than linear baselines (tens of seconds for \\~95k samples). Among DL, **100d models are \\~2× faster** than 300d; attention adds extra overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.5 Error patterns (qualitative)\n",
    "\n",
    "* **False negatives (missed sarcasm):** short, deadpan statements or those requiring **world knowledge/context** (e.g., “you are right obama totally is big brother”).\n",
    "* **False positives:** emphatic or exaggerated **non-sarcastic** statements (e.g., “OMG what a surprise!”) flagged due to irony-like surface cues.\n",
    "\n",
    "This mirrors the LR baseline but with fewer misses on subtle phrasing—particularly for the **300d** BiLSTM variants.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.6 Takeaways\n",
    "\n",
    "* **Best classical DL choice:** **BiLSTM + GloVe (300d)** for **balanced** precision/recall and top F1 (**0.721**).\n",
    "* **If you prefer higher precision (fewer false alarms):** **BiLSTM + GloVe (300d + Attention)** (Prec **0.737**) at a small recall cost.\n",
    "* **Speed–quality trade-off:**\n",
    "\n",
    "  * Need speed? Stick with **LogReg** or **CNN 100d** (but accept lower F1).\n",
    "  * Need quality? **BiLSTM 300d** (with/without attention) is the sweet spot among classical DL.\n",
    "* These results set a realistic ceiling for **non-transformer** models; transformer encoders (e.g., DistilRoBERTa) typically exceed this, especially on **recall** for subtle sarcasm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ee4fe-7fd1-4b2d-a88d-56209b7448df",
   "metadata": {},
   "source": [
    "BiLSTM + GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1b81911e-3838-49e1-adfe-f215200d7f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │    \u001b[38;5;34m19,357,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> (73.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,357,500\u001b[0m (73.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> (73.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m19,357,500\u001b[0m (73.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 41ms/step - accuracy: 0.6295 - loss: 0.6353 - val_accuracy: 0.6847 - val_loss: 0.5865\n",
      "Epoch 2/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 39ms/step - accuracy: 0.6843 - loss: 0.5888 - val_accuracy: 0.6996 - val_loss: 0.5687\n",
      "Epoch 3/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 38ms/step - accuracy: 0.6987 - loss: 0.5733 - val_accuracy: 0.7049 - val_loss: 0.5636\n",
      "Epoch 4/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 38ms/step - accuracy: 0.7078 - loss: 0.5638 - val_accuracy: 0.7050 - val_loss: 0.5628\n",
      "Epoch 5/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 38ms/step - accuracy: 0.7146 - loss: 0.5556 - val_accuracy: 0.7092 - val_loss: 0.5565\n",
      "\n",
      "Training Time (BiLSTM + GloVe): 1161.34s\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 950694 but corresponding boolean dimension is 770061",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred, misclassified_df\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Raw text alignment\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m texts_train_bilstm_glove \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(texts_bilstm_glove)[X_train_bilstm_glove[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m][:\u001b[38;5;28mlen\u001b[39m(y_train_bilstm_glove)]\n\u001b[1;32m    138\u001b[0m texts_val_bilstm_glove \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(texts_bilstm_glove)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_val_bilstm_glove):]\n\u001b[1;32m    139\u001b[0m texts_test_bilstm_glove \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(texts_bilstm_glove)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_test_bilstm_glove):]\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 950694 but corresponding boolean dimension is 770061"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# ===============================\n",
    "# 1. Preprocessing & Tokenization\n",
    "# ===============================\n",
    "texts_bilstm_glove = df_filtered['text'].astype(str).tolist()\n",
    "labels_bilstm_glove = df_filtered['label'].tolist()\n",
    "\n",
    "tokenizer_bilstm_glove = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer_bilstm_glove.fit_on_texts(texts_bilstm_glove)\n",
    "sequences_bilstm_glove = tokenizer_bilstm_glove.texts_to_sequences(texts_bilstm_glove)\n",
    "\n",
    "max_len_bilstm_glove = 40\n",
    "X_bilstm_glove = pad_sequences(sequences_bilstm_glove, maxlen=max_len_bilstm_glove, padding='post', truncating='post')\n",
    "y_bilstm_glove = np.array(labels_bilstm_glove)\n",
    "\n",
    "# ✅ Train / Val / Test split\n",
    "X_temp_bilstm_glove, X_test_bilstm_glove, y_temp_bilstm_glove, y_test_bilstm_glove = train_test_split(\n",
    "    X_bilstm_glove, y_bilstm_glove, test_size=0.1, random_state=42)\n",
    "X_train_bilstm_glove, X_val_bilstm_glove, y_train_bilstm_glove, y_val_bilstm_glove = train_test_split(\n",
    "    X_temp_bilstm_glove, y_temp_bilstm_glove, test_size=0.1, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 2. Load GloVe Embeddings\n",
    "# ===============================\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "embedding_index_bilstm_glove = {}\n",
    "\n",
    "with open(glove_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index_bilstm_glove[word] = vector\n",
    "\n",
    "embedding_dim = 100\n",
    "word_index_bilstm_glove = tokenizer_bilstm_glove.word_index\n",
    "vocab_size_bilstm_glove = len(word_index_bilstm_glove) + 1\n",
    "\n",
    "embedding_matrix_bilstm_glove = np.zeros((vocab_size_bilstm_glove, embedding_dim))\n",
    "for word, i in word_index_bilstm_glove.items():\n",
    "    vector = embedding_index_bilstm_glove.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix_bilstm_glove[i] = vector\n",
    "\n",
    "# ===============================\n",
    "# 3. Define BiLSTM Model\n",
    "# ===============================\n",
    "model_bilstm_glove = Sequential([\n",
    "    Embedding(input_dim=vocab_size_bilstm_glove,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix_bilstm_glove],\n",
    "              input_length=max_len_bilstm_glove,\n",
    "              trainable=False),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_bilstm_glove.compile(loss='binary_crossentropy',\n",
    "                           optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "model_bilstm_glove.summary()\n",
    "\n",
    "# ===============================\n",
    "# 4. Train the Model\n",
    "# ===============================\n",
    "start_train_bilstm_glove = time.time()\n",
    "history_bilstm_glove = model_bilstm_glove.fit(\n",
    "    X_train_bilstm_glove, y_train_bilstm_glove,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_bilstm_glove, y_val_bilstm_glove),\n",
    "    verbose=1\n",
    ")\n",
    "train_time_bilstm_glove = time.time() - start_train_bilstm_glove\n",
    "print(f\"\\nTraining Time (BiLSTM + GloVe): {train_time_bilstm_glove:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b9dfa4da-fb69-451c-8eb9-5f9ac99d75a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Set (BiLSTM + GloVe) ===\n",
      "Inference Time: 130.8256s for 770061 samples\n",
      "  Accuracy : 0.726\n",
      "  Precision: 0.780\n",
      "  Recall   : 0.641\n",
      "  F1 Score : 0.704\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.687     0.814     0.745    378870\n",
      "           1      0.780     0.641     0.704    391191\n",
      "\n",
      "    accuracy                          0.726    770061\n",
      "   macro avg      0.734     0.727     0.725    770061\n",
      "weighted avg      0.735     0.726     0.724    770061\n",
      "\n",
      "\n",
      "Precision per class: [0.687131   0.78042679]\n",
      "Recall per class: [0.81374614 0.64114972]\n",
      "F1 per class: [0.7450979  0.70396551]\n",
      "\n",
      "Misclassified: 210945 / 770061\n",
      "\n",
      "First 5 Misclassified Examples (Train):\n",
      "[1] True: 0 | Pred: 1 | Text: The posters promised they were the toughest and the uniforms were the neatest was not much of a choice really\n",
      "[2] True: 0 | Pred: 1 | Text: My dad once told me Doc\n",
      "[3] True: 1 | Pred: 0 | Text: I mean this is best gunnit\n",
      "[4] True: 1 | Pred: 0 | Text: Why would they lie if they were not paid\n",
      "[5] True: 0 | Pred: 1 | Text: Id circle the globe powered by my explosive diarrhea\n",
      "\n",
      "=== Validation Set (BiLSTM + GloVe) ===\n",
      "Inference Time: 14.0529s for 85563 samples\n",
      "  Accuracy : 0.709\n",
      "  Precision: 0.759\n",
      "  Recall   : 0.623\n",
      "  F1 Score : 0.684\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.674     0.797     0.730     42270\n",
      "           1      0.759     0.623     0.684     43293\n",
      "\n",
      "    accuracy                          0.709     85563\n",
      "   macro avg      0.716     0.710     0.707     85563\n",
      "weighted avg      0.717     0.709     0.707     85563\n",
      "\n",
      "\n",
      "Precision per class: [0.67386436 0.75902326]\n",
      "Recall per class: [0.79735037 0.62321853]\n",
      "F1 per class: [0.73042498 0.68444952]\n",
      "\n",
      "Misclassified: 24878 / 85563\n",
      "\n",
      "First 5 Misclassified Examples (Validation):\n",
      "[1] True: 0 | Pred: 1 | Text: No second amendment right to protect them damn crazies Fondling my gun at night and dry firing it at passerbys is perfectly and within my rights\n",
      "[2] True: 1 | Pred: 0 | Text: In my neighborhood its the kids themselves that keep getting bigger\n",
      "[3] True: 1 | Pred: 0 | Text: Good god you need a healthy dose of Occams razor in your life\n",
      "[4] True: 0 | Pred: 1 | Text: Is there any sort of draft assistant?\n",
      "[5] True: 1 | Pred: 0 | Text: Shaving Edit\n",
      "\n",
      "=== Test Set (BiLSTM + GloVe) ===\n",
      "Inference Time: 15.5503s for 95070 samples\n",
      "  Accuracy : 0.709\n",
      "  Precision: 0.763\n",
      "  Recall   : 0.624\n",
      "  F1 Score : 0.686\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.671     0.798     0.729     46618\n",
      "           1      0.763     0.624     0.686     48452\n",
      "\n",
      "    accuracy                          0.709     95070\n",
      "   macro avg      0.717     0.711     0.708     95070\n",
      "weighted avg      0.718     0.709     0.707     95070\n",
      "\n",
      "\n",
      "Precision per class: [0.67125338 0.76282181]\n",
      "Recall per class: [0.7984255  0.62377198]\n",
      "F1 per class: [0.7293373  0.68632483]\n",
      "\n",
      "Misclassified: 27626 / 95070\n",
      "\n",
      "First 5 Misclassified Examples (Test):\n",
      "[1] True: 1 | Pred: 0 | Text: Yes because there are no voices within any of those fields that would argue that patriarchy is a myth\n",
      "[2] True: 1 | Pred: 0 | Text: you are right obama totally is big brother\n",
      "[3] True: 1 | Pred: 0 | Text: Also because he is the remover of obstacles !\n",
      "[4] True: 1 | Pred: 0 | Text: Beatles move over\n",
      "[5] True: 0 | Pred: 1 | Text: I cannot believe you faked a wedding and pretended to dancr just to get all this karana\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_bilstm_glove(X, y_true, split_name=\"Validation\", raw_texts=None):\n",
    "    import time\n",
    "    start_infer = time.time()\n",
    "    y_pred_probs = model_bilstm_glove.predict(X, verbose=0)\n",
    "    infer_time = time.time() - start_infer\n",
    "\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {split_name} Set (BiLSTM + GloVe) ===\")\n",
    "    print(f\"Inference Time: {infer_time:.4f}s for {len(y_true)} samples\")\n",
    "    print(f\"  Accuracy : {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall   : {rec:.3f}\")\n",
    "    print(f\"  F1 Score : {f1:.3f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(\"\\nPrecision per class:\", precision_per_class)\n",
    "    print(\"Recall per class:\", recall_per_class)\n",
    "    print(\"F1 per class:\", f1_per_class)\n",
    "\n",
    "    misclassified_idx = (y_true != y_pred)\n",
    "    print(f\"\\nMisclassified: {np.sum(misclassified_idx)} / {len(y_true)}\")\n",
    "\n",
    "    if raw_texts is not None:\n",
    "        try:\n",
    "            misclassified_df = pd.DataFrame({\n",
    "                \"text\": np.array(raw_texts)[misclassified_idx],\n",
    "                \"true_label\": y_true[misclassified_idx],\n",
    "                \"pred_label\": y_pred[misclassified_idx]\n",
    "            }).reset_index(drop=True)\n",
    "\n",
    "            print(f\"\\nFirst 5 Misclassified Examples ({split_name}):\")\n",
    "            for i in range(min(5, len(misclassified_df))):\n",
    "                row = misclassified_df.iloc[i]\n",
    "                print(f\"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}\")\n",
    "        except:\n",
    "            print(\"Could not print misclassified examples.\")\n",
    "    return y_pred\n",
    "\n",
    "# Run evaluation on Train / Val / Test\n",
    "_ = evaluate_bilstm_glove(X_train_bilstm_glove, y_train_bilstm_glove, \"Train\", raw_texts=np.array(texts_bilstm_glove)[-len(y_train_bilstm_glove):])\n",
    "_ = evaluate_bilstm_glove(X_val_bilstm_glove, y_val_bilstm_glove, \"Validation\", raw_texts=np.array(texts_bilstm_glove)[-len(y_val_bilstm_glove):])\n",
    "_ = evaluate_bilstm_glove(X_test_bilstm_glove, y_test_bilstm_glove, \"Test\", raw_texts=np.array(texts_bilstm_glove)[-len(y_test_bilstm_glove):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3cce1-e173-4eb4-8766-dbddb772bd15",
   "metadata": {},
   "source": [
    "BiLSTM + GLOVE 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7c520e14-1ee3-4057-833d-6e3f9d2d97d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 51ms/step - accuracy: 0.6464 - loss: 0.6219 - val_accuracy: 0.6978 - val_loss: 0.5709\n",
      "Epoch 2/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 58ms/step - accuracy: 0.7018 - loss: 0.5704 - val_accuracy: 0.7119 - val_loss: 0.5552\n",
      "Epoch 3/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 81ms/step - accuracy: 0.7179 - loss: 0.5520 - val_accuracy: 0.7130 - val_loss: 0.5524\n",
      "Epoch 4/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 85ms/step - accuracy: 0.7301 - loss: 0.5368 - val_accuracy: 0.7158 - val_loss: 0.5477\n",
      "Epoch 5/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 79ms/step - accuracy: 0.7397 - loss: 0.5252 - val_accuracy: 0.7174 - val_loss: 0.5471\n",
      "\n",
      "Training Time (BiLSTM + GloVe 300d): 2134.10s\n"
     ]
    }
   ],
   "source": [
    "# ✅ BiLSTM + GloVe (300d)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "# ===============================\n",
    "# 1. Preprocessing & Tokenization\n",
    "# ===============================\n",
    "texts_bilstm_glove_300 = df_filtered['text'].astype(str).tolist()\n",
    "labels_bilstm_glove_300 = df_filtered['label'].tolist()\n",
    "\n",
    "tokenizer_bilstm_glove_300 = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer_bilstm_glove_300.fit_on_texts(texts_bilstm_glove_300)\n",
    "sequences_bilstm_glove_300 = tokenizer_bilstm_glove_300.texts_to_sequences(texts_bilstm_glove_300)\n",
    "\n",
    "max_len_bilstm_glove_300 = 40\n",
    "X_bilstm_glove_300 = pad_sequences(sequences_bilstm_glove_300, maxlen=max_len_bilstm_glove_300, padding='post', truncating='post')\n",
    "y_bilstm_glove_300 = np.array(labels_bilstm_glove_300)\n",
    "\n",
    "X_temp_bilstm_glove_300, X_test_bilstm_glove_300, y_temp_bilstm_glove_300, y_test_bilstm_glove_300 = train_test_split(\n",
    "    X_bilstm_glove_300, y_bilstm_glove_300, test_size=0.1, random_state=42)\n",
    "X_train_bilstm_glove_300, X_val_bilstm_glove_300, y_train_bilstm_glove_300, y_val_bilstm_glove_300 = train_test_split(\n",
    "    X_temp_bilstm_glove_300, y_temp_bilstm_glove_300, test_size=0.1, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 2. Load GloVe Embeddings (300d)\n",
    "# ===============================\n",
    "embedding_index_bilstm_glove_300 = {}\n",
    "with open('glove.6B.300d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index_bilstm_glove_300[word] = vector\n",
    "\n",
    "embedding_dim_300 = 300\n",
    "word_index_bilstm_glove_300 = tokenizer_bilstm_glove_300.word_index\n",
    "vocab_size_bilstm_glove_300 = len(word_index_bilstm_glove_300) + 1\n",
    "\n",
    "embedding_matrix_bilstm_glove_300 = np.zeros((vocab_size_bilstm_glove_300, embedding_dim_300))\n",
    "for word, i in word_index_bilstm_glove_300.items():\n",
    "    vector = embedding_index_bilstm_glove_300.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix_bilstm_glove_300[i] = vector\n",
    "\n",
    "# ===============================\n",
    "# 3. Define Model\n",
    "# ===============================\n",
    "model_bilstm_glove_300 = Sequential([\n",
    "    Embedding(input_dim=vocab_size_bilstm_glove_300,\n",
    "              output_dim=embedding_dim_300,\n",
    "              weights=[embedding_matrix_bilstm_glove_300],\n",
    "              input_length=max_len_bilstm_glove_300,\n",
    "              trainable=False),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_bilstm_glove_300.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ===============================\n",
    "# 4. Train the Model\n",
    "# ===============================\n",
    "start_train_bilstm_glove_300 = time.time()\n",
    "history_bilstm_glove_300 = model_bilstm_glove_300.fit(\n",
    "    X_train_bilstm_glove_300, y_train_bilstm_glove_300,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_bilstm_glove_300, y_val_bilstm_glove_300),\n",
    "    verbose=1\n",
    ")\n",
    "train_time_bilstm_glove_300 = time.time() - start_train_bilstm_glove_300\n",
    "print(f\"\\nTraining Time (BiLSTM + GloVe 300d): {train_time_bilstm_glove_300:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e39701d6-f63c-47cd-8c22-94cf6f3c2553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Set (BiLSTM + GloVe 300d) ===\n",
      "Inference Time: 375.0775s for 770061 samples\n",
      "  Accuracy : 0.756\n",
      "  Precision: 0.761\n",
      "  Recall   : 0.756\n",
      "  F1 Score : 0.759\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.755     0.753    378870\n",
      "           1      0.761     0.756     0.759    391191\n",
      "\n",
      "    accuracy                          0.756    770061\n",
      "   macro avg      0.756     0.756     0.756    770061\n",
      "weighted avg      0.756     0.756     0.756    770061\n",
      "\n",
      "\n",
      "Precision per class: [0.75019865 0.76127892]\n",
      "Recall per class: [0.75506374 0.75649747]\n",
      "F1 per class: [0.75262333 0.75888066]\n",
      "\n",
      "Misclassified: 188055 / 770061\n",
      "\n",
      "First 5 Misclassified Examples (Train):\n",
      "[1] True: 0 | Pred: 1 | Text: The posters promised they were the toughest and the uniforms were the neatest was not much of a choice really\n",
      "[2] True: 0 | Pred: 1 | Text: My dad once told me Doc\n",
      "[3] True: 1 | Pred: 0 | Text: Why would they lie if they were not paid\n",
      "[4] True: 0 | Pred: 1 | Text: Id circle the globe powered by my explosive diarrhea\n",
      "[5] True: 1 | Pred: 0 | Text: Fast sticky modsgods\n",
      "\n",
      "=== Validation Set (BiLSTM + GloVe 300d) ===\n",
      "Inference Time: 32.4124s for 85563 samples\n",
      "  Accuracy : 0.717\n",
      "  Precision: 0.721\n",
      "  Recall   : 0.720\n",
      "  F1 Score : 0.721\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.714     0.714     0.714     42270\n",
      "           1      0.721     0.720     0.721     43293\n",
      "\n",
      "    accuracy                          0.717     85563\n",
      "   macro avg      0.717     0.717     0.717     85563\n",
      "weighted avg      0.717     0.717     0.717     85563\n",
      "\n",
      "\n",
      "Precision per class: [0.71376238 0.72093776]\n",
      "Recall per class: [0.7144547  0.72025501]\n",
      "F1 per class: [0.71410837 0.72059622]\n",
      "\n",
      "Misclassified: 24181 / 85563\n",
      "\n",
      "First 5 Misclassified Examples (Validation):\n",
      "[1] True: 0 | Pred: 1 | Text: No second amendment right to protect them damn crazies Fondling my gun at night and dry firing it at passerbys is perfectly and within my rights\n",
      "[2] True: 1 | Pred: 0 | Text: In my neighborhood its the kids themselves that keep getting bigger\n",
      "[3] True: 1 | Pred: 0 | Text: Good god you need a healthy dose of Occams razor in your life\n",
      "[4] True: 0 | Pred: 1 | Text: Is there any sort of draft assistant?\n",
      "[5] True: 0 | Pred: 1 | Text: My guess is cocaine\n",
      "\n",
      "=== Test Set (BiLSTM + GloVe 300d) ===\n",
      "Inference Time: 31.6077s for 95070 samples\n",
      "  Accuracy : 0.717\n",
      "  Precision: 0.723\n",
      "  Recall   : 0.719\n",
      "  F1 Score : 0.721\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.710     0.714     0.712     46618\n",
      "           1      0.723     0.719     0.721     48452\n",
      "\n",
      "    accuracy                          0.717     95070\n",
      "   macro avg      0.717     0.717     0.717     95070\n",
      "weighted avg      0.717     0.717     0.717     95070\n",
      "\n",
      "\n",
      "Precision per class: [0.70988786 0.7234449 ]\n",
      "Recall per class: [0.71427346 0.71914472]\n",
      "F1 per class: [0.71207391 0.7212884 ]\n",
      "\n",
      "Misclassified: 26928 / 95070\n",
      "\n",
      "First 5 Misclassified Examples (Test):\n",
      "[1] True: 1 | Pred: 0 | Text: you are right obama totally is big brother\n",
      "[2] True: 1 | Pred: 0 | Text: Also because he is the remover of obstacles !\n",
      "[3] True: 1 | Pred: 0 | Text: But can he do it on another continent?\n",
      "[4] True: 0 | Pred: 1 | Text: OMG what a surprise!\n",
      "[5] True: 1 | Pred: 0 | Text: I like how you left all that persons personal information on the receipt so they could get reddit famous\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 5. Evaluation Function\n",
    "# ===============================\n",
    "def evaluate_bilstm_glove_300(X, y_true, split_name=\"Validation\", raw_texts=None):\n",
    "    start_infer = time.time()\n",
    "    y_pred_probs = model_bilstm_glove_300.predict(X, verbose=0)\n",
    "    infer_time = time.time() - start_infer\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {split_name} Set (BiLSTM + GloVe 300d) ===\")\n",
    "    print(f\"Inference Time: {infer_time:.4f}s for {len(y_true)} samples\")\n",
    "    print(f\"  Accuracy : {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall   : {rec:.3f}\")\n",
    "    print(f\"  F1 Score : {f1:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "    print(\"\\nPrecision per class:\", precision_per_class)\n",
    "    print(\"Recall per class:\", recall_per_class)\n",
    "    print(\"F1 per class:\", f1_per_class)\n",
    "\n",
    "    misclassified_idx = (y_true != y_pred)\n",
    "    print(f\"\\nMisclassified: {np.sum(misclassified_idx)} / {len(y_true)}\")\n",
    "\n",
    "    if raw_texts is not None:\n",
    "        try:\n",
    "            misclassified_df = pd.DataFrame({\n",
    "                \"text\": np.array(raw_texts)[misclassified_idx],\n",
    "                \"true_label\": y_true[misclassified_idx],\n",
    "                \"pred_label\": y_pred[misclassified_idx]\n",
    "            }).reset_index(drop=True)\n",
    "            print(f\"\\nFirst 5 Misclassified Examples ({split_name}):\")\n",
    "            for i in range(min(5, len(misclassified_df))):\n",
    "                row = misclassified_df.iloc[i]\n",
    "                print(f\"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}\")\n",
    "        except:\n",
    "            print(\"Could not print misclassified examples.\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Evaluate Train / Val / Test\n",
    "# ===============================\n",
    "_ = evaluate_bilstm_glove_300(X_train_bilstm_glove_300, y_train_bilstm_glove_300, \"Train\", raw_texts=np.array(texts_bilstm_glove_300)[-len(y_train_bilstm_glove_300):])\n",
    "_ = evaluate_bilstm_glove_300(X_val_bilstm_glove_300, y_val_bilstm_glove_300, \"Validation\", raw_texts=np.array(texts_bilstm_glove_300)[-len(y_val_bilstm_glove_300):])\n",
    "_ = evaluate_bilstm_glove_300(X_test_bilstm_glove_300, y_test_bilstm_glove_300, \"Test\", raw_texts=np.array(texts_bilstm_glove_300)[-len(y_test_bilstm_glove_300):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08dece-f68d-45fc-9f3a-9c787a82d50b",
   "metadata": {},
   "source": [
    "BiLSTM + GLOVE 100 + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "95d5a277-76ef-4b7b-9b17-9884471097cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">84,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │    \u001b[38;5;34m19,357,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m84,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,446,141</span> (74.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,446,141\u001b[0m (74.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,641</span> (346.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m88,641\u001b[0m (346.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> (73.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m19,357,500\u001b[0m (73.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 66ms/step - accuracy: 0.6082 - loss: 0.6589 - val_accuracy: 0.6769 - val_loss: 0.5943\n",
      "Epoch 2/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 60ms/step - accuracy: 0.6827 - loss: 0.5931 - val_accuracy: 0.6878 - val_loss: 0.5808\n",
      "Epoch 3/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 70ms/step - accuracy: 0.7011 - loss: 0.5733 - val_accuracy: 0.7052 - val_loss: 0.5618\n",
      "Epoch 4/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 104ms/step - accuracy: 0.7138 - loss: 0.5585 - val_accuracy: 0.7105 - val_loss: 0.5534\n",
      "Epoch 5/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 93ms/step - accuracy: 0.7212 - loss: 0.5484 - val_accuracy: 0.7129 - val_loss: 0.5508\n",
      "\n",
      "Training Time (BiLSTM + GloVe 100d + Attention): 2366.92s\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# BiLSTM + GloVe (100d) + Attention\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, Layer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# 1. Prepare Data\n",
    "texts_glove100_attn = df_filtered['text'].astype(str).tolist()\n",
    "labels_glove100_attn = df_filtered['label'].tolist()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer_glove100_attn = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer_glove100_attn.fit_on_texts(texts_glove100_attn)\n",
    "sequences_glove100_attn = tokenizer_glove100_attn.texts_to_sequences(texts_glove100_attn)\n",
    "\n",
    "max_len_glove100_attn = 40\n",
    "X_glove100_attn = pad_sequences(sequences_glove100_attn, maxlen=max_len_glove100_attn, padding='post', truncating='post')\n",
    "y_glove100_attn = np.array(labels_glove100_attn)\n",
    "\n",
    "# Train/Val/Test Split\n",
    "X_temp_glove100_attn, X_test_glove100_attn, y_temp_glove100_attn, y_test_glove100_attn = train_test_split(\n",
    "    X_glove100_attn, y_glove100_attn, test_size=0.1, random_state=42)\n",
    "X_train_glove100_attn, X_val_glove100_attn, y_train_glove100_attn, y_val_glove100_attn = train_test_split(\n",
    "    X_temp_glove100_attn, y_temp_glove100_attn, test_size=0.1, random_state=42)\n",
    "\n",
    "# 2. Load GloVe 100d\n",
    "embedding_index_glove100_attn = {}\n",
    "with open(\"glove.6B.100d.txt\", encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index_glove100_attn[word] = vector\n",
    "\n",
    "embedding_dim_glove100_attn = 100\n",
    "word_index_glove100_attn = tokenizer_glove100_attn.word_index\n",
    "vocab_size_glove100_attn = len(word_index_glove100_attn) + 1\n",
    "\n",
    "embedding_matrix_glove100_attn = np.zeros((vocab_size_glove100_attn, embedding_dim_glove100_attn))\n",
    "for word, i in word_index_glove100_attn.items():\n",
    "    vector = embedding_index_glove100_attn.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix_glove100_attn[i] = vector\n",
    "\n",
    "# 3. Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, value = inputs, inputs\n",
    "        score = tf.matmul(query, value, transpose_b=True)\n",
    "        weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(weights, value)\n",
    "        return tf.reduce_sum(context, axis=1)\n",
    "\n",
    "# 4. Build Model\n",
    "input_layer = Input(shape=(max_len_glove100_attn,))\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=vocab_size_glove100_attn,\n",
    "    output_dim=embedding_dim_glove100_attn,\n",
    "    weights=[embedding_matrix_glove100_attn],\n",
    "    input_length=max_len_glove100_attn,\n",
    "    trainable=False\n",
    ")(input_layer)\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
    "attention_out = AttentionLayer()(lstm_out)\n",
    "dense1 = Dense(32, activation='relu')(attention_out)\n",
    "dropout = Dropout(0.3)(dense1)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "model_glove100_attn = Model(inputs=input_layer, outputs=output_layer)\n",
    "model_glove100_attn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_glove100_attn.summary()\n",
    "\n",
    "# 5. Train\n",
    "start_train_glove100_attn = time.time()\n",
    "history_glove100_attn = model_glove100_attn.fit(\n",
    "    X_train_glove100_attn, y_train_glove100_attn,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_glove100_attn, y_val_glove100_attn),\n",
    "    verbose=1\n",
    ")\n",
    "train_time_glove100_attn = time.time() - start_train_glove100_attn\n",
    "print(f\"\\nTraining Time (BiLSTM + GloVe 100d + Attention): {train_time_glove100_attn:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0ff69da5-ab75-428d-9224-685125d3cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Set (BiLSTM + GloVe 100d + Attention) ===\n",
      "Inference Time: 290.2400s for 770061 samples\n",
      "  Accuracy : 0.730\n",
      "  Precision: 0.735\n",
      "  Recall   : 0.732\n",
      "  F1 Score : 0.734\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.725     0.727     0.726    378870\n",
      "           1      0.735     0.732     0.734    391191\n",
      "\n",
      "    accuracy                          0.730    770061\n",
      "   macro avg      0.730     0.730     0.730    770061\n",
      "weighted avg      0.730     0.730     0.730    770061\n",
      "\n",
      "\n",
      "Precision per class: [0.72455435 0.73495655]\n",
      "Recall per class: [0.72736559 0.73219476]\n",
      "F1 per class: [0.72595725 0.73357305]\n",
      "\n",
      "Misclassified: 208056 / 770061\n",
      "\n",
      "First 5 Misclassified Examples (Train):\n",
      "[1] True: 0 | Pred: 1 | Text: The posters promised they were the toughest and the uniforms were the neatest was not much of a choice really\n",
      "[2] True: 1 | Pred: 0 | Text: Why would they lie if they were not paid\n",
      "[3] True: 0 | Pred: 1 | Text: Id circle the globe powered by my explosive diarrhea\n",
      "[4] True: 0 | Pred: 1 | Text: They Are releasing too many non3x3s!\n",
      "[5] True: 1 | Pred: 0 | Text: Fast sticky modsgods\n",
      "\n",
      "=== Validation Set (BiLSTM + GloVe 100d + Attention) ===\n",
      "Inference Time: 38.1708s for 85563 samples\n",
      "  Accuracy : 0.713\n",
      "  Precision: 0.717\n",
      "  Recall   : 0.716\n",
      "  F1 Score : 0.716\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.709     0.710     0.710     42270\n",
      "           1      0.717     0.716     0.716     43293\n",
      "\n",
      "    accuracy                          0.713     85563\n",
      "   macro avg      0.713     0.713     0.713     85563\n",
      "weighted avg      0.713     0.713     0.713     85563\n",
      "\n",
      "\n",
      "Precision per class: [0.70916824 0.71653216]\n",
      "Recall per class: [0.7100071  0.71570462]\n",
      "F1 per class: [0.70958742 0.71611815]\n",
      "\n",
      "Misclassified: 24566 / 85563\n",
      "\n",
      "First 5 Misclassified Examples (Validation):\n",
      "[1] True: 0 | Pred: 1 | Text: No second amendment right to protect them damn crazies Fondling my gun at night and dry firing it at passerbys is perfectly and within my rights\n",
      "[2] True: 0 | Pred: 1 | Text: Omg hes so right that is why when you walk on a college campus you soo cannot tell who is a freshman\n",
      "[3] True: 1 | Pred: 0 | Text: In my neighborhood its the kids themselves that keep getting bigger\n",
      "[4] True: 1 | Pred: 0 | Text: Good god you need a healthy dose of Occams razor in your life\n",
      "[5] True: 0 | Pred: 1 | Text: Is there any sort of draft assistant?\n",
      "\n",
      "=== Test Set (BiLSTM + GloVe 100d + Attention) ===\n",
      "Inference Time: 51.2146s for 95070 samples\n",
      "  Accuracy : 0.713\n",
      "  Precision: 0.718\n",
      "  Recall   : 0.717\n",
      "  F1 Score : 0.718\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.706     0.708     0.707     46618\n",
      "           1      0.718     0.717     0.718     48452\n",
      "\n",
      "    accuracy                          0.713     95070\n",
      "   macro avg      0.712     0.712     0.712     95070\n",
      "weighted avg      0.713     0.713     0.713     95070\n",
      "\n",
      "\n",
      "Precision per class: [0.706338   0.71847071]\n",
      "Recall per class: [0.70809559 0.7167506 ]\n",
      "F1 per class: [0.7072157  0.71760962]\n",
      "\n",
      "Misclassified: 27332 / 95070\n",
      "\n",
      "First 5 Misclassified Examples (Test):\n",
      "[1] True: 1 | Pred: 0 | Text: Yes because there are no voices within any of those fields that would argue that patriarchy is a myth\n",
      "[2] True: 0 | Pred: 1 | Text: Its Liberal hype to promote Global Warming\n",
      "[3] True: 0 | Pred: 1 | Text: OMG what a surprise!\n",
      "[4] True: 1 | Pred: 0 | Text: Beatles move over\n",
      "[5] True: 0 | Pred: 1 | Text: Haha america is so awesome its great to live in a country where being verbally disrespectful means jail time fuck that 1st amendment who needs that anyway\n"
     ]
    }
   ],
   "source": [
    "# 6. Evaluation\n",
    "\n",
    "def evaluate_glove100_attn(X, y_true, split_name=\"Validation\", raw_texts=None):\n",
    "    start_infer = time.time()\n",
    "    y_pred_probs = model_glove100_attn.predict(X, verbose=0)\n",
    "    infer_time = time.time() - start_infer\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {split_name} Set (BiLSTM + GloVe 100d + Attention) ===\")\n",
    "    print(f\"Inference Time: {infer_time:.4f}s for {len(y_true)} samples\")\n",
    "    print(f\"  Accuracy : {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall   : {rec:.3f}\")\n",
    "    print(f\"  F1 Score : {f1:.3f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(\"\\nPrecision per class:\", precision_per_class)\n",
    "    print(\"Recall per class:\", recall_per_class)\n",
    "    print(\"F1 per class:\", f1_per_class)\n",
    "\n",
    "    misclassified_idx = (y_true != y_pred)\n",
    "    print(f\"\\nMisclassified: {np.sum(misclassified_idx)} / {len(y_true)}\")\n",
    "\n",
    "    if raw_texts is not None:\n",
    "        try:\n",
    "            misclassified_df = pd.DataFrame({\n",
    "                \"text\": np.array(raw_texts)[misclassified_idx],\n",
    "                \"true_label\": y_true[misclassified_idx],\n",
    "                \"pred_label\": y_pred[misclassified_idx]\n",
    "            }).reset_index(drop=True)\n",
    "\n",
    "            print(f\"\\nFirst 5 Misclassified Examples ({split_name}):\")\n",
    "            for i in range(min(5, len(misclassified_df))):\n",
    "                row = misclassified_df.iloc[i]\n",
    "                print(f\"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}\")\n",
    "        except:\n",
    "            print(\"Could not print misclassified examples.\")\n",
    "    return y_pred\n",
    "\n",
    "# Run evaluation\n",
    "_ = evaluate_glove100_attn(X_train_glove100_attn, y_train_glove100_attn, \"Train\", raw_texts=np.array(texts_glove100_attn)[-len(y_train_glove100_attn):])\n",
    "_ = evaluate_glove100_attn(X_val_glove100_attn, y_val_glove100_attn, \"Validation\", raw_texts=np.array(texts_glove100_attn)[-len(y_val_glove100_attn):])\n",
    "_ = evaluate_glove100_attn(X_test_glove100_attn, y_test_glove100_attn, \"Test\", raw_texts=np.array(texts_glove100_attn)[-len(y_test_glove100_attn):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94814a46-8f39-4f22-afd2-9aaa479be6a7",
   "metadata": {},
   "source": [
    "BiLSTM + GLOVE 300 + Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "423eb226-a6ea-433c-9d2b-694b95d4180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">58,072,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">186,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer_1               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">168</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionLayer</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m300\u001b[0m)        │    \u001b[38;5;34m58,072,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m186,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_layer_1               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m168\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionLayer\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,263,709</span> (222.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,263,709\u001b[0m (222.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">191,209</span> (746.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m191,209\u001b[0m (746.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,072,500</span> (221.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m58,072,500\u001b[0m (221.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 89ms/step - accuracy: 0.6545 - loss: 0.6134 - val_accuracy: 0.6967 - val_loss: 0.5706\n",
      "Epoch 2/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m508s\u001b[0m 84ms/step - accuracy: 0.7153 - loss: 0.5535 - val_accuracy: 0.7154 - val_loss: 0.5475\n",
      "Epoch 3/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 78ms/step - accuracy: 0.7310 - loss: 0.5342 - val_accuracy: 0.7210 - val_loss: 0.5413\n",
      "Epoch 4/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 61ms/step - accuracy: 0.7424 - loss: 0.5182 - val_accuracy: 0.7195 - val_loss: 0.5446\n",
      "Epoch 5/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 92ms/step - accuracy: 0.7530 - loss: 0.5041 - val_accuracy: 0.7224 - val_loss: 0.5422\n",
      "\n",
      "Training Time (BiLSTM + GloVe 300d + Attention): 2441.84s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# ===============================\n",
    "# 1. Preprocessing & Tokenization\n",
    "# ===============================\n",
    "texts_bilstm_glove300_attn = df_filtered['text'].astype(str).tolist()\n",
    "labels_bilstm_glove300_attn = df_filtered['label'].tolist()\n",
    "\n",
    "tokenizer_bilstm_glove300_attn = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer_bilstm_glove300_attn.fit_on_texts(texts_bilstm_glove300_attn)\n",
    "sequences_bilstm_glove300_attn = tokenizer_bilstm_glove300_attn.texts_to_sequences(texts_bilstm_glove300_attn)\n",
    "\n",
    "max_len_bilstm_glove300_attn = 40\n",
    "X_bilstm_glove300_attn = pad_sequences(sequences_bilstm_glove300_attn, maxlen=max_len_bilstm_glove300_attn, padding='post', truncating='post')\n",
    "y_bilstm_glove300_attn = np.array(labels_bilstm_glove300_attn)\n",
    "\n",
    "# ✅ Train / Val / Test split\n",
    "X_temp_bilstm_glove300_attn, X_test_bilstm_glove300_attn, y_temp_bilstm_glove300_attn, y_test_bilstm_glove300_attn = train_test_split(\n",
    "    X_bilstm_glove300_attn, y_bilstm_glove300_attn, test_size=0.1, random_state=42)\n",
    "X_train_bilstm_glove300_attn, X_val_bilstm_glove300_attn, y_train_bilstm_glove300_attn, y_val_bilstm_glove300_attn = train_test_split(\n",
    "    X_temp_bilstm_glove300_attn, y_temp_bilstm_glove300_attn, test_size=0.1, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 2. Load GloVe Embeddings (300d)\n",
    "# ===============================\n",
    "glove_path_300 = 'glove.6B.300d.txt'\n",
    "embedding_index_bilstm_glove300_attn = {}\n",
    "\n",
    "with open(glove_path_300, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index_bilstm_glove300_attn[word] = vector\n",
    "\n",
    "embedding_dim_300 = 300\n",
    "word_index_bilstm_glove300_attn = tokenizer_bilstm_glove300_attn.word_index\n",
    "vocab_size_bilstm_glove300_attn = len(word_index_bilstm_glove300_attn) + 1\n",
    "\n",
    "embedding_matrix_bilstm_glove300_attn = np.zeros((vocab_size_bilstm_glove300_attn, embedding_dim_300))\n",
    "for word, i in word_index_bilstm_glove300_attn.items():\n",
    "    vector = embedding_index_bilstm_glove300_attn.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix_bilstm_glove300_attn[i] = vector\n",
    "\n",
    "# ===============================\n",
    "# 3. Define BiLSTM + Attention Model\n",
    "# ===============================\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1), initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        e = K.tanh(K.dot(inputs, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = inputs * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "input_bilstm_glove300_attn = Input(shape=(max_len_bilstm_glove300_attn,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size_bilstm_glove300_attn,\n",
    "                             output_dim=embedding_dim_300,\n",
    "                             weights=[embedding_matrix_bilstm_glove300_attn],\n",
    "                             input_length=max_len_bilstm_glove300_attn,\n",
    "                             trainable=False)(input_bilstm_glove300_attn)\n",
    "\n",
    "bilstm_layer = Bidirectional(LSTM(64, return_sequences=True))(embedding_layer)\n",
    "attention_output = AttentionLayer()(bilstm_layer)\n",
    "dense1 = Dense(32, activation='relu')(attention_output)\n",
    "drop1 = Dropout(0.3)(dense1)\n",
    "output_bilstm_glove300_attn = Dense(1, activation='sigmoid')(drop1)\n",
    "\n",
    "model_bilstm_glove300_attn = Model(inputs=input_bilstm_glove300_attn, outputs=output_bilstm_glove300_attn)\n",
    "model_bilstm_glove300_attn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_bilstm_glove300_attn.summary()\n",
    "\n",
    "# ===============================\n",
    "# 4. Train the Model\n",
    "# ===============================\n",
    "start_train_bilstm_glove300_attn = time.time()\n",
    "history_bilstm_glove300_attn = model_bilstm_glove300_attn.fit(\n",
    "    X_train_bilstm_glove300_attn, y_train_bilstm_glove300_attn,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_bilstm_glove300_attn, y_val_bilstm_glove300_attn),\n",
    "    verbose=1\n",
    ")\n",
    "train_time_bilstm_glove300_attn = time.time() - start_train_bilstm_glove300_attn\n",
    "print(f\"\\nTraining Time (BiLSTM + GloVe 300d + Attention): {train_time_bilstm_glove300_attn:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b7681bd1-195f-428d-8fc3-e6b2b618a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Set (BiLSTM + GloVe 300d + Attention) ===\n",
      "Inference Time: 611.0598s for 770061 samples\n",
      "  Accuracy : 0.767\n",
      "  Precision: 0.782\n",
      "  Recall   : 0.749\n",
      "  F1 Score : 0.765\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.752     0.785     0.768    378870\n",
      "           1      0.782     0.749     0.765    391191\n",
      "\n",
      "    accuracy                          0.767    770061\n",
      "   macro avg      0.767     0.767     0.767    770061\n",
      "weighted avg      0.767     0.767     0.767    770061\n",
      "\n",
      "\n",
      "Precision per class: [0.7515484 0.7823872]\n",
      "Recall per class: [0.78499485 0.74866497]\n",
      "F1 per class: [0.76790761 0.76515471]\n",
      "\n",
      "Misclassified: 179779 / 770061\n",
      "\n",
      "First 5 Misclassified Examples (Train):\n",
      "[1] True: 1 | Pred: 0 | Text: Why would they lie if they were not paid\n",
      "[2] True: 0 | Pred: 1 | Text: Id circle the globe powered by my explosive diarrhea\n",
      "[3] True: 1 | Pred: 0 | Text: Fast sticky modsgods\n",
      "[4] True: 1 | Pred: 0 | Text: Yeah just like all Germans have no sense of humor and all Americans are fat\n",
      "[5] True: 1 | Pred: 0 | Text: I cannot tell if you are being sarcastic but I genuinely appreciate it when people do that\n",
      "\n",
      "=== Validation Set (BiLSTM + GloVe 300d + Attention) ===\n",
      "Inference Time: 62.5183s for 85563 samples\n",
      "  Accuracy : 0.722\n",
      "  Precision: 0.735\n",
      "  Recall   : 0.705\n",
      "  F1 Score : 0.720\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.710     0.740     0.725     42270\n",
      "           1      0.735     0.705     0.720     43293\n",
      "\n",
      "    accuracy                          0.722     85563\n",
      "   macro avg      0.723     0.723     0.722     85563\n",
      "weighted avg      0.723     0.722     0.722     85563\n",
      "\n",
      "\n",
      "Precision per class: [0.71019    0.73538906]\n",
      "Recall per class: [0.74014668 0.70510244]\n",
      "F1 per class: [0.72485896 0.71992736]\n",
      "\n",
      "Misclassified: 23751 / 85563\n",
      "\n",
      "First 5 Misclassified Examples (Validation):\n",
      "[1] True: 0 | Pred: 1 | Text: Omg hes so right that is why when you walk on a college campus you soo cannot tell who is a freshman\n",
      "[2] True: 1 | Pred: 0 | Text: In my neighborhood its the kids themselves that keep getting bigger\n",
      "[3] True: 1 | Pred: 0 | Text: Good god you need a healthy dose of Occams razor in your life\n",
      "[4] True: 0 | Pred: 1 | Text: Is there any sort of draft assistant?\n",
      "[5] True: 0 | Pred: 1 | Text: My guess is cocaine\n",
      "\n",
      "=== Test Set (BiLSTM + GloVe 300d + Attention) ===\n",
      "Inference Time: 67.3430s for 95070 samples\n",
      "  Accuracy : 0.721\n",
      "  Precision: 0.737\n",
      "  Recall   : 0.705\n",
      "  F1 Score : 0.721\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.707     0.738     0.722     46618\n",
      "           1      0.737     0.705     0.721     48452\n",
      "\n",
      "    accuracy                          0.721     95070\n",
      "   macro avg      0.722     0.722     0.721     95070\n",
      "weighted avg      0.722     0.721     0.721     95070\n",
      "\n",
      "\n",
      "Precision per class: [0.70656973 0.73665826]\n",
      "Recall per class: [0.73801965 0.70511021]\n",
      "F1 per class: [0.72195235 0.72053908]\n",
      "\n",
      "Misclassified: 26501 / 95070\n",
      "\n",
      "First 5 Misclassified Examples (Test):\n",
      "[1] True: 1 | Pred: 0 | Text: you are right obama totally is big brother\n",
      "[2] True: 1 | Pred: 0 | Text: Also because he is the remover of obstacles !\n",
      "[3] True: 0 | Pred: 1 | Text: Its Liberal hype to promote Global Warming\n",
      "[4] True: 1 | Pred: 0 | Text: Beatles move over\n",
      "[5] True: 1 | Pred: 0 | Text: Corporations are people my friend but teachers and firefighters are not\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 5. Evaluation\n",
    "# ===============================\n",
    "def evaluate_bilstm_glove300_attn(X, y_true, split_name=\"Validation\", raw_texts=None):\n",
    "    start_infer = time.time()\n",
    "    y_pred_probs = model_bilstm_glove300_attn.predict(X, verbose=0)\n",
    "    infer_time = time.time() - start_infer\n",
    "\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {split_name} Set (BiLSTM + GloVe 300d + Attention) ===\")\n",
    "    print(f\"Inference Time: {infer_time:.4f}s for {len(y_true)} samples\")\n",
    "    print(f\"  Accuracy : {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall   : {rec:.3f}\")\n",
    "    print(f\"  F1 Score : {f1:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(\"\\nPrecision per class:\", precision_per_class)\n",
    "    print(\"Recall per class:\", recall_per_class)\n",
    "    print(\"F1 per class:\", f1_per_class)\n",
    "\n",
    "    misclassified_idx = (y_true != y_pred)\n",
    "    print(f\"\\nMisclassified: {np.sum(misclassified_idx)} / {len(y_true)}\")\n",
    "\n",
    "    if raw_texts is not None:\n",
    "        try:\n",
    "            misclassified_df_bilstm_glove300_attn = pd.DataFrame({\n",
    "                \"text\": np.array(raw_texts)[misclassified_idx],\n",
    "                \"true_label\": y_true[misclassified_idx],\n",
    "                \"pred_label\": y_pred[misclassified_idx]\n",
    "            }).reset_index(drop=True)\n",
    "\n",
    "            print(f\"\\nFirst 5 Misclassified Examples ({split_name}):\")\n",
    "            for i in range(min(5, len(misclassified_df_bilstm_glove300_attn))):\n",
    "                row = misclassified_df_bilstm_glove300_attn.iloc[i]\n",
    "                print(f\"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}\")\n",
    "        except:\n",
    "            print(\"Could not print misclassified examples.\")\n",
    "    return y_pred\n",
    "\n",
    "# Run evaluation\n",
    "_ = evaluate_bilstm_glove300_attn(X_train_bilstm_glove300_attn, y_train_bilstm_glove300_attn, \"Train\", raw_texts=np.array(texts_bilstm_glove300_attn)[-len(y_train_bilstm_glove300_attn):])\n",
    "_ = evaluate_bilstm_glove300_attn(X_val_bilstm_glove300_attn, y_val_bilstm_glove300_attn, \"Validation\", raw_texts=np.array(texts_bilstm_glove300_attn)[-len(y_val_bilstm_glove300_attn):])\n",
    "_ = evaluate_bilstm_glove300_attn(X_test_bilstm_glove300_attn, y_test_bilstm_glove300_attn, \"Test\", raw_texts=np.array(texts_bilstm_glove300_attn)[-len(y_test_bilstm_glove300_attn):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc27e7-a0d6-4888-81ff-b92b1e6db8a2",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2ad8f178-8572-455d-985c-fc31600ffbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │    \u001b[38;5;34m19,357,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> (73.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,357,500\u001b[0m (73.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,357,500</span> (73.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m19,357,500\u001b[0m (73.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.5877 - loss: 0.6653 - val_accuracy: 0.6505 - val_loss: 0.6228\n",
      "Epoch 2/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.6412 - loss: 0.6291 - val_accuracy: 0.6642 - val_loss: 0.6124\n",
      "Epoch 3/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 21ms/step - accuracy: 0.6515 - loss: 0.6195 - val_accuracy: 0.6676 - val_loss: 0.6087\n",
      "Epoch 4/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.6600 - loss: 0.6124 - val_accuracy: 0.6746 - val_loss: 0.6063\n",
      "Epoch 5/5\n",
      "\u001b[1m6017/6017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.6643 - loss: 0.6080 - val_accuracy: 0.6782 - val_loss: 0.5991\n",
      "\n",
      "Training Time (CNN + GloVe 100d): 607.05s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "# ===============================\n",
    "# 1. Preprocessing & Tokenization\n",
    "# ===============================\n",
    "texts_cnn_glove100 = df_filtered['text'].astype(str).tolist()\n",
    "labels_cnn_glove100 = df_filtered['label'].tolist()\n",
    "\n",
    "tokenizer_cnn_glove100 = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer_cnn_glove100.fit_on_texts(texts_cnn_glove100)\n",
    "sequences_cnn_glove100 = tokenizer_cnn_glove100.texts_to_sequences(texts_cnn_glove100)\n",
    "\n",
    "max_len_cnn_glove100 = 40\n",
    "X_cnn_glove100 = pad_sequences(sequences_cnn_glove100, maxlen=max_len_cnn_glove100, padding='post', truncating='post')\n",
    "y_cnn_glove100 = np.array(labels_cnn_glove100)\n",
    "\n",
    "# ✅ Train / Val / Test split\n",
    "X_temp_cnn_glove100, X_test_cnn_glove100, y_temp_cnn_glove100, y_test_cnn_glove100 = train_test_split(\n",
    "    X_cnn_glove100, y_cnn_glove100, test_size=0.1, random_state=42)\n",
    "X_train_cnn_glove100, X_val_cnn_glove100, y_train_cnn_glove100, y_val_cnn_glove100 = train_test_split(\n",
    "    X_temp_cnn_glove100, y_temp_cnn_glove100, test_size=0.1, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 2. Load GloVe Embeddings (100d)\n",
    "# ===============================\n",
    "glove_path_cnn = 'glove.6B.100d.txt'\n",
    "embedding_index_cnn_glove100 = {}\n",
    "\n",
    "with open(glove_path_cnn, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index_cnn_glove100[word] = vector\n",
    "\n",
    "embedding_dim_cnn = 100\n",
    "word_index_cnn_glove100 = tokenizer_cnn_glove100.word_index\n",
    "vocab_size_cnn_glove100 = len(word_index_cnn_glove100) + 1\n",
    "\n",
    "embedding_matrix_cnn_glove100 = np.zeros((vocab_size_cnn_glove100, embedding_dim_cnn))\n",
    "for word, i in word_index_cnn_glove100.items():\n",
    "    vector = embedding_index_cnn_glove100.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix_cnn_glove100[i] = vector\n",
    "\n",
    "# ===============================\n",
    "# 3. Define CNN Model\n",
    "# ===============================\n",
    "model_cnn_glove100 = Sequential([\n",
    "    Embedding(input_dim=vocab_size_cnn_glove100,\n",
    "              output_dim=embedding_dim_cnn,\n",
    "              weights=[embedding_matrix_cnn_glove100],\n",
    "              input_length=max_len_cnn_glove100,\n",
    "              trainable=False),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn_glove100.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn_glove100.summary()\n",
    "\n",
    "# ===============================\n",
    "# 4. Train the Model\n",
    "# ===============================\n",
    "start_train_cnn_glove100 = time.time()\n",
    "history_cnn_glove100 = model_cnn_glove100.fit(\n",
    "    X_train_cnn_glove100, y_train_cnn_glove100,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_cnn_glove100, y_val_cnn_glove100),\n",
    "    verbose=1\n",
    ")\n",
    "train_time_cnn_glove100 = time.time() - start_train_cnn_glove100\n",
    "print(f\"\\nTraining Time (CNN + GloVe 100d): {train_time_cnn_glove100:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "da513725-3bc6-4943-b0d4-19f5ca9df9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train Set (CNN + GloVe 100d) ===\n",
      "Inference Time: 85.4828s for 770061 samples\n",
      "  Accuracy : 0.690\n",
      "  Precision: 0.722\n",
      "  Recall   : 0.633\n",
      "  F1 Score : 0.674\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.664     0.748     0.703    378870\n",
      "           1      0.722     0.633     0.674    391191\n",
      "\n",
      "    accuracy                          0.690    770061\n",
      "   macro avg      0.693     0.690     0.689    770061\n",
      "weighted avg      0.693     0.690     0.689    770061\n",
      "\n",
      "\n",
      "Precision per class: [0.66364679 0.7218358 ]\n",
      "Recall per class: [0.74825138 0.63271139]\n",
      "F1 per class: [0.70341422 0.67434156]\n",
      "\n",
      "Misclassified: 239060 / 770061\n",
      "\n",
      "First 5 Misclassified Examples (Train):\n",
      "[1] True: 0 | Pred: 1 | Text: The posters promised they were the toughest and the uniforms were the neatest was not much of a choice really\n",
      "[2] True: 0 | Pred: 1 | Text: My dad once told me Doc\n",
      "[3] True: 1 | Pred: 0 | Text: I mean this is best gunnit\n",
      "[4] True: 1 | Pred: 0 | Text: Why would they lie if they were not paid\n",
      "[5] True: 1 | Pred: 0 | Text: Fast sticky modsgods\n",
      "\n",
      "=== Validation Set (CNN + GloVe 100d) ===\n",
      "Inference Time: 9.3351s for 85563 samples\n",
      "  Accuracy : 0.678\n",
      "  Precision: 0.707\n",
      "  Recall   : 0.621\n",
      "  F1 Score : 0.661\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.655     0.737     0.693     42270\n",
      "           1      0.707     0.621     0.661     43293\n",
      "\n",
      "    accuracy                          0.678     85563\n",
      "   macro avg      0.681     0.679     0.677     85563\n",
      "weighted avg      0.681     0.678     0.677     85563\n",
      "\n",
      "\n",
      "Precision per class: [0.65487302 0.70735623]\n",
      "Recall per class: [0.73695292 0.6207932 ]\n",
      "F1 per class: [0.69349273 0.66125381]\n",
      "\n",
      "Misclassified: 27536 / 85563\n",
      "\n",
      "First 5 Misclassified Examples (Validation):\n",
      "[1] True: 0 | Pred: 1 | Text: because that is how evolution works\n",
      "[2] True: 0 | Pred: 1 | Text: No second amendment right to protect them damn crazies Fondling my gun at night and dry firing it at passerbys is perfectly and within my rights\n",
      "[3] True: 0 | Pred: 1 | Text: Omg hes so right that is why when you walk on a college campus you soo cannot tell who is a freshman\n",
      "[4] True: 1 | Pred: 0 | Text: In my neighborhood its the kids themselves that keep getting bigger\n",
      "[5] True: 1 | Pred: 0 | Text: Good god you need a healthy dose of Occams razor in your life\n",
      "\n",
      "=== Test Set (CNN + GloVe 100d) ===\n",
      "Inference Time: 9.4145s for 95070 samples\n",
      "  Accuracy : 0.678\n",
      "  Precision: 0.710\n",
      "  Recall   : 0.621\n",
      "  F1 Score : 0.663\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.652     0.737     0.692     46618\n",
      "           1      0.710     0.621     0.663     48452\n",
      "\n",
      "    accuracy                          0.678     95070\n",
      "   macro avg      0.681     0.679     0.677     95070\n",
      "weighted avg      0.682     0.678     0.677     95070\n",
      "\n",
      "\n",
      "Precision per class: [0.65171033 0.7104176 ]\n",
      "Recall per class: [0.7368613  0.62110955]\n",
      "F1 per class: [0.69167497 0.66276855]\n",
      "\n",
      "Misclassified: 30625 / 95070\n",
      "\n",
      "First 5 Misclassified Examples (Test):\n",
      "[1] True: 1 | Pred: 0 | Text: Yes because there are no voices within any of those fields that would argue that patriarchy is a myth\n",
      "[2] True: 1 | Pred: 0 | Text: Also because he is the remover of obstacles !\n",
      "[3] True: 0 | Pred: 1 | Text: I almost thing the Obama administration is in league with gungearammo distributors as every time something happens there is a talk about control and everything everywhere sells out immediately\n",
      "[4] True: 0 | Pred: 1 | Text: OMG what a surprise!\n",
      "[5] True: 0 | Pred: 1 | Text: Haha america is so awesome its great to live in a country where being verbally disrespectful means jail time fuck that 1st amendment who needs that anyway\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===============================\n",
    "# 5. Evaluation\n",
    "# ===============================\n",
    "def evaluate_cnn_glove100(X, y_true, split_name=\"Validation\", raw_texts=None):\n",
    "    start_infer = time.time()\n",
    "    y_pred_probs = model_cnn_glove100.predict(X, verbose=0)\n",
    "    infer_time = time.time() - start_infer\n",
    "\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {split_name} Set (CNN + GloVe 100d) ===\")\n",
    "    print(f\"Inference Time: {infer_time:.4f}s for {len(y_true)} samples\")\n",
    "    print(f\"  Accuracy : {acc:.3f}\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall   : {rec:.3f}\")\n",
    "    print(f\"  F1 Score : {f1:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(\"\\nPrecision per class:\", precision_per_class)\n",
    "    print(\"Recall per class:\", recall_per_class)\n",
    "    print(\"F1 per class:\", f1_per_class)\n",
    "\n",
    "    misclassified_idx = (y_true != y_pred)\n",
    "    print(f\"\\nMisclassified: {np.sum(misclassified_idx)} / {len(y_true)}\")\n",
    "\n",
    "    if raw_texts is not None:\n",
    "        try:\n",
    "            misclassified_df_cnn_glove100 = pd.DataFrame({\n",
    "                \"text\": np.array(raw_texts)[misclassified_idx],\n",
    "                \"true_label\": y_true[misclassified_idx],\n",
    "                \"pred_label\": y_pred[misclassified_idx]\n",
    "            }).reset_index(drop=True)\n",
    "\n",
    "            print(f\"\\nFirst 5 Misclassified Examples ({split_name}):\")\n",
    "            for i in range(min(5, len(misclassified_df_cnn_glove100))):\n",
    "                row = misclassified_df_cnn_glove100.iloc[i]\n",
    "                print(f\"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}\")\n",
    "        except:\n",
    "            print(\"Could not print misclassified examples.\")\n",
    "    return y_pred\n",
    "\n",
    "# Run evaluation\n",
    "_ = evaluate_cnn_glove100(X_train_cnn_glove100, y_train_cnn_glove100, \"Train\", raw_texts=np.array(texts_cnn_glove100)[-len(y_train_cnn_glove100):])\n",
    "_ = evaluate_cnn_glove100(X_val_cnn_glove100, y_val_cnn_glove100, \"Validation\", raw_texts=np.array(texts_cnn_glove100)[-len(y_val_cnn_glove100):])\n",
    "_ = evaluate_cnn_glove100(X_test_cnn_glove100, y_test_cnn_glove100, \"Test\", raw_texts=np.array(texts_cnn_glove100)[-len(y_test_cnn_glove100):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb0f52-164a-4ded-97d2-283d00b19b34",
   "metadata": {},
   "source": [
    "# 5. Transformers — Per-Domain Analysis\n",
    "\n",
    "This section digs into how **BERT**, **RoBERTa**, and **DistilRoBERTa** behave on each source domain (**News**, **Reddit**, **Twitter**) under the balanced/equalized setup. The goal is to separate **true domain difficulty** from data-size effects and surface **model–domain matches** you can actually use.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 What was controlled\n",
    "\n",
    "* **Balanced labels (50/50) per domain**, then **equalized total N** across domains (per seed).\n",
    "* **Three independent balanced variants** (seeds), each split **70/20/10**.\n",
    "* Same tokenizer, **max\\_len=128**, **3 epochs**, **2e-5** LR, linear warm-up.\n",
    "* Report **mean ± SD over seeds** per domain.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Headline per-domain performance (Test)\n",
    "\n",
    "| Domain      | Best model        |         F1 (± sd) | Accuracy (± sd) |  AUPRC (± sd) | Notes                                                  |\n",
    "| ----------- | ----------------- | ----------------: | --------------: | ------------: | ------------------------------------------------------ |\n",
    "| **News**    | **RoBERTa**       | **0.868 ± 0.008** |   0.876 ± 0.008 | 0.966 ± 0.009 | Clear winner; very strong precision (≈0.93).           |\n",
    "| **Reddit**  | **DistilRoBERTa** | **0.652 ± 0.041** |   0.651 ± 0.042 | 0.726 ± 0.062 | Hardest domain; Distil slightly ahead of BERT/RoBERTa. |\n",
    "| **Twitter** | **BERT**          | **0.658 ± 0.025** |   0.636 ± 0.031 | 0.746 ± 0.025 | BERT edges out; all three close.                       |\n",
    "\n",
    "**Takeaway:** Domains differ more than models. **News** is comparatively easy, **Reddit** is challenging (context, irony), **Twitter** sits in between.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Model averages & efficiency (grand means over domains × seeds)\n",
    "\n",
    "* **BERT:** F1 **0.717 ± 0.101**, latency ≈ **20 ms/sample**, size ≈ **419 MB**\n",
    "* **RoBERTa:** F1 **0.715 ± 0.119**, latency ≈ **17 ms/sample**, size ≈ **480 MB**\n",
    "* **DistilRoBERTa:** F1 **0.713 ± 0.098**, latency ≈ **12 ms/sample**, size ≈ **318 MB**\n",
    "\n",
    "**Pareto note:** **DistilRoBERTa** gives near-BERT performance at the **lowest latency**. If you need speed, it’s the easiest win.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4 Calibration & reliability (ECE/Brier)\n",
    "\n",
    "* Pre-calibration ECE is moderate (e.g., **News / RoBERTa** ≈ **0.474**).\n",
    "* **Temperature scaling** reduces ECE/Brier slightly (e.g., ECE **0.474 → 0.469**, Brier **0.095 → 0.088** for News/RoBERTa).\n",
    "* **Recommendation:** keep temperature scaling in the inference stack if you threshold or abstain on confidence.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5 Error patterns by domain (qualitative)\n",
    "\n",
    "* **Reddit:** Missed cases trend toward **deadpan/implicit sarcasm** or multi-turn context; world knowledge helps.\n",
    "* **Twitter:** False positives often triggered by **exclamation/hyperbole** that isn’t actually sarcastic.\n",
    "* **News:** Fewer ambiguous cues; models largely agree (high precision, stable recall).\n",
    "\n",
    "---\n",
    "\n",
    "## 5.6 Statistical comparisons\n",
    "\n",
    "* Within each **domain × seed**, I ran **McNemar** and **permutation tests** on paired predictions.\n",
    "* No single model **dominates across all domains**; the small observed gaps are **domain-specific** and sometimes **not statistically decisive** after pairing.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.7 Practical picks\n",
    "\n",
    "* **Single model for all domains:**\n",
    "\n",
    "  * **BERT** for balance, or **DistilRoBERTa** if **latency/size** matter.\n",
    "* **Routing by domain:**\n",
    "\n",
    "  * **News → RoBERTa** (highest F1, best AUPRC).\n",
    "  * **Reddit → DistilRoBERTa** (slight F1 edge + fastest).\n",
    "  * **Twitter → BERT** (best mean F1).\n",
    "* Keep **temperature scaling** on; track **ECE/Brier** in production.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.8 One-liner you can drop into Results\n",
    "\n",
    "> With per-domain 50/50 balancing and equalized size, transformer performance clusters tightly on average (F1 ≈ **0.71–0.72**) but **domain effects dominate**: **RoBERTa** excels on **News** (F1≈**0.87**), **DistilRoBERTa** leads **Reddit** (F1≈**0.65**) with the best latency (\\~**12 ms/sample**), and **BERT** tops **Twitter** (F1≈**0.66**); temperature scaling modestly improves calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cf206442-39f9-4a9f-b777-d86688fece42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain counts before balancing:\n",
      "source\n",
      "Reddit     919475\n",
      "News        26597\n",
      "Twitter      4622\n",
      "Name: count, dtype: int64\n",
      "\n",
      "===== Balanced variant seed=0 (new 50/50 draws per domain) =====\n",
      "  Twitter: balanced 50/50 → 2176 rows (per class 1088)\n",
      "  Reddit: balanced 50/50 → 898548 rows (per class 449274)\n",
      "  News: balanced 50/50 → 23294 rows (per class 11647)\n",
      "  Equalized cap across domains (seed 0) → total N per domain = 2176 (each class 1088)\n",
      "\n",
      "  -- Domain Twitter | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.660\n",
      "      epoch 2/3 | val_f1=0.634\n",
      "      epoch 3/3 | val_f1=0.642\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.694\n",
      "      epoch 2/3 | val_f1=0.584\n",
      "      epoch 3/3 | val_f1=0.662\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.670\n",
      "      epoch 2/3 | val_f1=0.602\n",
      "      epoch 3/3 | val_f1=0.615\n",
      "\n",
      "  -- Domain Reddit | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.522\n",
      "      epoch 2/3 | val_f1=0.660\n",
      "      epoch 3/3 | val_f1=0.635\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.669\n",
      "      epoch 2/3 | val_f1=0.674\n",
      "      epoch 3/3 | val_f1=0.677\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.674\n",
      "      epoch 2/3 | val_f1=0.689\n",
      "      epoch 3/3 | val_f1=0.664\n",
      "\n",
      "  -- Domain News | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.857\n",
      "      epoch 2/3 | val_f1=0.866\n",
      "      epoch 3/3 | val_f1=0.874\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.856\n",
      "      epoch 2/3 | val_f1=0.869\n",
      "      epoch 3/3 | val_f1=0.874\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.812\n",
      "      epoch 2/3 | val_f1=0.859\n",
      "      epoch 3/3 | val_f1=0.858\n",
      "\n",
      "===== Balanced variant seed=1 (new 50/50 draws per domain) =====\n",
      "  Twitter: balanced 50/50 → 2176 rows (per class 1088)\n",
      "  Reddit: balanced 50/50 → 898548 rows (per class 449274)\n",
      "  News: balanced 50/50 → 23294 rows (per class 11647)\n",
      "  Equalized cap across domains (seed 1) → total N per domain = 2176 (each class 1088)\n",
      "\n",
      "  -- Domain Twitter | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.651\n",
      "      epoch 2/3 | val_f1=0.619\n",
      "      epoch 3/3 | val_f1=0.634\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.670\n",
      "      epoch 2/3 | val_f1=0.712\n",
      "      epoch 3/3 | val_f1=0.685\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.624\n",
      "      epoch 2/3 | val_f1=0.684\n",
      "      epoch 3/3 | val_f1=0.671\n",
      "\n",
      "  -- Domain Reddit | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.633\n",
      "      epoch 2/3 | val_f1=0.632\n",
      "      epoch 3/3 | val_f1=0.634\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.666\n",
      "      epoch 2/3 | val_f1=0.614\n",
      "      epoch 3/3 | val_f1=0.642\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.684\n",
      "      epoch 2/3 | val_f1=0.626\n",
      "      epoch 3/3 | val_f1=0.641\n",
      "\n",
      "  -- Domain News | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.824\n",
      "      epoch 2/3 | val_f1=0.792\n",
      "      epoch 3/3 | val_f1=0.847\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.853\n",
      "      epoch 2/3 | val_f1=0.810\n",
      "      epoch 3/3 | val_f1=0.836\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.839\n",
      "      epoch 2/3 | val_f1=0.839\n",
      "      epoch 3/3 | val_f1=0.838\n",
      "\n",
      "===== Balanced variant seed=2 (new 50/50 draws per domain) =====\n",
      "  Twitter: balanced 50/50 → 2176 rows (per class 1088)\n",
      "  Reddit: balanced 50/50 → 898548 rows (per class 449274)\n",
      "  News: balanced 50/50 → 23294 rows (per class 11647)\n",
      "  Equalized cap across domains (seed 2) → total N per domain = 2176 (each class 1088)\n",
      "\n",
      "  -- Domain Twitter | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.621\n",
      "      epoch 2/3 | val_f1=0.612\n",
      "      epoch 3/3 | val_f1=0.658\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.597\n",
      "      epoch 2/3 | val_f1=0.538\n",
      "      epoch 3/3 | val_f1=0.635\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.018\n",
      "      epoch 2/3 | val_f1=0.641\n",
      "      epoch 3/3 | val_f1=0.627\n",
      "\n",
      "  -- Domain Reddit | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.506\n",
      "      epoch 2/3 | val_f1=0.624\n",
      "      epoch 3/3 | val_f1=0.649\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.648\n",
      "      epoch 2/3 | val_f1=0.647\n",
      "      epoch 3/3 | val_f1=0.637\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.628\n",
      "      epoch 2/3 | val_f1=0.658\n",
      "      epoch 3/3 | val_f1=0.629\n",
      "\n",
      "  -- Domain News | sizes: train=1523 val=435 test=218\n",
      "    > Training BERT ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.856\n",
      "      epoch 2/3 | val_f1=0.858\n",
      "      epoch 3/3 | val_f1=0.867\n",
      "    > Training RoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.822\n",
      "      epoch 2/3 | val_f1=0.840\n",
      "      epoch 3/3 | val_f1=0.852\n",
      "    > Training DistilRoBERTa ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      epoch 1/3 | val_f1=0.814\n",
      "      epoch 2/3 | val_f1=0.851\n",
      "      epoch 3/3 | val_f1=0.850\n",
      "\n",
      "=== Per-domain summary (mean ± std across 3 balanced equalized variants) ===\n",
      " domain         model  f1_mean  f1_std  acc_mean  acc_std  prec_mean  prec_std  rec_mean  rec_std  auprc_mean  auprc_std  lat_ms_mean  lat_ms_std  size_mb_mean  size_mb_std  ece_mean  ece_std  brier_mean  brier_std  ece_post_mean  ece_post_std  brier_post_mean  brier_post_std  mis_total  mis_neg  mis_excl  mis_hyp  mis_over  runs\n",
      "   News       RoBERTa    0.868   0.008     0.876    0.008      0.930     0.015     0.813    0.011       0.966      0.009       15.965       0.568       480.262        0.001     0.474    0.025       0.095      0.010          0.469         0.022            0.088           0.009         81       11         0        1         1     3\n",
      "   News          BERT    0.846   0.027     0.852    0.025      0.879     0.051     0.820    0.060       0.923      0.047       17.390       2.028       418.728        0.004     0.454    0.013       0.113      0.023          0.422         0.028            0.107           0.022         97       10         0        3         1     3\n",
      "   News DistilRoBERTa    0.838   0.035     0.847    0.031      0.887     0.029     0.795    0.047       0.940      0.024       14.793       9.101       318.026        0.002     0.434    0.029       0.113      0.022          0.415         0.037            0.107           0.019        100       13         0        2         2     3\n",
      " Reddit DistilRoBERTa    0.652   0.041     0.654    0.042      0.657     0.047     0.648    0.045       0.726      0.062       12.145       2.346       318.028        0.001     0.258    0.019       0.228      0.033          0.187         0.027            0.216           0.021        226       45        13        8         4     3\n",
      " Reddit          BERT    0.646   0.041     0.648    0.037      0.654     0.048     0.645    0.082       0.725      0.055       25.393       9.397       418.730        0.003     0.257    0.029       0.227      0.024          0.193         0.018            0.217           0.018        230       39        13       12         4     3\n",
      " Reddit       RoBERTa    0.633   0.051     0.631    0.062      0.636     0.077     0.633    0.049       0.740      0.062       18.354       3.163       480.266        0.001     0.282    0.026       0.236      0.047          0.198         0.036            0.216           0.026        241       49        14       12         3     3\n",
      "Twitter          BERT    0.658   0.025     0.636    0.031      0.621     0.030     0.700    0.023       0.746      0.025       17.433       3.026       418.740        0.001     0.260    0.041       0.232      0.018          0.182         0.024            0.219           0.009        238       60        24       25         4     3\n",
      "Twitter DistilRoBERTa    0.647   0.010     0.651    0.005      0.655     0.008     0.639    0.023       0.749      0.020        9.285       0.684       318.039        0.001     0.267    0.014       0.222      0.011          0.203         0.004            0.210           0.007        228       54        28       24         3     3\n",
      "Twitter       RoBERTa    0.643   0.038     0.662    0.037      0.682     0.045     0.609    0.037       0.786      0.032       16.554       1.457       480.277        0.000     0.303    0.029       0.216      0.026          0.222         0.021            0.200           0.016        221       50        32       22         2     3\n",
      "\n",
      "=== Overall (grand mean over domains × equalized variants) ===\n",
      "        model  f1_mean  f1_std  acc_mean  acc_std  prec_mean  prec_std  rec_mean  rec_std  auprc_mean  auprc_std  lat_ms_mean  lat_ms_std  size_mb_mean  size_mb_std  ece_mean  ece_std  brier_mean  brier_std  ece_post_mean  ece_post_std  brier_post_mean  brier_post_std  mis_total  mis_neg  mis_excl  mis_hyp  mis_over  runs\n",
      "         BERT    0.717   0.101     0.712    0.108      0.718     0.127     0.722    0.093       0.798      0.102       20.072       6.428       418.732        0.006     0.324    0.101       0.191      0.061          0.266         0.119            0.181           0.057        565      109        37       40         9     9\n",
      "      RoBERTa    0.715   0.119     0.723    0.121      0.749     0.144     0.685    0.102       0.830      0.109       16.958       2.067       480.268        0.007     0.353    0.094       0.183      0.071          0.296         0.132            0.168           0.062        543      110        46       35         6     9\n",
      "DistilRoBERTa    0.713   0.098     0.718    0.101      0.733     0.119     0.694    0.083       0.805      0.107       12.074       5.281       318.031        0.006     0.320    0.088       0.187      0.060          0.268         0.112            0.178           0.055        554      112        41       34         9     9\n",
      "\n",
      "=== Variance snapshot (F1 SD) ===\n",
      "        model  within_domain_sd_over_seeds  across_domain_sd\n",
      "         BERT                       0.0311            0.1123\n",
      "      RoBERTa                       0.0326            0.1328\n",
      "DistilRoBERTa                       0.0285            0.1090\n",
      "\n",
      "All artifacts saved under: sarcasm_balanced_runs_v3_equal/run_20250826_200935\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, random, re, math, datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, brier_score_loss, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup, set_seed as hf_set_seed\n",
    ")\n",
    "\n",
    "# Optional (for p-values in McNemar)\n",
    "try:\n",
    "    from scipy import stats\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "# -------------------------------\n",
    "# Google Drive (Colab) — persistence\n",
    "# -------------------------------\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "DATA_CSV      = \"df_filtered.csv\"  # update if needed\n",
    "TEXT_COL      = \"text\"\n",
    "LABEL_COL     = \"label\"   # 0 = non-sarc, 1 = sarc\n",
    "SOURCE_COL    = \"source\"  # expects values like Twitter / Reddit / News\n",
    "\n",
    "DOMAINS       = [\"Twitter\", \"Reddit\", \"News\"]\n",
    "SEEDS_BAL     = [0, 1, 2]   # three balanced variants per domain\n",
    "RNG_BASE      = 42\n",
    "\n",
    "MODELS = {\n",
    "    \"BERT\": \"bert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\",\n",
    "    \"DistilRoBERTa\": \"distilroberta-base\",\n",
    "}\n",
    "\n",
    "MAX_EPOCHS    = 3\n",
    "MAX_LEN       = 128\n",
    "BATCH_TRAIN   = 16\n",
    "BATCH_EVAL    = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO  = 0.10\n",
    "LAT_REPS      = 50\n",
    "BOOT_ITERS    = 500\n",
    "PERM_ITERS    = 1000\n",
    "N_CALIB_BINS  = 15\n",
    "\n",
    "RUN_TAG       = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_BASE      = Path(\"/content/drive/MyDrive/sarcasm_balanced_runs_v3_equal\") if IN_COLAB else Path(\"./sarcasm_balanced_runs_v3_equal\")\n",
    "OUT_DIR       = OUT_BASE / f\"run_{RUN_TAG}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                      else \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "                      else \"cpu\")\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    hf_set_seed(seed)\n",
    "\n",
    "def make_loader(tokenizer, texts, labels, batch_size=32, shuffle=False, max_len=128):\n",
    "    toks = tokenizer(list(texts), truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    ds = TensorDataset(toks[\"input_ids\"], toks[\"attention_mask\"], torch.tensor(list(labels), dtype=torch.long))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_split(model, dataloader):\n",
    "    model.eval()\n",
    "    ys, preds, probs, logits_all = [], [], [], []\n",
    "    for input_ids, attn, labels in dataloader:\n",
    "        input_ids, attn = input_ids.to(device), attn.to(device)\n",
    "        out = model(input_ids, attention_mask=attn)\n",
    "        logits = out.logits\n",
    "        logits_all.append(logits.cpu())\n",
    "        pr = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        probs.extend(pr.tolist())\n",
    "        preds.extend(pr.argmax(1))\n",
    "        ys.extend(labels.numpy())\n",
    "    logits_all = torch.cat(logits_all, dim=0).numpy()\n",
    "    ys, preds, probs = np.array(ys), np.array(preds), np.array(probs)\n",
    "    return ys, preds, probs, logits_all\n",
    "\n",
    "def basic_metrics(y, p):\n",
    "    return {\n",
    "        \"acc\": accuracy_score(y,p),\n",
    "        \"prec\": precision_score(y,p, zero_division=0),\n",
    "        \"rec\": recall_score(y,p, zero_division=0),\n",
    "        \"f1\": f1_score(y,p, zero_division=0),\n",
    "        \"f1_macro\": f1_score(y,p, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "def measure_latency_ms(model, tokenizer, sample_text, reps=50):\n",
    "    sample = tokenizer([sample_text], truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    sample = {k:v.to(device) for k,v in sample.items()}\n",
    "    model.eval()\n",
    "    with torch.no_grad(): _ = model(**sample)\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(reps):\n",
    "            _ = model(**sample)\n",
    "    t1 = time.time()\n",
    "    return (t1 - t0) / reps * 1000.0\n",
    "\n",
    "def save_dir_size_mb(path: Path) -> float:\n",
    "    total = 0\n",
    "    for p in path.rglob(\"*\"):\n",
    "        if p.is_file(): total += p.stat().st_size\n",
    "    return total / (1024*1024)\n",
    "\n",
    "def ece_score(y_true, p_pos, n_bins=15):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    p_pos  = np.asarray(p_pos).astype(float)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    ece = 0.0\n",
    "    bin_stats = []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        mask = (p_pos > lo) & (p_pos <= hi) if i>0 else (p_pos >= lo) & (p_pos <= hi)\n",
    "        if not np.any(mask):\n",
    "            bin_stats.append({\"bin_lo\": lo, \"bin_hi\": hi, \"n\": 0, \"conf\": None, \"acc\": None})\n",
    "            continue\n",
    "        conf = p_pos[mask].mean()\n",
    "        acc  = (y_true[mask] == (p_pos[mask] >= 0.5)).mean()\n",
    "        w    = mask.mean()\n",
    "        ece += w * abs(acc - conf)\n",
    "        bin_stats.append({\"bin_lo\": lo, \"bin_hi\": hi, \"n\": int(mask.sum()), \"conf\": float(conf), \"acc\": float(acc)})\n",
    "    return float(ece), bin_stats\n",
    "\n",
    "def reliability_plot(y_true, p_pos, out_png, n_bins=15, title=\"Reliability\"):\n",
    "    ece, bins = ece_score(y_true, p_pos, n_bins=n_bins)\n",
    "    xs = [(b[\"bin_lo\"]+b[\"bin_hi\"])/2 for b in bins if b[\"n\"]>0]\n",
    "    ys = [b[\"acc\"] for b in bins if b[\"n\"]>0]\n",
    "    cs = [b[\"conf\"] for b in bins if b[\"n\"]>0]\n",
    "    plt.figure(figsize=(4.2,4.2))\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.scatter(cs, ys)\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Empirical accuracy\")\n",
    "    plt.title(f\"{title} (ECE={ece:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "    return ece, bins\n",
    "\n",
    "def pr_curve_plot(y_true, p_pos, out_png, title=\"PR Curve\"):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p_pos)\n",
    "    auprc = average_precision_score(y_true, p_pos)\n",
    "    plt.figure(figsize=(4.6,4.0))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{title} (AUPRC={auprc:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=160)\n",
    "    plt.close()\n",
    "    return float(auprc)\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, p_pos, iters=500, alpha=0.05, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    n = len(y_true)\n",
    "    def metrics_once(idx):\n",
    "        yt = y_true[idx]; yp = y_pred[idx]; pp = p_pos[idx]\n",
    "        return (\n",
    "            f1_score(yt, yp, zero_division=0),\n",
    "            accuracy_score(yt, yp),\n",
    "            precision_score(yt, yp, zero_division=0),\n",
    "            recall_score(yt, yp, zero_division=0),\n",
    "            average_precision_score(yt, pp)\n",
    "        )\n",
    "    f1s, accs, precs, recs, auprcs = [], [], [], [], []\n",
    "    for _ in range(iters):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        f1, acc, prc, rec, au = metrics_once(idx)\n",
    "        f1s.append(f1); accs.append(acc); precs.append(prc); recs.append(rec); auprcs.append(au)\n",
    "    def ci(arr):\n",
    "        arr = np.sort(arr)\n",
    "        lo = int((alpha/2)*iters)\n",
    "        hi = int((1 - alpha/2)*iters) - 1\n",
    "        return float(arr[lo]), float(arr[hi])\n",
    "    return {\n",
    "        \"f1_ci\": ci(f1s),\n",
    "        \"acc_ci\": ci(accs),\n",
    "        \"prec_ci\": ci(precs),\n",
    "        \"rec_ci\": ci(recs),\n",
    "        \"auprc_ci\": ci(auprcs)\n",
    "    }\n",
    "\n",
    "def mcnemar_test(y_true, pred_a, pred_b):\n",
    "    a_correct = (pred_a == y_true)\n",
    "    b_correct = (pred_b == y_true)\n",
    "    b_err = np.logical_and(a_correct, ~b_correct).sum()\n",
    "    c_err = np.logical_and(~a_correct, b_correct).sum()\n",
    "    stat = (abs(b_err - c_err) - 1)**2 / (b_err + c_err + 1e-12)\n",
    "    if HAVE_SCIPY:\n",
    "        p = stats.chi2.sf(stat, 1)\n",
    "    else:\n",
    "        p = float(\"nan\")\n",
    "    return int(b_err), int(c_err), float(stat), float(p)\n",
    "\n",
    "def permutation_test_delta_f1(y_true, pred_a, pred_b, iters=1000, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    def f1(p): return f1_score(y_true, p, zero_division=0)\n",
    "    f1_a = f1(pred_a); f1_b = f1(pred_b)\n",
    "    observed = f1_a - f1_b\n",
    "    n = len(y_true)\n",
    "    count = 0\n",
    "    for _ in range(iters):\n",
    "        swap = rng.random(n) < 0.5\n",
    "        pa = pred_a.copy(); pb = pred_b.copy()\n",
    "        pa[swap], pb[swap] = pb[swap], pa[swap]\n",
    "        if abs(f1(pa) - f1(pb)) >= abs(observed) - 1e-12:\n",
    "            count += 1\n",
    "    p = (count + 1) / (iters + 1)\n",
    "    return float(observed), float(p)\n",
    "\n",
    "# Heuristic flags\n",
    "_NEGATION_RE   = re.compile(r\"\\b(no|not|never|n't|cannot|can't|won't|don'?t)\\b\", re.IGNORECASE)\n",
    "_EXCLAM_RE     = re.compile(r\"!{1,}\")\n",
    "_HYPERBOLE_RE  = re.compile(r\"\\b(always|never|literally|absolutely|everyone|no one|best|worst|totally|completely)\\b\", re.IGNORECASE)\n",
    "_OVERCONF_RE   = re.compile(r\"\\b(of course|obviously|clearly|as everyone knows|without a doubt)\\b\", re.IGNORECASE)\n",
    "def pattern_flags(text):\n",
    "    return {\n",
    "        \"negation\": bool(_NEGATION_RE.search(text)),\n",
    "        \"exclamation\": bool(_EXCLAM_RE.search(text)),\n",
    "        \"hyperbole\": bool(_HYPERBOLE_RE.search(text)),\n",
    "        \"overconfidence\": bool(_OVERCONF_RE.search(text)),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Balancing + Equalization\n",
    "# -------------------------------\n",
    "def downsample_balance(df_src: pd.DataFrame, label_col: str, seed: int) -> pd.DataFrame:\n",
    "    \"\"\"Return a 50/50 balanced dataframe by downsampling the majority class with given random_state.\"\"\"\n",
    "    df0 = df_src[df_src[label_col] == 0]\n",
    "    df1 = df_src[df_src[label_col] == 1]\n",
    "    if len(df0) == 0 or len(df1) == 0:\n",
    "        raise ValueError(\"One of the classes is empty in this source. Cannot balance.\")\n",
    "    minority = min(len(df0), len(df1))\n",
    "    if len(df0) > len(df1):\n",
    "        df0_bal = df0.sample(n=minority, random_state=seed, replace=False)\n",
    "        df1_bal = df1\n",
    "    else:\n",
    "        df1_bal = df1.sample(n=minority, random_state=seed, replace=False)\n",
    "        df0_bal = df0\n",
    "    df_bal = pd.concat([df0_bal, df1_bal]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return df_bal  # exactly 2*minority rows (50/50)\n",
    "\n",
    "def equalize_domains_by_cap(balanced_by_domain: dict, seed: int):\n",
    "    \"\"\"\n",
    "    Given {domain: balanced_df}, cap each domain to the SAME total N.\n",
    "    cap = min(total over domains) so no domain is forced to upsample.\n",
    "    Maintain 50/50 by downsampling EACH CLASS to cap/2 with random_state=seed.\n",
    "    \"\"\"\n",
    "    totals = {d: len(df) for d, df in balanced_by_domain.items()}\n",
    "    cap = min(totals.values())\n",
    "    if cap % 2 == 1:  # ensure even cap for 50/50\n",
    "        cap -= 1\n",
    "    out = {}\n",
    "    for d, df in balanced_by_domain.items():\n",
    "        per_class = cap // 2\n",
    "        df0 = df[df[LABEL_COL] == 0].sample(n=per_class, random_state=seed, replace=False)\n",
    "        df1 = df[df[LABEL_COL] == 1].sample(n=per_class, random_state=seed, replace=False)\n",
    "        out[d] = pd.concat([df0, df1]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return out, cap\n",
    "\n",
    "def split_70_20_10(df_bal: pd.DataFrame, seed: int):\n",
    "    X = df_bal[TEXT_COL].astype(str).tolist()\n",
    "    y = df_bal[LABEL_COL].astype(int).tolist()\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1/3), stratify=y_temp, random_state=seed)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# -------- Temperature scaling --------\n",
    "class _TempScale(torch.nn.Module):\n",
    "    def __init__(self, init_T=1.0):\n",
    "        super().__init__()\n",
    "        self.log_T = torch.nn.Parameter(torch.tensor([math.log(init_T)], dtype=torch.float32))\n",
    "    def forward(self, logits):\n",
    "        T = torch.exp(self.log_T)\n",
    "        return logits / T\n",
    "\n",
    "def fit_temperature(logits_val, y_val, max_iter=200, lr=0.01):\n",
    "    ts = _TempScale(1.0)\n",
    "    opt = torch.optim.LBFGS(ts.parameters(), lr=0.5, max_iter=100) if hasattr(torch.optim, \"LBFGS\") else torch.optim.Adam(ts.parameters(), lr=lr)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "    logits_val = torch.tensor(logits_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    def _closure():\n",
    "        opt.zero_grad()\n",
    "        loss = ce(ts(logits_val), y_val)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    if isinstance(opt, torch.optim.LBFGS):\n",
    "        opt.step(_closure)\n",
    "    else:\n",
    "        for _ in range(max_iter):\n",
    "            loss = _closure()\n",
    "            opt.step()\n",
    "    with torch.no_grad():\n",
    "        T = math.exp(ts.log_T.item())\n",
    "    return float(T)\n",
    "\n",
    "# -------------------------------\n",
    "# Training + full evaluation\n",
    "# -------------------------------\n",
    "def train_eval_model(pretrained_name: str, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                     seed: int, save_dir: Path, title_prefix: str):\n",
    "    set_all_seeds(seed)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_name, num_labels=2).to(device)\n",
    "\n",
    "    train_loader = make_loader(tokenizer, X_train, y_train, batch_size=BATCH_TRAIN, shuffle=True,  max_len=MAX_LEN)\n",
    "    val_loader   = make_loader(tokenizer, X_val,   y_val,   batch_size=BATCH_EVAL,  shuffle=False, max_len=MAX_LEN)\n",
    "    test_loader  = make_loader(tokenizer, X_test,  y_test,  batch_size=BATCH_EVAL,  shuffle=False, max_len=MAX_LEN)\n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = MAX_EPOCHS * len(train_loader)\n",
    "    warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    for ep in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        for input_ids, attn, labels in train_loader:\n",
    "            input_ids, attn, labels = input_ids.to(device), attn.to(device), labels.to(device)\n",
    "            optim.zero_grad()\n",
    "            out = model(input_ids, attention_mask=attn, labels=labels)\n",
    "            out.loss.backward()\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "        yv, pv, qv, _ = eval_split(model, val_loader)\n",
    "        print(f\"      epoch {ep+1}/{MAX_EPOCHS} | val_f1={f1_score(yv, pv, zero_division=0):.3f}\")\n",
    "\n",
    "    # Eval\n",
    "    yt, pt, qt, lt = eval_split(model, test_loader)\n",
    "    yv, pv, qv, lv = eval_split(model, val_loader)\n",
    "\n",
    "    p_pos_test = qt[:,1]\n",
    "    m = basic_metrics(yt, pt)\n",
    "    lat_ms = measure_latency_ms(model, tokenizer, X_test[0] if len(X_test) else \"ok\", reps=LAT_REPS)\n",
    "    ece, _ = ece_score(yt, p_pos_test, n_bins=N_CALIB_BINS)\n",
    "    brier = brier_score_loss(yt, p_pos_test)\n",
    "    auprc = average_precision_score(yt, p_pos_test)\n",
    "\n",
    "    # Temperature scaling\n",
    "    T = fit_temperature(lv, yv)\n",
    "    p_pos_test_cal = torch.softmax(torch.tensor(lt) / T, dim=1).numpy()[:,1]\n",
    "    ece_cal, _ = ece_score(yt, p_pos_test_cal, n_bins=N_CALIB_BINS)\n",
    "    brier_cal = brier_score_loss(yt, p_pos_test_cal)\n",
    "\n",
    "    # Plots\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pr_curve_plot(yt, p_pos_test, save_dir/\"pr_curve.png\", title=f\"{title_prefix} PR\")\n",
    "    reliability_plot(yt, p_pos_test, save_dir/\"reliability_pre.png\", n_bins=N_CALIB_BINS, title=f\"{title_prefix} Reliability (pre)\")\n",
    "    reliability_plot(yt, p_pos_test_cal, save_dir/\"reliability_post_temp.png\", n_bins=N_CALIB_BINS, title=f\"{title_prefix} Reliability (post temp)\")\n",
    "\n",
    "    # Bootstrap CIs\n",
    "    ci = bootstrap_metric_ci(yt, pt, p_pos_test, iters=BOOT_ITERS, rng=seed)\n",
    "\n",
    "    # Save artifacts\n",
    "    pd.DataFrame({\"text\": X_test, \"y\": yt, \"pred\": pt, \"p_pos\": p_pos_test}).to_csv(save_dir/\"preds_test.csv\", index=False)\n",
    "    np.savez(save_dir/\"logits_test.npz\", logits=lt, y=yt)\n",
    "    with open(save_dir/\"report_test.txt\",\"w\") as f:\n",
    "        f.write(classification_report(yt, pt, target_names=[\"Not Sarcastic\",\"Sarcastic\"], digits=3))\n",
    "\n",
    "    # Misclassification analysis\n",
    "    df_err = pd.DataFrame({\"text\": X_test, \"y\": yt, \"pred\": pt, \"p_pos\": p_pos_test})\n",
    "    df_err[\"is_error\"] = df_err[\"y\"] != df_err[\"pred\"]\n",
    "    for k in [\"negation\",\"exclamation\",\"hyperbole\",\"overconfidence\"]:\n",
    "        df_err[k] = df_err[\"text\"].apply(lambda t, kk=k: pattern_flags(t)[kk])\n",
    "    mis = {\n",
    "        \"total_errors\": int(df_err[\"is_error\"].sum()),\n",
    "        \"negation_errors\": int(((df_err[\"is_error\"]) & (df_err[\"negation\"])).sum()),\n",
    "        \"exclamation_errors\": int(((df_err[\"is_error\"]) & (df_err[\"exclamation\"])).sum()),\n",
    "        \"hyperbole_errors\": int(((df_err[\"is_error\"]) & (df_err[\"hyperbole\"])).sum()),\n",
    "        \"overconfidence_errors\": int(((df_err[\"is_error\"]) & (df_err[\"overconfidence\"])).sum()),\n",
    "    }\n",
    "    df_err.to_csv(save_dir/\"errors_tagged.csv\", index=False)\n",
    "\n",
    "    # Save model + tokenizer\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    size_mb = save_dir_size_mb(save_dir)\n",
    "\n",
    "    metrics_json = {\n",
    "        \"f1\": float(m[\"f1\"]), \"acc\": float(m[\"acc\"]), \"prec\": float(m[\"prec\"]), \"rec\": float(m[\"rec\"]),\n",
    "        \"auprc\": float(auprc), \"latency_ms\": float(lat_ms), \"size_mb\": float(size_mb),\n",
    "        \"ece\": float(ece), \"brier\": float(brier),\n",
    "        \"ece_post_temp\": float(ece_cal), \"brier_post_temp\": float(brier_cal),\n",
    "        \"temp_scaling_T\": float(T),\n",
    "        \"bootstrap_ci\": ci,\n",
    "        \"mis\": mis\n",
    "    }\n",
    "    with open(save_dir/\"metrics.json\",\"w\") as f:\n",
    "        json.dump(metrics_json, f, indent=2)\n",
    "\n",
    "    return {\"y_test\": yt, \"pred_test\": pt, \"p_pos_test\": p_pos_test, \"metrics\": metrics_json}\n",
    "\n",
    "# -------------------------------\n",
    "# Load data & normalize source labels (robust: underscores/hyphens/spaces)\n",
    "# -------------------------------\n",
    "df_full = pd.read_csv(DATA_CSV)\n",
    "assert {TEXT_COL, LABEL_COL, SOURCE_COL}.issubset(df_full.columns), \\\n",
    "    f\"CSV must have columns: {TEXT_COL}, {LABEL_COL}, {SOURCE_COL}\"\n",
    "\n",
    "df_full[SOURCE_COL] = (\n",
    "    df_full[SOURCE_COL]\n",
    "      .astype(str).str.strip().str.lower()\n",
    "      .str.replace(r\"[\\s\\-]+\", \"_\", regex=True)  # \"news headline\" → \"news_headline\"\n",
    ")\n",
    "\n",
    "SOURCE_MAP = {\n",
    "    \"twitter\": \"Twitter\", \"tw\": \"Twitter\",\n",
    "    \"reddit\": \"Reddit\",\n",
    "    \"news\": \"News\",\n",
    "    \"headline\": \"News\",\n",
    "    \"news_headline\": \"News\",      # ✅ covers singular\n",
    "    \"news_headlines\": \"News\",     # ✅ covers plural\n",
    "    \"newsheadline\": \"News\",\n",
    "    \"newsheadlines\": \"News\",\n",
    "}\n",
    "df_full[SOURCE_COL] = df_full[SOURCE_COL].map(SOURCE_MAP).fillna(df_full[SOURCE_COL].str.title())\n",
    "\n",
    "print(\"Domain counts before balancing:\")\n",
    "print(df_full[SOURCE_COL].value_counts())\n",
    "\n",
    "with open(OUT_DIR/\"_run_info.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"domains\": DOMAINS,\n",
    "        \"seeds_bal\": SEEDS_BAL,\n",
    "        \"models\": MODELS,\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"max_len\": MAX_LEN,\n",
    "        \"batch_train\": BATCH_TRAIN,\n",
    "        \"batch_eval\": BATCH_EVAL,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"bootstrap_iters\": BOOT_ITERS,\n",
    "        \"perm_iters\": PERM_ITERS,\n",
    "        \"n_calib_bins\": N_CALIB_BINS,\n",
    "        \"equalize_strategy\": \"cap = min(balanced_totals across domains) per seed; per-class downsample to cap/2\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Main loop: equalized size per seed\n",
    "# -------------------------------\n",
    "rows_all, pairwise_rows = [], []\n",
    "per_variant_preds = {}\n",
    "\n",
    "for bal_seed in SEEDS_BAL:\n",
    "    print(f\"\\n===== Balanced variant seed={bal_seed} (new 50/50 draws per domain) =====\")\n",
    "\n",
    "    # 1) Balance EACH domain 50/50 with its own random draw (non-sarc subset changes per seed)\n",
    "    balanced_raw = {}\n",
    "    for domain in DOMAINS:\n",
    "        df_domain = df_full[df_full[SOURCE_COL] == domain].copy()\n",
    "        if df_domain.empty:\n",
    "            print(f\"[warn] No rows for domain '{domain}'. Skipping this domain for seed {bal_seed}.\")\n",
    "            continue\n",
    "        df_bal = downsample_balance(df_domain, LABEL_COL, seed=bal_seed)\n",
    "        balanced_raw[domain] = df_bal\n",
    "        print(f\"  {domain}: balanced 50/50 → {len(df_bal)} rows (per class {len(df_bal)//2})\")\n",
    "\n",
    "    # 2) Equalize total N across domains for THIS seed\n",
    "    balanced_equal, cap_total = equalize_domains_by_cap(balanced_raw, seed=RNG_BASE + bal_seed)\n",
    "    print(f\"  Equalized cap across domains (seed {bal_seed}) → total N per domain = {cap_total} (each class {cap_total//2})\")\n",
    "\n",
    "    # 3) For each domain, split 70/20/10 and train all models\n",
    "    seed_dir = OUT_DIR / f\"bal_seed_{bal_seed}\"\n",
    "    seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save equalized datasets used this seed\n",
    "    for domain, df_eq in balanced_equal.items():\n",
    "        df_eq.to_csv(seed_dir / f\"{domain}_equalized_50_50.csv\", index=False)\n",
    "\n",
    "    for domain, df_eq in balanced_equal.items():\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = split_70_20_10(df_eq, seed=RNG_BASE + bal_seed)\n",
    "        print(f\"\\n  -- Domain {domain} | sizes: train={len(X_train)} val={len(X_val)} test={len(X_test)}\")\n",
    "\n",
    "        dom_dir = seed_dir / f\"domain_{domain}\"\n",
    "        dom_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pd.DataFrame({\n",
    "            \"split\": [\"train\"]*len(X_train) + [\"val\"]*len(X_val) + [\"test\"]*len(X_test),\n",
    "            \"text\": X_train + X_val + X_test,\n",
    "            \"label\": y_train + y_val + y_test\n",
    "        }).to_csv(dom_dir/\"data_split.csv\", index=False)\n",
    "\n",
    "        # Train models\n",
    "        for pretty_name, pretrained in MODELS.items():\n",
    "            print(f\"    > Training {pretty_name} ...\")\n",
    "            model_dir = dom_dir / pretty_name\n",
    "            res = train_eval_model(\n",
    "                pretrained, X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                seed=RNG_BASE + bal_seed, save_dir=model_dir, title_prefix=f\"{domain}-{pretty_name}\"\n",
    "            )\n",
    "            met = res[\"metrics\"]\n",
    "            rows_all.append({\n",
    "                \"bal_seed\": bal_seed, \"domain\": domain, \"model\": pretty_name,\n",
    "                \"f1\": met[\"f1\"], \"acc\": met[\"acc\"], \"prec\": met[\"prec\"], \"rec\": met[\"rec\"],\n",
    "                \"auprc\": met[\"auprc\"], \"latency_ms\": met[\"latency_ms\"], \"size_mb\": met[\"size_mb\"],\n",
    "                \"ece\": met[\"ece\"], \"brier\": met[\"brier\"],\n",
    "                \"ece_post_temp\": met[\"ece_post_temp\"], \"brier_post_temp\": met[\"brier_post_temp\"],\n",
    "                \"mis_total\": met[\"mis\"][\"total_errors\"],\n",
    "                \"mis_neg\": met[\"mis\"][\"negation_errors\"],\n",
    "                \"mis_excl\": met[\"mis\"][\"exclamation_errors\"],\n",
    "                \"mis_hyp\": met[\"mis\"][\"hyperbole_errors\"],\n",
    "                \"mis_over\": met[\"mis\"][\"overconfidence_errors\"]\n",
    "            })\n",
    "            per_variant_preds[(bal_seed, domain, pretty_name)] = {\n",
    "                \"y\": np.array(res[\"y_test\"]), \"pred\": np.array(res[\"pred_test\"])\n",
    "            }\n",
    "\n",
    "        # Pairwise tests within THIS domain and seed (paired: same test set)\n",
    "        for (mA, mB) in combinations(MODELS.keys(), 2):\n",
    "            ya = per_variant_preds[(bal_seed, domain, mA)][\"y\"]\n",
    "            pa = per_variant_preds[(bal_seed, domain, mA)][\"pred\"]\n",
    "            pb = per_variant_preds[(bal_seed, domain, mB)][\"pred\"]\n",
    "            b_err, c_err, stat, p_mcn = mcnemar_test(ya, pa, pb)\n",
    "            dF1, p_perm = permutation_test_delta_f1(ya, pa, pb, iters=PERM_ITERS, rng=RNG_BASE+bal_seed)\n",
    "            acc_a = accuracy_score(ya, pa); acc_b = accuracy_score(ya, pb)\n",
    "            rd = acc_a - acc_b\n",
    "            n = len(ya); se = math.sqrt(acc_a*(1-acc_a)/n + acc_b*(1-acc_b)/n)\n",
    "            rd_lo = rd - 1.96*se; rd_hi = rd + 1.96*se\n",
    "            pairwise_rows.append({\n",
    "                \"bal_seed\": bal_seed, \"domain\": domain,\n",
    "                \"model_A\": mA, \"model_B\": mB,\n",
    "                \"acc_A\": acc_a, \"acc_B\": acc_b,\n",
    "                \"delta_acc\": rd, \"delta_acc_ci_lo\": rd_lo, \"delta_acc_ci_hi\": rd_hi,\n",
    "                \"mcnemar_b\": b_err, \"mcnemar_c\": c_err, \"mcnemar_stat\": stat, \"mcnemar_p\": p_mcn,\n",
    "                \"delta_f1\": dF1, \"perm_p_f1\": p_perm, \"n\": n\n",
    "            })\n",
    "        pd.DataFrame(pairwise_rows).to_csv(dom_dir/\"pairwise_tests.csv\", index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# Aggregate summaries + prints\n",
    "# -------------------------------\n",
    "df_all = pd.DataFrame(rows_all)\n",
    "df_all.to_csv(OUT_DIR/\"runs_detailed_all.csv\", index=False)\n",
    "\n",
    "def agg_mean_std(df, group_cols):\n",
    "    return (df.groupby(group_cols)\n",
    "            .agg(f1_mean=(\"f1\",\"mean\"), f1_std=(\"f1\",\"std\"),\n",
    "                 acc_mean=(\"acc\",\"mean\"), acc_std=(\"acc\",\"std\"),\n",
    "                 prec_mean=(\"prec\",\"mean\"), prec_std=(\"prec\",\"std\"),\n",
    "                 rec_mean=(\"rec\",\"mean\"), rec_std=(\"rec\",\"std\"),\n",
    "                 auprc_mean=(\"auprc\",\"mean\"), auprc_std=(\"auprc\",\"std\"),\n",
    "                 lat_ms_mean=(\"latency_ms\",\"mean\"), lat_ms_std=(\"latency_ms\",\"std\"),\n",
    "                 size_mb_mean=(\"size_mb\",\"mean\"), size_mb_std=(\"size_mb\",\"std\"),\n",
    "                 ece_mean=(\"ece\",\"mean\"), ece_std=(\"ece\",\"std\"),\n",
    "                 brier_mean=(\"brier\",\"mean\"), brier_std=(\"brier\",\"std\"),\n",
    "                 ece_post_mean=(\"ece_post_temp\",\"mean\"), ece_post_std=(\"ece_post_temp\",\"std\"),\n",
    "                 brier_post_mean=(\"brier_post_temp\",\"mean\"), brier_post_std=(\"brier_post_temp\",\"std\"),\n",
    "                 mis_total=(\"mis_total\",\"sum\"),\n",
    "                 mis_neg=(\"mis_neg\",\"sum\"),\n",
    "                 mis_excl=(\"mis_excl\",\"sum\"),\n",
    "                 mis_hyp=(\"mis_hyp\",\"sum\"),\n",
    "                 mis_over=(\"mis_over\",\"sum\"),\n",
    "                 runs=(\"f1\",\"count\"))\n",
    "            .reset_index())\n",
    "\n",
    "# Per-domain summary across seeds\n",
    "summary_dom = agg_mean_std(df_all, [\"domain\",\"model\"]).sort_values([\"domain\",\"f1_mean\"], ascending=[True, False])\n",
    "summary_dom.round(3).to_csv(OUT_DIR/\"summary_by_domain.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Per-domain summary (mean ± std across 3 balanced equalized variants) ===\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(summary_dom.round(3).to_string(index=False))\n",
    "\n",
    "# Overall model means\n",
    "summary_model = agg_mean_std(df_all, [\"model\"]).sort_values(\"f1_mean\", ascending=False)\n",
    "summary_model.round(3).to_csv(OUT_DIR/\"summary_overall_by_model.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Overall (grand mean over domains × equalized variants) ===\")\n",
    "print(summary_model.round(3).to_string(index=False))\n",
    "\n",
    "# Variance snapshot\n",
    "var_rows = []\n",
    "for m in MODELS.keys():\n",
    "    dfm = summary_dom[summary_dom[\"model\"] == m]\n",
    "    across_domains_sd = float(dfm[\"f1_mean\"].std(ddof=1)) if len(dfm)>1 else 0.0\n",
    "    within_domain_sd_mean = float(df_all[df_all[\"model\"]==m].groupby(\"domain\")[\"f1\"].std(ddof=1).mean())\n",
    "    var_rows.append({\"model\": m, \"within_domain_sd_over_seeds\": within_domain_sd_mean, \"across_domain_sd\": across_domains_sd})\n",
    "df_var = pd.DataFrame(var_rows)\n",
    "df_var.to_csv(OUT_DIR/\"variance_snapshot.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Variance snapshot (F1 SD) ===\")\n",
    "print(df_var.round(4).to_string(index=False))\n",
    "\n",
    "# Save pairwise tests collected\n",
    "df_pair = pd.DataFrame(pairwise_rows)\n",
    "df_pair.to_csv(OUT_DIR/\"pairwise_tests_all.csv\", index=False)\n",
    "\n",
    "# Pareto plot (F1 vs latency; bubble=size)\n",
    "plt.figure(figsize=(6.0,4.6))\n",
    "for m in MODELS.keys():\n",
    "    row = summary_model[summary_model[\"model\"]==m]\n",
    "    if row.empty: continue\n",
    "    x = row[\"lat_ms_mean\"].values[0]; y = row[\"f1_mean\"].values[0]; s = (row[\"size_mb_mean\"].values[0]+1e-6)*6\n",
    "    plt.scatter(x, y, s=s, label=m, alpha=0.8)\n",
    "plt.xlabel(\"Latency (ms / sample)\")\n",
    "plt.ylabel(\"F1 (mean over domains & variants)\")\n",
    "plt.title(\"Pareto: Performance vs Latency (bubble=size MB)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"pareto_f1_latency.png\", dpi=180)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAll artifacts saved under: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1890a77-5380-43c5-862e-22a7281e3245",
   "metadata": {},
   "source": [
    "# 6. Classic Deep-Learning Suite — Initial Evaluation & Per-Domain Follow-ups\n",
    "\n",
    "This section evaluates a family of **classic neural text models** (BiLSTM/CNN with fixed GloVe embeddings) under two views:\n",
    "\n",
    "1. a **single fixed train/val/test split** reused across **3 seeds** (sanity check + stability), and\n",
    "2. a **per-domain, balanced & equalized** setup (Twitter/Reddit/News) to probe domain difficulty without data-size confounds.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1 What I ran (at a glance)\n",
    "\n",
    "* **Models:**\n",
    "\n",
    "  * **BiLSTM + GloVe:** 100d, 300d\n",
    "  * **BiLSTM + Attention + GloVe:** 100d, 300d\n",
    "  * **TextCNN + GloVe:** 100d, 300d\n",
    "* **Tokenization/length:** one Keras `Tokenizer`, **max\\_len=40**.\n",
    "* **Training:** batch=128, up to 20 epochs, **EarlyStopping(patience=1)**, best weights restored.\n",
    "* **Metrics/artifacts:** accuracy, precision, recall, **F1**, ROC-AUC/PR-AUC, **Brier**, **ECE (10 bins)**, confusion matrices, misclassified examples, calibration bins, per-seed JSON “meta”.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.2 Headline results — Same split, 3 seeds (mean ± sd)\n",
    "\n",
    "* **Top F1 (tied):**\n",
    "\n",
    "  * **BiLSTM + GloVe 300d:** **0.707 ± 0.006**\n",
    "  * **BiLSTM + Attn + GloVe 300d:** **0.707 ± 0.011**\n",
    "* **Close contenders:**\n",
    "\n",
    "  * BiLSTM + GloVe 100d: **0.701 ± 0.003**\n",
    "  * BiLSTM + Attn + GloVe 100d: **0.699 ± 0.010**\n",
    "* **CNNs trail slightly:**\n",
    "\n",
    "  * CNN + GloVe 300d: **0.694 ± 0.005**\n",
    "  * CNN + GloVe 100d: **0.689 ± 0.008**\n",
    "* **Calibration:** CNNs show **lower ECE** (≈0.18) than LSTMs (≈0.22), i.e., slightly **better-calibrated** probabilities out-of-the-box.\n",
    "* **Thresholding:** simple val-tuned thresholds boost test **F1 to \\~0.726–0.729** across BiLSTM variants.\n",
    "* **Speed:** extremely fast batched inference (≈**0.07–0.19 ms/sample**), with **CNN-100d** the quickest.\n",
    "\n",
    "**Takeaway:** With a fixed split, **BiLSTM + GloVe (300d)** (with or without attention) gives the **best raw F1 (\\~0.707)**; **CNNs** are a hair behind but **better calibrated** and **faster**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.3 Per-domain, balanced & equalized (Twitter/Reddit/News)\n",
    "\n",
    "I re-ran the suite per domain with **50/50 label balance** and **equal total N across domains** (per seed), using **70/20/10** splits. This isolates **domain difficulty** from class balance and dataset size.\n",
    "\n",
    "### Best by domain (mean F1 across 3 balanced variants × 3 seeds)\n",
    "\n",
    "* **News → BiLSTM + Attn + GloVe 300d:** **0.760 ± 0.035**\n",
    "* **Reddit → BiLSTM + Attn + GloVe 100d:** **0.629 ± 0.053**\n",
    "* **Twitter → CNN + GloVe 100d:** **0.620 ± 0.044**\n",
    "\n",
    "**Observations**\n",
    "\n",
    "* **News is easiest** for classic DL; attention + higher-dim embeddings help.\n",
    "* **Reddit is hardest** (dense irony, context needs); **lightweight attention** with **100d** edges out others.\n",
    "* **Twitter** favors **CNN-100d**—short, punchy patterns + speed.\n",
    "\n",
    "### Overall model ranking (grand mean across domains × variants × seeds)\n",
    "\n",
    "* **BiLSTM + GloVe 300d:** **F1 0.640** (best mean)\n",
    "* **BiLSTM + Attn + GloVe 100d:** **0.637**\n",
    "* **BiLSTM + GloVe 100d:** **0.631**\n",
    "* **CNN + GloVe 300d:** **0.619**\n",
    "* **BiLSTM + Attn + GloVe 300d:** **0.616**\n",
    "* **CNN + GloVe 100d:** **0.604** (but **fastest**)\n",
    "\n",
    "### Calibration & speed (per-domain runs)\n",
    "\n",
    "* **Best ECE** on average: **CNN-100d (≈0.118)** → **most reliable confidence scores**.\n",
    "* **Fastest inference:** **CNN-100d (\\~0.44 ms/sample)**, then **CNN-300d (\\~0.53 ms)**.\n",
    "* **BiLSTMs** are slower (\\~**1.08–1.94 ms/sample**), especially with attention.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.4 Error tendencies (quick read)\n",
    "\n",
    "* **Reddit:** Many misses are **deadpan/implicit sarcasm** that lacks surface cues—classic DL struggles without context.\n",
    "* **Twitter:** **Hyperbole/exclamation** can inflate false positives; threshold tuning helps.\n",
    "* **News:** Cleaner, less slangy; models are steadier and benefit from **attention + richer embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.5 Practical picks\n",
    "\n",
    "* **If you want one classic DL model for all domains:**\n",
    "\n",
    "  * **BiLSTM + GloVe 300d** for the **best average F1**.\n",
    "* **If you care about speed + calibration:**\n",
    "\n",
    "  * **CNN + GloVe 100d** (fastest; best ECE), accept a small F1 hit.\n",
    "* **Route by domain (simple policy):**\n",
    "\n",
    "  * **News → BiLSTM + Attn + 300d**\n",
    "  * **Reddit → BiLSTM + Attn + 100d**\n",
    "  * **Twitter → CNN + 100d**\n",
    "\n",
    "---\n",
    "\n",
    "## 6.6 One-liner for the Results section\n",
    "\n",
    "> Classic GloVe-based models remain competitive: on a fixed split the **BiLSTM (300d)** reaches **F1≈0.707**, while per-domain balanced runs show **News** is most tractable (**BiLSTM+Attn-300d F1≈0.76**), **Reddit** the hardest (**BiLSTM+Attn-100d F1≈0.63**), and **Twitter** favors **CNN-100d (F1≈0.62)**; **CNN-100d** also delivers the **best calibration (ECE≈0.12)** and **fastest inference (\\~0.44 ms/sample)**, and simple **threshold tuning** lifts BiLSTM test **F1 to \\~0.73**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bb7ddc92-b194-4864-a483-1bd4cbfe3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ BiLSTM_GloVe100 ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 313s - 60ms/step - accuracy: 0.6512 - loss: 0.6179 - val_accuracy: 0.6803 - val_loss: 0.5895\n",
      "Epoch 2/20\n",
      "5200/5200 - 213s - 41ms/step - accuracy: 0.6854 - loss: 0.5867 - val_accuracy: 0.6964 - val_loss: 0.5734\n",
      "Epoch 3/20\n",
      "5200/5200 - 222s - 43ms/step - accuracy: 0.6988 - loss: 0.5724 - val_accuracy: 0.7021 - val_loss: 0.5664\n",
      "Epoch 4/20\n",
      "5200/5200 - 262s - 50ms/step - accuracy: 0.7081 - loss: 0.5619 - val_accuracy: 0.7087 - val_loss: 0.5600\n",
      "Epoch 5/20\n",
      "5200/5200 - 264s - 51ms/step - accuracy: 0.7155 - loss: 0.5537 - val_accuracy: 0.7118 - val_loss: 0.5560\n",
      "Epoch 6/20\n",
      "5200/5200 - 262s - 50ms/step - accuracy: 0.7218 - loss: 0.5462 - val_accuracy: 0.7141 - val_loss: 0.5543\n",
      "Epoch 7/20\n",
      "5200/5200 - 289s - 56ms/step - accuracy: 0.7267 - loss: 0.5399 - val_accuracy: 0.7137 - val_loss: 0.5548\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 320s - 62ms/step - accuracy: 0.6521 - loss: 0.6178 - val_accuracy: 0.6770 - val_loss: 0.5938\n",
      "Epoch 2/20\n",
      "5200/5200 - 321s - 62ms/step - accuracy: 0.6845 - loss: 0.5879 - val_accuracy: 0.6906 - val_loss: 0.5805\n",
      "Epoch 3/20\n",
      "5200/5200 - 308s - 59ms/step - accuracy: 0.6962 - loss: 0.5758 - val_accuracy: 0.7005 - val_loss: 0.5685\n",
      "Epoch 4/20\n",
      "5200/5200 - 282s - 54ms/step - accuracy: 0.7052 - loss: 0.5665 - val_accuracy: 0.7054 - val_loss: 0.5627\n",
      "Epoch 5/20\n",
      "5200/5200 - 224s - 43ms/step - accuracy: 0.7116 - loss: 0.5584 - val_accuracy: 0.7076 - val_loss: 0.5607\n",
      "Epoch 6/20\n",
      "5200/5200 - 209s - 40ms/step - accuracy: 0.7171 - loss: 0.5517 - val_accuracy: 0.7083 - val_loss: 0.5605\n",
      "Epoch 7/20\n",
      "5200/5200 - 205s - 39ms/step - accuracy: 0.7232 - loss: 0.5448 - val_accuracy: 0.7108 - val_loss: 0.5588\n",
      "Epoch 8/20\n",
      "5200/5200 - 194s - 37ms/step - accuracy: 0.7274 - loss: 0.5394 - val_accuracy: 0.7116 - val_loss: 0.5587\n",
      "Epoch 9/20\n",
      "5200/5200 - 197s - 38ms/step - accuracy: 0.7316 - loss: 0.5339 - val_accuracy: 0.7105 - val_loss: 0.5593\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 214s - 41ms/step - accuracy: 0.6524 - loss: 0.6169 - val_accuracy: 0.6843 - val_loss: 0.5853\n",
      "Epoch 2/20\n",
      "5200/5200 - 233s - 45ms/step - accuracy: 0.6906 - loss: 0.5820 - val_accuracy: 0.7000 - val_loss: 0.5681\n",
      "Epoch 3/20\n",
      "5200/5200 - 274s - 53ms/step - accuracy: 0.7045 - loss: 0.5671 - val_accuracy: 0.7061 - val_loss: 0.5613\n",
      "Epoch 4/20\n",
      "5200/5200 - 255s - 49ms/step - accuracy: 0.7135 - loss: 0.5571 - val_accuracy: 0.7103 - val_loss: 0.5577\n",
      "Epoch 5/20\n",
      "5200/5200 - 272s - 52ms/step - accuracy: 0.7199 - loss: 0.5486 - val_accuracy: 0.7107 - val_loss: 0.5612\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step\n",
      "\n",
      "=== BiLSTM_GloVe100 — Aggregate (3 seeds) ===\n",
      "                       acc: 0.7101 ± 0.0016\n",
      "                 precision: 0.7359 ± 0.0093\n",
      "                    recall: 0.6700 ± 0.0126\n",
      "                        f1: 0.7013 ± 0.0028\n",
      "                   roc_auc: 0.7829 ± 0.0026\n",
      "                    pr_auc: 0.7982 ± 0.0025\n",
      "                     brier: 0.1894 ± 0.0013\n",
      "                 ece_10bin: 0.2183 ± 0.0059\n",
      " f1_at_best_threshold_test: 0.7261 ± 0.0017\n",
      "            train_time_sec: 1778.1749 ± 507.5410\n",
      "            infer_time_sec: 26.1219 ± 5.7875\n",
      "   infer_avg_ms_per_sample: 0.1374 ± 0.0304\n",
      "\n",
      "================ BiLSTM_GloVe300 ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 328s - 63ms/step - accuracy: 0.6685 - loss: 0.6031 - val_accuracy: 0.6939 - val_loss: 0.5760\n",
      "Epoch 2/20\n",
      "5200/5200 - 328s - 63ms/step - accuracy: 0.7030 - loss: 0.5675 - val_accuracy: 0.7081 - val_loss: 0.5597\n",
      "Epoch 3/20\n",
      "5200/5200 - 302s - 58ms/step - accuracy: 0.7179 - loss: 0.5500 - val_accuracy: 0.7102 - val_loss: 0.5560\n",
      "Epoch 4/20\n",
      "5200/5200 - 305s - 59ms/step - accuracy: 0.7294 - loss: 0.5368 - val_accuracy: 0.7148 - val_loss: 0.5512\n",
      "Epoch 5/20\n",
      "5200/5200 - 348s - 67ms/step - accuracy: 0.7382 - loss: 0.5254 - val_accuracy: 0.7147 - val_loss: 0.5509\n",
      "Epoch 6/20\n",
      "5200/5200 - 283s - 54ms/step - accuracy: 0.7463 - loss: 0.5149 - val_accuracy: 0.7141 - val_loss: 0.5553\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 18ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 414s - 80ms/step - accuracy: 0.6678 - loss: 0.6036 - val_accuracy: 0.6930 - val_loss: 0.5761\n",
      "Epoch 2/20\n",
      "5200/5200 - 332s - 64ms/step - accuracy: 0.7001 - loss: 0.5713 - val_accuracy: 0.7057 - val_loss: 0.5637\n",
      "Epoch 3/20\n",
      "5200/5200 - 282s - 54ms/step - accuracy: 0.7144 - loss: 0.5553 - val_accuracy: 0.7117 - val_loss: 0.5567\n",
      "Epoch 4/20\n",
      "5200/5200 - 245s - 47ms/step - accuracy: 0.7260 - loss: 0.5419 - val_accuracy: 0.7120 - val_loss: 0.5557\n",
      "Epoch 5/20\n",
      "5200/5200 - 236s - 45ms/step - accuracy: 0.7345 - loss: 0.5303 - val_accuracy: 0.7103 - val_loss: 0.5592\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 20ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 248s - 48ms/step - accuracy: 0.6690 - loss: 0.6024 - val_accuracy: 0.6991 - val_loss: 0.5700\n",
      "Epoch 2/20\n",
      "5200/5200 - 242s - 47ms/step - accuracy: 0.7047 - loss: 0.5662 - val_accuracy: 0.7107 - val_loss: 0.5578\n",
      "Epoch 3/20\n",
      "5200/5200 - 239s - 46ms/step - accuracy: 0.7200 - loss: 0.5488 - val_accuracy: 0.7157 - val_loss: 0.5539\n",
      "Epoch 4/20\n",
      "5200/5200 - 236s - 45ms/step - accuracy: 0.7306 - loss: 0.5355 - val_accuracy: 0.7170 - val_loss: 0.5561\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step\n",
      "\n",
      "=== BiLSTM_GloVe300 — Aggregate (3 seeds) ===\n",
      "                       acc: 0.7143 ± 0.0018\n",
      "                 precision: 0.7380 ± 0.0140\n",
      "                    recall: 0.6795 ± 0.0230\n",
      "                        f1: 0.7072 ± 0.0064\n",
      "                   roc_auc: 0.7876 ± 0.0022\n",
      "                    pr_auc: 0.8029 ± 0.0024\n",
      "                     brier: 0.1876 ± 0.0010\n",
      "                 ece_10bin: 0.2264 ± 0.0019\n",
      " f1_at_best_threshold_test: 0.7287 ± 0.0013\n",
      "            train_time_sec: 1456.3513 ± 467.3320\n",
      "            infer_time_sec: 30.6662 ± 3.2888\n",
      "   infer_avg_ms_per_sample: 0.1613 ± 0.0173\n",
      "\n",
      "================ BiLSTM_Attn_100 ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 227s - 44ms/step - accuracy: 0.6550 - loss: 0.6157 - val_accuracy: 0.6866 - val_loss: 0.5851\n",
      "Epoch 2/20\n",
      "5200/5200 - 233s - 45ms/step - accuracy: 0.6917 - loss: 0.5806 - val_accuracy: 0.7010 - val_loss: 0.5695\n",
      "Epoch 3/20\n",
      "5200/5200 - 297s - 57ms/step - accuracy: 0.7059 - loss: 0.5655 - val_accuracy: 0.7064 - val_loss: 0.5622\n",
      "Epoch 4/20\n",
      "5200/5200 - 340s - 65ms/step - accuracy: 0.7141 - loss: 0.5553 - val_accuracy: 0.7096 - val_loss: 0.5578\n",
      "Epoch 5/20\n",
      "5200/5200 - 330s - 63ms/step - accuracy: 0.7215 - loss: 0.5470 - val_accuracy: 0.7106 - val_loss: 0.5564\n",
      "Epoch 6/20\n",
      "5200/5200 - 253s - 49ms/step - accuracy: 0.7270 - loss: 0.5397 - val_accuracy: 0.7111 - val_loss: 0.5570\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 284s - 55ms/step - accuracy: 0.6547 - loss: 0.6149 - val_accuracy: 0.6787 - val_loss: 0.5911\n",
      "Epoch 2/20\n",
      "5200/5200 - 258s - 50ms/step - accuracy: 0.6925 - loss: 0.5794 - val_accuracy: 0.6988 - val_loss: 0.5693\n",
      "Epoch 3/20\n",
      "5200/5200 - 254s - 49ms/step - accuracy: 0.7060 - loss: 0.5650 - val_accuracy: 0.7030 - val_loss: 0.5634\n",
      "Epoch 4/20\n",
      "5200/5200 - 265s - 51ms/step - accuracy: 0.7150 - loss: 0.5549 - val_accuracy: 0.7061 - val_loss: 0.5600\n",
      "Epoch 5/20\n",
      "5200/5200 - 289s - 56ms/step - accuracy: 0.7219 - loss: 0.5463 - val_accuracy: 0.7091 - val_loss: 0.5571\n",
      "Epoch 6/20\n",
      "5200/5200 - 276s - 53ms/step - accuracy: 0.7273 - loss: 0.5392 - val_accuracy: 0.7097 - val_loss: 0.5580\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 405s - 78ms/step - accuracy: 0.6556 - loss: 0.6149 - val_accuracy: 0.6891 - val_loss: 0.5831\n",
      "Epoch 2/20\n",
      "5200/5200 - 262s - 50ms/step - accuracy: 0.6921 - loss: 0.5806 - val_accuracy: 0.7014 - val_loss: 0.5678\n",
      "Epoch 3/20\n",
      "5200/5200 - 238s - 46ms/step - accuracy: 0.7054 - loss: 0.5663 - val_accuracy: 0.7073 - val_loss: 0.5615\n",
      "Epoch 4/20\n",
      "5200/5200 - 235s - 45ms/step - accuracy: 0.7144 - loss: 0.5559 - val_accuracy: 0.7101 - val_loss: 0.5582\n",
      "Epoch 5/20\n",
      "5200/5200 - 228s - 44ms/step - accuracy: 0.7211 - loss: 0.5475 - val_accuracy: 0.7119 - val_loss: 0.5584\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 17ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step\n",
      "\n",
      "=== BiLSTM_Attn_100 — Aggregate (3 seeds) ===\n",
      "                       acc: 0.7100 ± 0.0012\n",
      "                 precision: 0.7390 ± 0.0178\n",
      "                    recall: 0.6650 ± 0.0329\n",
      "                        f1: 0.6994 ± 0.0098\n",
      "                   roc_auc: 0.7834 ± 0.0013\n",
      "                    pr_auc: 0.7991 ± 0.0014\n",
      "                     brier: 0.1891 ± 0.0004\n",
      "                 ece_10bin: 0.2180 ± 0.0042\n",
      " f1_at_best_threshold_test: 0.7259 ± 0.0006\n",
      "            train_time_sec: 1557.6349 ± 166.9168\n",
      "            infer_time_sec: 33.7939 ± 7.9790\n",
      "   infer_avg_ms_per_sample: 0.1777 ± 0.0420\n",
      "\n",
      "================ BiLSTM_Attn_300 ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 323s - 62ms/step - accuracy: 0.6718 - loss: 0.6005 - val_accuracy: 0.6952 - val_loss: 0.5759\n",
      "Epoch 2/20\n",
      "5200/5200 - 294s - 57ms/step - accuracy: 0.7065 - loss: 0.5647 - val_accuracy: 0.7090 - val_loss: 0.5583\n",
      "Epoch 3/20\n",
      "5200/5200 - 291s - 56ms/step - accuracy: 0.7215 - loss: 0.5478 - val_accuracy: 0.7125 - val_loss: 0.5547\n",
      "Epoch 4/20\n",
      "5200/5200 - 272s - 52ms/step - accuracy: 0.7328 - loss: 0.5340 - val_accuracy: 0.7151 - val_loss: 0.5536\n",
      "Epoch 5/20\n",
      "5200/5200 - 264s - 51ms/step - accuracy: 0.7421 - loss: 0.5222 - val_accuracy: 0.7164 - val_loss: 0.5544\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 22ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 263s - 51ms/step - accuracy: 0.6729 - loss: 0.5989 - val_accuracy: 0.6959 - val_loss: 0.5741\n",
      "Epoch 2/20\n",
      "5200/5200 - 285s - 55ms/step - accuracy: 0.7079 - loss: 0.5628 - val_accuracy: 0.7078 - val_loss: 0.5597\n",
      "Epoch 3/20\n",
      "5200/5200 - 284s - 55ms/step - accuracy: 0.7231 - loss: 0.5457 - val_accuracy: 0.7119 - val_loss: 0.5534\n",
      "Epoch 4/20\n",
      "5200/5200 - 273s - 52ms/step - accuracy: 0.7339 - loss: 0.5323 - val_accuracy: 0.7129 - val_loss: 0.5537\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 281s - 54ms/step - accuracy: 0.6727 - loss: 0.5988 - val_accuracy: 0.6990 - val_loss: 0.5700\n",
      "Epoch 2/20\n",
      "5200/5200 - 284s - 55ms/step - accuracy: 0.7071 - loss: 0.5643 - val_accuracy: 0.7106 - val_loss: 0.5577\n",
      "Epoch 3/20\n",
      "5200/5200 - 277s - 53ms/step - accuracy: 0.7219 - loss: 0.5468 - val_accuracy: 0.7163 - val_loss: 0.5530\n",
      "Epoch 4/20\n",
      "5200/5200 - 280s - 54ms/step - accuracy: 0.7331 - loss: 0.5332 - val_accuracy: 0.7183 - val_loss: 0.5550\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step\n",
      "\n",
      "=== BiLSTM_Attn_300 — Aggregate (3 seeds) ===\n",
      "                       acc: 0.7138 ± 0.0022\n",
      "                 precision: 0.7372 ± 0.0241\n",
      "                    recall: 0.6815 ± 0.0417\n",
      "                        f1: 0.7072 ± 0.0109\n",
      "                   roc_auc: 0.7878 ± 0.0001\n",
      "                    pr_auc: 0.8035 ± 0.0004\n",
      "                     brier: 0.1876 ± 0.0003\n",
      "                 ece_10bin: 0.2256 ± 0.0067\n",
      " f1_at_best_threshold_test: 0.7294 ± 0.0005\n",
      "            train_time_sec: 1223.5989 ± 191.0371\n",
      "            infer_time_sec: 31.4697 ± 2.0682\n",
      "   infer_avg_ms_per_sample: 0.1655 ± 0.0109\n",
      "\n",
      "================ CNN_GloVe100 ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 108s - 21ms/step - accuracy: 0.6232 - loss: 0.6417 - val_accuracy: 0.6608 - val_loss: 0.6113\n",
      "Epoch 2/20\n",
      "5200/5200 - 109s - 21ms/step - accuracy: 0.6585 - loss: 0.6134 - val_accuracy: 0.6759 - val_loss: 0.5972\n",
      "Epoch 3/20\n",
      "5200/5200 - 108s - 21ms/step - accuracy: 0.6701 - loss: 0.6019 - val_accuracy: 0.6841 - val_loss: 0.5872\n",
      "Epoch 4/20\n",
      "5200/5200 - 106s - 20ms/step - accuracy: 0.6775 - loss: 0.5941 - val_accuracy: 0.6882 - val_loss: 0.5824\n",
      "Epoch 5/20\n",
      "5200/5200 - 112s - 21ms/step - accuracy: 0.6832 - loss: 0.5884 - val_accuracy: 0.6895 - val_loss: 0.5805\n",
      "Epoch 6/20\n",
      "5200/5200 - 111s - 21ms/step - accuracy: 0.6871 - loss: 0.5835 - val_accuracy: 0.6934 - val_loss: 0.5775\n",
      "Epoch 7/20\n",
      "5200/5200 - 111s - 21ms/step - accuracy: 0.6916 - loss: 0.5790 - val_accuracy: 0.6934 - val_loss: 0.5783\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 119s - 23ms/step - accuracy: 0.6230 - loss: 0.6420 - val_accuracy: 0.6554 - val_loss: 0.6137\n",
      "Epoch 2/20\n",
      "5200/5200 - 118s - 23ms/step - accuracy: 0.6581 - loss: 0.6134 - val_accuracy: 0.6706 - val_loss: 0.5979\n",
      "Epoch 3/20\n",
      "5200/5200 - 121s - 23ms/step - accuracy: 0.6696 - loss: 0.6018 - val_accuracy: 0.6784 - val_loss: 0.5915\n",
      "Epoch 4/20\n",
      "5200/5200 - 120s - 23ms/step - accuracy: 0.6778 - loss: 0.5940 - val_accuracy: 0.6838 - val_loss: 0.5859\n",
      "Epoch 5/20\n",
      "5200/5200 - 115s - 22ms/step - accuracy: 0.6835 - loss: 0.5885 - val_accuracy: 0.6893 - val_loss: 0.5792\n",
      "Epoch 6/20\n",
      "5200/5200 - 114s - 22ms/step - accuracy: 0.6878 - loss: 0.5834 - val_accuracy: 0.6941 - val_loss: 0.5761\n",
      "Epoch 7/20\n",
      "5200/5200 - 121s - 23ms/step - accuracy: 0.6911 - loss: 0.5789 - val_accuracy: 0.6936 - val_loss: 0.5746\n",
      "Epoch 8/20\n",
      "5200/5200 - 118s - 23ms/step - accuracy: 0.6945 - loss: 0.5756 - val_accuracy: 0.6941 - val_loss: 0.5745\n",
      "Epoch 9/20\n",
      "5200/5200 - 126s - 24ms/step - accuracy: 0.6978 - loss: 0.5723 - val_accuracy: 0.6948 - val_loss: 0.5720\n",
      "Epoch 10/20\n",
      "5200/5200 - 113s - 22ms/step - accuracy: 0.6999 - loss: 0.5693 - val_accuracy: 0.6949 - val_loss: 0.5703\n",
      "Epoch 11/20\n",
      "5200/5200 - 117s - 22ms/step - accuracy: 0.7027 - loss: 0.5668 - val_accuracy: 0.6955 - val_loss: 0.5699\n",
      "Epoch 12/20\n",
      "5200/5200 - 119s - 23ms/step - accuracy: 0.7048 - loss: 0.5639 - val_accuracy: 0.6970 - val_loss: 0.5697\n",
      "Epoch 13/20\n",
      "5200/5200 - 119s - 23ms/step - accuracy: 0.7075 - loss: 0.5615 - val_accuracy: 0.6966 - val_loss: 0.5702\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 133s - 25ms/step - accuracy: 0.6218 - loss: 0.6428 - val_accuracy: 0.6648 - val_loss: 0.6081\n",
      "Epoch 2/20\n",
      "5200/5200 - 112s - 22ms/step - accuracy: 0.6580 - loss: 0.6141 - val_accuracy: 0.6790 - val_loss: 0.5927\n",
      "Epoch 3/20\n",
      "5200/5200 - 106s - 20ms/step - accuracy: 0.6698 - loss: 0.6022 - val_accuracy: 0.6823 - val_loss: 0.5881\n",
      "Epoch 4/20\n",
      "5200/5200 - 111s - 21ms/step - accuracy: 0.6772 - loss: 0.5943 - val_accuracy: 0.6844 - val_loss: 0.5827\n",
      "Epoch 5/20\n",
      "5200/5200 - 122s - 24ms/step - accuracy: 0.6836 - loss: 0.5880 - val_accuracy: 0.6884 - val_loss: 0.5805\n",
      "Epoch 6/20\n",
      "5200/5200 - 126s - 24ms/step - accuracy: 0.6874 - loss: 0.5838 - val_accuracy: 0.6935 - val_loss: 0.5762\n",
      "Epoch 7/20\n",
      "5200/5200 - 111s - 21ms/step - accuracy: 0.6908 - loss: 0.5792 - val_accuracy: 0.6947 - val_loss: 0.5772\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "\n",
      "=== CNN_GloVe100 — Aggregate (3 seeds) ===\n",
      "                       acc: 0.6957 ± 0.0034\n",
      "                 precision: 0.7161 ± 0.0078\n",
      "                    recall: 0.6648 ± 0.0193\n",
      "                        f1: 0.6893 ± 0.0077\n",
      "                   roc_auc: 0.7664 ± 0.0041\n",
      "                    pr_auc: 0.7834 ± 0.0040\n",
      "                     brier: 0.1964 ± 0.0020\n",
      "                 ece_10bin: 0.1775 ± 0.0043\n",
      " f1_at_best_threshold_test: 0.7165 ± 0.0022\n",
      "            train_time_sec: 1042.6825 ± 432.6803\n",
      "            infer_time_sec: 13.4586 ± 1.2232\n",
      "   infer_avg_ms_per_sample: 0.0708 ± 0.0064\n",
      "\n",
      "================ CNN_GloVe300 ================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 293s - 56ms/step - accuracy: 0.6397 - loss: 0.6288 - val_accuracy: 0.6757 - val_loss: 0.5984\n",
      "Epoch 2/20\n",
      "5200/5200 - 305s - 59ms/step - accuracy: 0.6726 - loss: 0.5993 - val_accuracy: 0.6904 - val_loss: 0.5818\n",
      "Epoch 3/20\n",
      "5200/5200 - 307s - 59ms/step - accuracy: 0.6862 - loss: 0.5856 - val_accuracy: 0.6957 - val_loss: 0.5760\n",
      "Epoch 4/20\n",
      "5200/5200 - 310s - 60ms/step - accuracy: 0.6947 - loss: 0.5754 - val_accuracy: 0.6988 - val_loss: 0.5705\n",
      "Epoch 5/20\n",
      "5200/5200 - 337s - 65ms/step - accuracy: 0.7029 - loss: 0.5672 - val_accuracy: 0.7008 - val_loss: 0.5688\n",
      "Epoch 6/20\n",
      "5200/5200 - 325s - 62ms/step - accuracy: 0.7080 - loss: 0.5605 - val_accuracy: 0.7001 - val_loss: 0.5659\n",
      "Epoch 7/20\n",
      "5200/5200 - 354s - 68ms/step - accuracy: 0.7137 - loss: 0.5537 - val_accuracy: 0.7000 - val_loss: 0.5669\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 29ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 385s - 74ms/step - accuracy: 0.6392 - loss: 0.6289 - val_accuracy: 0.6765 - val_loss: 0.5938\n",
      "Epoch 2/20\n",
      "5200/5200 - 390s - 75ms/step - accuracy: 0.6731 - loss: 0.5989 - val_accuracy: 0.6879 - val_loss: 0.5812\n",
      "Epoch 3/20\n",
      "5200/5200 - 403s - 78ms/step - accuracy: 0.6869 - loss: 0.5852 - val_accuracy: 0.6949 - val_loss: 0.5741\n",
      "Epoch 4/20\n",
      "5200/5200 - 375s - 72ms/step - accuracy: 0.6960 - loss: 0.5748 - val_accuracy: 0.6978 - val_loss: 0.5705\n",
      "Epoch 5/20\n",
      "5200/5200 - 247s - 48ms/step - accuracy: 0.7028 - loss: 0.5667 - val_accuracy: 0.7016 - val_loss: 0.5667\n",
      "Epoch 6/20\n",
      "5200/5200 - 253s - 49ms/step - accuracy: 0.7090 - loss: 0.5591 - val_accuracy: 0.7028 - val_loss: 0.5644\n",
      "Epoch 7/20\n",
      "5200/5200 - 250s - 48ms/step - accuracy: 0.7144 - loss: 0.5526 - val_accuracy: 0.7028 - val_loss: 0.5665\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 20ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5200/5200 - 264s - 51ms/step - accuracy: 0.6379 - loss: 0.6307 - val_accuracy: 0.6765 - val_loss: 0.5943\n",
      "Epoch 2/20\n",
      "5200/5200 - 282s - 54ms/step - accuracy: 0.6715 - loss: 0.6006 - val_accuracy: 0.6887 - val_loss: 0.5804\n",
      "Epoch 3/20\n",
      "5200/5200 - 275s - 53ms/step - accuracy: 0.6853 - loss: 0.5863 - val_accuracy: 0.6943 - val_loss: 0.5751\n",
      "Epoch 4/20\n",
      "5200/5200 - 283s - 54ms/step - accuracy: 0.6944 - loss: 0.5763 - val_accuracy: 0.6988 - val_loss: 0.5700\n",
      "Epoch 5/20\n",
      "5200/5200 - 267s - 51ms/step - accuracy: 0.7025 - loss: 0.5673 - val_accuracy: 0.6983 - val_loss: 0.5719\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m1486/1486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 21ms/step\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step\n",
      "\n",
      "=== CNN_GloVe300 — Aggregate (3 seeds) ===\n",
      "                       acc: 0.7010 ± 0.0021\n",
      "                 precision: 0.7228 ± 0.0021\n",
      "                    recall: 0.6673 ± 0.0099\n",
      "                        f1: 0.6939 ± 0.0046\n",
      "                   roc_auc: 0.7727 ± 0.0025\n",
      "                    pr_auc: 0.7893 ± 0.0023\n",
      "                     brier: 0.1935 ± 0.0011\n",
      "                 ece_10bin: 0.1824 ± 0.0061\n",
      " f1_at_best_threshold_test: 0.7194 ± 0.0014\n",
      "            train_time_sec: 1968.9602 ± 519.2351\n",
      "            infer_time_sec: 35.5707 ± 6.9570\n",
      "   infer_avg_ms_per_sample: 0.1871 ± 0.0366\n",
      "\n",
      "All done. Artifacts under: /Users/evelinaivanova/Dissertation/out_fair/Classic_DL_Suite\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Classic DL Suite — Same Split, 3 Seeds\n",
    "# Models:\n",
    "#  - BiLSTM + GloVe (100d, 300d)\n",
    "#  - BiLSTM + Attention + GloVe (100d, 300d)\n",
    "#  - CNN + GloVe (100d, 300d)\n",
    "# Protocol:\n",
    "#  - One tokenizer + fixed split (70/10/20, stratified) shared by all models & seeds\n",
    "#  - EarlyStopping(patience=1), best weights\n",
    "#  - Per-seed artifacts (probs/preds/y, classification report, CM, misclassified, calibration bins, meta)\n",
    "#  - Per-model aggregate mean±std across seeds\n",
    "# ===============================\n",
    "\n",
    "import os, json, time, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Embedding, Bidirectional, LSTM, Dense, Dropout,\n",
    "                                     GlobalMaxPooling1D, Conv1D, Input, Concatenate)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, roc_auc_score,\n",
    "                             average_precision_score, brier_score_loss)\n",
    "\n",
    "# -------------------------------\n",
    "# Config (edit paths only)\n",
    "# -------------------------------\n",
    "CFG = {\n",
    "    \"glove_100_path\": \"glove.6B.100d.txt\",\n",
    "    \"glove_300_path\": \"glove.6B.300d.txt\",\n",
    "    \"max_len\": 40,\n",
    "    \"batch_size\": 128,\n",
    "    \"max_epochs\": 20,\n",
    "    \"patience\": 1,\n",
    "    \"seeds\": [13, 17, 23],\n",
    "    \"out_dir\": \"./out_fair/Classic_DL_Suite\",\n",
    "}\n",
    "\n",
    "# Expect df_filtered with columns: 'text' (str) and 'label' (0/1)\n",
    "texts_all = df_filtered['text'].astype(str).tolist()\n",
    "labels_all = df_filtered['label'].astype(int).to_numpy()\n",
    "\n",
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "def load_glove(path):\n",
    "    emb_index = {}\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            vals = line.rstrip().split(\" \")\n",
    "            word, vec = vals[0], np.asarray(vals[1:], dtype=\"float32\")\n",
    "            emb_index[word] = vec\n",
    "    return emb_index\n",
    "\n",
    "def build_embedding_matrix(tokenizer, emb_index, emb_dim):\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "    matrix = np.zeros((vocab_size, emb_dim), dtype=\"float32\")\n",
    "    for w, i in word_index.items():\n",
    "        vec = emb_index.get(w)\n",
    "        if vec is not None:\n",
    "            matrix[i] = vec\n",
    "    return matrix, vocab_size\n",
    "\n",
    "def ece_score(probs, labels, n_bins=10):\n",
    "    probs = np.asarray(probs, dtype=np.float64)\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    rows = []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        mask = (probs >= lo) & (probs < hi) if i < n_bins-1 else (probs >= lo) & (probs <= hi)\n",
    "        if not np.any(mask):\n",
    "            rows.append((float(lo), float(hi), 0, np.nan, np.nan))\n",
    "            continue\n",
    "        p = probs[mask]\n",
    "        y = labels[mask]\n",
    "        conf = p.mean()\n",
    "        acc = ((p >= 0.5).astype(int) == y).mean()\n",
    "        w = mask.mean()\n",
    "        ece += w * abs(acc - conf)\n",
    "        rows.append((float(lo), float(hi), int(mask.sum()), float(acc), float(conf)))\n",
    "    return float(ece), pd.DataFrame(rows, columns=[\"bin_lo\",\"bin_hi\",\"count\",\"bin_acc\",\"bin_conf\"])\n",
    "\n",
    "def threshold_tune(probs_val, y_val):\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in np.linspace(0.05, 0.95, 19):\n",
    "        f1v = f1_score(y_val, (probs_val >= t).astype(int), zero_division=0)\n",
    "        if f1v > best_f1:\n",
    "            best_f1, best_t = f1v, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenize once + fixed split\n",
    "# -------------------------------\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts_all)\n",
    "seqs = tokenizer.texts_to_sequences(texts_all)\n",
    "X = pad_sequences(seqs, maxlen=CFG[\"max_len\"], padding='post', truncating='post')\n",
    "\n",
    "X_tmp, X_test, y_tmp, y_test, idx_tmp, idx_test = train_test_split(\n",
    "    X, labels_all, np.arange(len(labels_all)),\n",
    "    test_size=0.20, random_state=42, stratify=labels_all\n",
    ")\n",
    "X_train, X_val, y_train, y_val, idx_train, idx_val = train_test_split(\n",
    "    X_tmp, y_tmp, idx_tmp,\n",
    "    test_size=0.125, random_state=42, stratify=y_tmp\n",
    ")\n",
    "texts_test = df_filtered.iloc[idx_test][\"text\"].tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# Load GloVe once (100d & 300d)\n",
    "# -------------------------------\n",
    "glove100 = load_glove(CFG[\"glove_100_path\"])\n",
    "glove300 = load_glove(CFG[\"glove_300_path\"])\n",
    "emb100, vocab_size = build_embedding_matrix(tokenizer, glove100, 100)\n",
    "emb300, _           = build_embedding_matrix(tokenizer, glove300, 300)\n",
    "\n",
    "# -------------------------------\n",
    "# Model builders\n",
    "# -------------------------------\n",
    "def build_bilstm(emb_matrix, max_len):\n",
    "    emb_dim = emb_matrix.shape[1]\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=emb_matrix.shape[0],\n",
    "                  output_dim=emb_dim,\n",
    "                  weights=[emb_matrix],\n",
    "                  input_length=max_len,\n",
    "                  trainable=False),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class AdditiveAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Simple additive attention over time steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, units=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.W = layers.Dense(units, use_bias=True, activation='tanh')\n",
    "        self.v = layers.Dense(1, use_bias=False)\n",
    "\n",
    "    def call(self, h, mask=None):\n",
    "        # h: (batch, time, dim)\n",
    "        score = self.v(self.W(h))   # (batch, time, 1)\n",
    "        weights = tf.nn.softmax(score, axis=1)  # (batch, time, 1)\n",
    "        if mask is not None:\n",
    "            # mask: (batch, time)\n",
    "            mask = tf.cast(mask[:, :, tf.newaxis], tf.float32)\n",
    "            weights *= mask\n",
    "            weights = weights / (tf.reduce_sum(weights, axis=1, keepdims=True) + 1e-8)\n",
    "        context = tf.reduce_sum(weights * h, axis=1)  # (batch, dim)\n",
    "        return context\n",
    "\n",
    "def build_bilstm_attention(emb_matrix, max_len):\n",
    "    emb_dim = emb_matrix.shape[1]\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    emb = Embedding(input_dim=emb_matrix.shape[0],\n",
    "                    output_dim=emb_dim,\n",
    "                    weights=[emb_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=False, mask_zero=False)(inp)\n",
    "    h = Bidirectional(LSTM(64, return_sequences=True))(emb)\n",
    "    att = AdditiveAttention(64)(h)  # (batch, hidden*2)\n",
    "    x = Dropout(0.5)(att)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn(emb_matrix, max_len):\n",
    "    emb_dim = emb_matrix.shape[1]\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    emb = Embedding(input_dim=emb_matrix.shape[0],\n",
    "                    output_dim=emb_dim,\n",
    "                    weights=[emb_matrix],\n",
    "                    input_length=max_len,\n",
    "                    trainable=False)(inp)\n",
    "    # multi-kernel TextCNN\n",
    "    convs = []\n",
    "    for k in (3,4,5):\n",
    "        c = Conv1D(filters=128, kernel_size=k, activation='relu', padding='valid')(emb)\n",
    "        p = GlobalMaxPooling1D()(c)\n",
    "        convs.append(p)\n",
    "    x = Concatenate()(convs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Model registry: (name, builder, emb_matrix)\n",
    "# -------------------------------\n",
    "MODEL_SPECS = [\n",
    "    (\"BiLSTM_GloVe100\",      build_bilstm,           emb100),\n",
    "    (\"BiLSTM_GloVe300\",      build_bilstm,           emb300),\n",
    "    (\"BiLSTM_Attn_100\",      build_bilstm_attention, emb100),\n",
    "    (\"BiLSTM_Attn_300\",      build_bilstm_attention, emb300),\n",
    "    (\"CNN_GloVe100\",         build_cnn,              emb100),\n",
    "    (\"CNN_GloVe300\",         build_cnn,              emb300),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Train & evaluate per model × seed\n",
    "# -------------------------------\n",
    "OUT_ROOT = Path(CFG[\"out_dir\"])\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def mean_std(df, col):\n",
    "    if len(df) == 1:\n",
    "        return float(df[col].iloc[0]), 0.0\n",
    "    return float(df[col].mean()), float(df[col].std(ddof=1))\n",
    "\n",
    "all_model_aggregates = {}\n",
    "\n",
    "for model_name, builder, emb_matrix in MODEL_SPECS:\n",
    "    print(f\"\\n================ {model_name} ================\\n\")\n",
    "    per_seed_rows = []\n",
    "    model_dir = OUT_ROOT / model_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for seed in CFG[\"seeds\"]:\n",
    "        set_all_seeds(seed)\n",
    "        run_dir = model_dir / f\"seed_{seed}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Build & train\n",
    "        model = builder(emb_matrix, CFG[\"max_len\"])\n",
    "        es = EarlyStopping(monitor=\"val_loss\", patience=CFG[\"patience\"], restore_best_weights=True, verbose=1)\n",
    "\n",
    "        t0 = time.time()\n",
    "        hist = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=CFG[\"max_epochs\"],\n",
    "            batch_size=CFG[\"batch_size\"],\n",
    "            verbose=2,\n",
    "            callbacks=[es]\n",
    "        )\n",
    "        train_time = time.time() - t0\n",
    "\n",
    "        # Predict\n",
    "        t1 = time.time()\n",
    "        probs_test = model.predict(X_test, batch_size=CFG[\"batch_size\"]).reshape(-1)\n",
    "        infer_time = time.time() - t1\n",
    "        preds_test = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "        # Metrics\n",
    "        acc  = accuracy_score(y_test, preds_test)\n",
    "        prec = precision_score(y_test, preds_test, zero_division=0)\n",
    "        rec  = recall_score(y_test, preds_test, zero_division=0)\n",
    "        f1   = f1_score(y_test, preds_test, zero_division=0)\n",
    "        try:\n",
    "            rocauc = roc_auc_score(y_test, probs_test)\n",
    "        except Exception:\n",
    "            rocauc = float('nan')\n",
    "        ap = average_precision_score(y_test, probs_test)\n",
    "        brier = brier_score_loss(y_test, probs_test)\n",
    "        ece, ece_table = ece_score(probs_test, y_test, n_bins=10)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, preds_test)\n",
    "        # Misclassifications\n",
    "        mis_mask = (preds_test != y_test)\n",
    "        df_miss = pd.DataFrame({\n",
    "            \"text\": np.array(texts_test, dtype=object)[mis_mask],\n",
    "            \"label_true\": y_test[mis_mask],\n",
    "            \"prob_pred\": probs_test[mis_mask],\n",
    "            \"pred_label\": preds_test[mis_mask],\n",
    "            \"abs_error\": np.abs(probs_test[mis_mask] - y_test[mis_mask])\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        # Threshold tuning on val\n",
    "        probs_val = model.predict(X_val, batch_size=CFG[\"batch_size\"]).reshape(-1)\n",
    "        best_t, best_f1_val = threshold_tune(probs_val, y_val)\n",
    "        f1_tuned = f1_score(y_test, (probs_test >= best_t).astype(int), zero_division=0)\n",
    "\n",
    "        # Report\n",
    "        report = classification_report(y_test, preds_test, output_dict=True, zero_division=0)\n",
    "        report_df = pd.DataFrame(report).T\n",
    "\n",
    "        # Save artifacts\n",
    "        np.save(run_dir / \"probs_test.npy\", probs_test)\n",
    "        np.save(run_dir / \"preds_test.npy\", preds_test)\n",
    "        np.save(run_dir / \"y_test.npy\",   y_test)\n",
    "\n",
    "        report_df.to_csv(run_dir / \"classification_report.csv\")\n",
    "        pd.DataFrame(cm, index=[0,1], columns=[0,1]).to_csv(run_dir / \"confusion_matrix.csv\")\n",
    "        ece_table.to_csv(run_dir / \"calibration_bins.csv\", index=False)\n",
    "        df_miss.to_csv(run_dir / \"misclassified.csv\", index=False)\n",
    "\n",
    "        meta = {\n",
    "            \"model\": model_name,\n",
    "            \"seed\": seed,\n",
    "            \"epochs_run\": int(len(hist.history.get(\"loss\", []))),\n",
    "            \"train_time_sec\": float(train_time),\n",
    "            \"infer_time_sec\": float(infer_time),\n",
    "            \"infer_avg_ms_per_sample\": float((infer_time / len(y_test)) * 1000.0),\n",
    "            \"acc\": float(acc),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "            \"f1\": float(f1),\n",
    "            \"roc_auc\": float(rocauc),\n",
    "            \"pr_auc\": float(ap),\n",
    "            \"brier\": float(brier),\n",
    "            \"ece_10bin\": float(ece),\n",
    "            \"best_threshold_val_f1\": float(best_t),\n",
    "            \"f1_at_best_threshold_test\": float(f1_tuned)\n",
    "        }\n",
    "        with open(run_dir / \"meta.json\", \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        per_seed_rows.append(meta)\n",
    "\n",
    "    # Aggregate for this model\n",
    "    df_seeds = pd.DataFrame(per_seed_rows)\n",
    "    df_seeds.to_csv(model_dir / \"per_seed_metrics.csv\", index=False)\n",
    "\n",
    "    agg = {}\n",
    "    for col in [\"acc\",\"precision\",\"recall\",\"f1\",\"roc_auc\",\"pr_auc\",\"brier\",\"ece_10bin\",\n",
    "                \"f1_at_best_threshold_test\",\"train_time_sec\",\"infer_time_sec\",\"infer_avg_ms_per_sample\"]:\n",
    "        m, s = mean_std(df_seeds, col)\n",
    "        agg[col] = {\"mean\": m, \"std\": s}\n",
    "\n",
    "    with open(model_dir / \"aggregate_mean_std.json\", \"w\") as f:\n",
    "        json.dump(agg, f, indent=2)\n",
    "\n",
    "    print(f\"\\n=== {model_name} — Aggregate (3 seeds) ===\")\n",
    "    for k, vs in agg.items():\n",
    "        print(f\"{k:>26}: {vs['mean']:.4f} ± {vs['std']:.4f}\")\n",
    "\n",
    "    all_model_aggregates[model_name] = agg\n",
    "\n",
    "# Optional: write a single summary file across all models\n",
    "with open(OUT_ROOT / \"ALL_MODELS_aggregate_summary.json\", \"w\") as f:\n",
    "    json.dump(all_model_aggregates, f, indent=2)\n",
    "\n",
    "print(\"\\nAll done. Artifacts under:\", OUT_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd2e12c0-f8a0-4495-8a0a-b83a68aa13d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading GloVe ...\n",
      "[info] GloVe loaded.\n",
      "Domain counts before balancing:\n",
      "source\n",
      "Reddit     919475\n",
      "News        26597\n",
      "Twitter      4622\n",
      "Name: count, dtype: int64\n",
      "\n",
      "===== Balanced variant seed=0 =====\n",
      "  Twitter: balanced 50/50 → 2176 rows\n",
      "  Reddit: balanced 50/50 → 898548 rows\n",
      "  News: balanced 50/50 → 23294 rows\n",
      "  Equalized cap across domains → N per domain = 2176 (each class 1088)\n",
      "    > Twitter :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 144ms/step - accuracy: 0.5121 - loss: 0.6986 - val_accuracy: 0.4943 - val_loss: 0.6941\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5049 - loss: 0.6966 - val_accuracy: 0.5287 - val_loss: 0.6914\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5227 - loss: 0.6931 - val_accuracy: 0.5195 - val_loss: 0.6894\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5456 - loss: 0.6845 - val_accuracy: 0.5264 - val_loss: 0.6877\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5450 - loss: 0.6850 - val_accuracy: 0.5356 - val_loss: 0.6863\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5732 - loss: 0.6807 - val_accuracy: 0.5471 - val_loss: 0.6845\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5614 - loss: 0.6821 - val_accuracy: 0.5655 - val_loss: 0.6781\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5824 - loss: 0.6703 - val_accuracy: 0.5471 - val_loss: 0.6777\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5726 - loss: 0.6714 - val_accuracy: 0.5862 - val_loss: 0.6724\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6100 - loss: 0.6570 - val_accuracy: 0.5816 - val_loss: 0.6669\n",
      "Epoch 11/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6389 - loss: 0.6414 - val_accuracy: 0.5954 - val_loss: 0.6640\n",
      "Epoch 12/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6612 - loss: 0.6179 - val_accuracy: 0.5977 - val_loss: 0.6607\n",
      "Epoch 13/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6750 - loss: 0.6206 - val_accuracy: 0.5908 - val_loss: 0.6786\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 126ms/step - accuracy: 0.5082 - loss: 0.6935 - val_accuracy: 0.5448 - val_loss: 0.6887\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5259 - loss: 0.6926 - val_accuracy: 0.5632 - val_loss: 0.6874\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5213 - loss: 0.6925 - val_accuracy: 0.5517 - val_loss: 0.6837\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5529 - loss: 0.6837 - val_accuracy: 0.5770 - val_loss: 0.6814\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5535 - loss: 0.6824 - val_accuracy: 0.5747 - val_loss: 0.6787\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5588 - loss: 0.6807 - val_accuracy: 0.5563 - val_loss: 0.6773\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5785 - loss: 0.6736 - val_accuracy: 0.5701 - val_loss: 0.6734\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5916 - loss: 0.6687 - val_accuracy: 0.5931 - val_loss: 0.6692\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5785 - loss: 0.6702 - val_accuracy: 0.5862 - val_loss: 0.6689\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5949 - loss: 0.6595 - val_accuracy: 0.5448 - val_loss: 0.6693\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 107ms/stepWARNING:tensorflow:5 out of the last 717 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x42e574e00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 126ms/step - accuracy: 0.4846 - loss: 0.7121 - val_accuracy: 0.4966 - val_loss: 0.6953\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5246 - loss: 0.6955 - val_accuracy: 0.5655 - val_loss: 0.6887\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5292 - loss: 0.6903 - val_accuracy: 0.5908 - val_loss: 0.6862\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5391 - loss: 0.6874 - val_accuracy: 0.5678 - val_loss: 0.6856\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5522 - loss: 0.6844 - val_accuracy: 0.5701 - val_loss: 0.6837\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5535 - loss: 0.6830 - val_accuracy: 0.5885 - val_loss: 0.6798\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5673 - loss: 0.6776 - val_accuracy: 0.5793 - val_loss: 0.6763\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 30ms/step - accuracy: 0.5831 - loss: 0.6743 - val_accuracy: 0.5816 - val_loss: 0.6693\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 54ms/step - accuracy: 0.5693 - loss: 0.6708 - val_accuracy: 0.6230 - val_loss: 0.6650\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6041 - loss: 0.6621 - val_accuracy: 0.5471 - val_loss: 0.6748\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "WARNING:tensorflow:6 out of the last 718 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x430e5c0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "    > Twitter :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 137ms/step - accuracy: 0.5062 - loss: 0.6945 - val_accuracy: 0.5218 - val_loss: 0.6896\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5575 - loss: 0.6879 - val_accuracy: 0.5540 - val_loss: 0.6849\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5693 - loss: 0.6778 - val_accuracy: 0.5724 - val_loss: 0.6801\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5752 - loss: 0.6722 - val_accuracy: 0.5793 - val_loss: 0.6783\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5955 - loss: 0.6554 - val_accuracy: 0.5977 - val_loss: 0.6737\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6559 - loss: 0.6290 - val_accuracy: 0.5908 - val_loss: 0.6680\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6566 - loss: 0.6069 - val_accuracy: 0.5885 - val_loss: 0.6661\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6907 - loss: 0.5793 - val_accuracy: 0.5931 - val_loss: 0.6915\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 132ms/step - accuracy: 0.5056 - loss: 0.6931 - val_accuracy: 0.5103 - val_loss: 0.6898\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5338 - loss: 0.6898 - val_accuracy: 0.5563 - val_loss: 0.6853\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.5791 - loss: 0.6770 - val_accuracy: 0.5724 - val_loss: 0.6802\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.5680 - loss: 0.6719 - val_accuracy: 0.5770 - val_loss: 0.6756\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.6047 - loss: 0.6575 - val_accuracy: 0.5862 - val_loss: 0.6634\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6165 - loss: 0.6496 - val_accuracy: 0.5770 - val_loss: 0.6733\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 134ms/step - accuracy: 0.5030 - loss: 0.7035 - val_accuracy: 0.5356 - val_loss: 0.6905\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5181 - loss: 0.6924 - val_accuracy: 0.5724 - val_loss: 0.6867\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5561 - loss: 0.6838 - val_accuracy: 0.5563 - val_loss: 0.6822\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5673 - loss: 0.6779 - val_accuracy: 0.5701 - val_loss: 0.6750\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5870 - loss: 0.6648 - val_accuracy: 0.5908 - val_loss: 0.6663\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6106 - loss: 0.6515 - val_accuracy: 0.5931 - val_loss: 0.6603\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6494 - loss: 0.6289 - val_accuracy: 0.6092 - val_loss: 0.6557\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6763 - loss: 0.5921 - val_accuracy: 0.6023 - val_loss: 0.6483\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 48ms/step - accuracy: 0.7131 - loss: 0.5540 - val_accuracy: 0.5977 - val_loss: 0.6574\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "    > Twitter :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.5135 - loss: 0.6973 - val_accuracy: 0.4989 - val_loss: 0.6921\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.4898 - loss: 0.6940 - val_accuracy: 0.4966 - val_loss: 0.6931\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 134ms/step - accuracy: 0.4885 - loss: 0.6960 - val_accuracy: 0.5057 - val_loss: 0.6926\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5161 - loss: 0.6928 - val_accuracy: 0.5310 - val_loss: 0.6924\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5069 - loss: 0.6925 - val_accuracy: 0.5287 - val_loss: 0.6916\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5489 - loss: 0.6893 - val_accuracy: 0.5241 - val_loss: 0.6911\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5522 - loss: 0.6868 - val_accuracy: 0.5471 - val_loss: 0.6905\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5752 - loss: 0.6803 - val_accuracy: 0.5310 - val_loss: 0.6933\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 135ms/step - accuracy: 0.5082 - loss: 0.6960 - val_accuracy: 0.5011 - val_loss: 0.6942\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5154 - loss: 0.6921 - val_accuracy: 0.5356 - val_loss: 0.6915\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5299 - loss: 0.6916 - val_accuracy: 0.5333 - val_loss: 0.6903\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5535 - loss: 0.6870 - val_accuracy: 0.5218 - val_loss: 0.6905\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "    > Twitter :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 143ms/step - accuracy: 0.5259 - loss: 0.6950 - val_accuracy: 0.5540 - val_loss: 0.6919\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5522 - loss: 0.6900 - val_accuracy: 0.5609 - val_loss: 0.6909\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5358 - loss: 0.6889 - val_accuracy: 0.5333 - val_loss: 0.6904\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 51ms/step - accuracy: 0.5719 - loss: 0.6821 - val_accuracy: 0.5356 - val_loss: 0.6926\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 145ms/step - accuracy: 0.4826 - loss: 0.6957 - val_accuracy: 0.5333 - val_loss: 0.6920\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5246 - loss: 0.6924 - val_accuracy: 0.5057 - val_loss: 0.6928\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 145ms/step - accuracy: 0.5174 - loss: 0.6931 - val_accuracy: 0.5080 - val_loss: 0.6929\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5305 - loss: 0.6914 - val_accuracy: 0.5494 - val_loss: 0.6910\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5437 - loss: 0.6858 - val_accuracy: 0.5333 - val_loss: 0.6902\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5739 - loss: 0.6808 - val_accuracy: 0.5448 - val_loss: 0.6887\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5824 - loss: 0.6760 - val_accuracy: 0.5609 - val_loss: 0.6876\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6198 - loss: 0.6571 - val_accuracy: 0.5977 - val_loss: 0.6762\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6513 - loss: 0.6330 - val_accuracy: 0.5931 - val_loss: 0.6732\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6704 - loss: 0.6044 - val_accuracy: 0.5908 - val_loss: 0.6737\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "    > Twitter :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5128 - loss: 0.7950 - val_accuracy: 0.4782 - val_loss: 0.6947\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 17ms/step - accuracy: 0.5450 - loss: 0.6951 - val_accuracy: 0.5264 - val_loss: 0.6943\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5233 - loss: 0.6943 - val_accuracy: 0.4943 - val_loss: 0.6934\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5358 - loss: 0.6881 - val_accuracy: 0.5034 - val_loss: 0.6925\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5706 - loss: 0.6776 - val_accuracy: 0.5264 - val_loss: 0.6913\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5824 - loss: 0.6685 - val_accuracy: 0.5103 - val_loss: 0.6856\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6041 - loss: 0.6491 - val_accuracy: 0.5425 - val_loss: 0.6832\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6218 - loss: 0.6416 - val_accuracy: 0.5517 - val_loss: 0.6771\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6592 - loss: 0.6157 - val_accuracy: 0.5448 - val_loss: 0.6749\n",
      "Epoch 10/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.7026 - loss: 0.5677 - val_accuracy: 0.5678 - val_loss: 0.6776\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.4767 - loss: 0.8653 - val_accuracy: 0.4966 - val_loss: 0.7001\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5338 - loss: 0.6985 - val_accuracy: 0.5448 - val_loss: 0.6924\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.4944 - loss: 0.7026 - val_accuracy: 0.5126 - val_loss: 0.6913\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5089 - loss: 0.6953 - val_accuracy: 0.5218 - val_loss: 0.6923\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 68ms/step - accuracy: 0.4911 - loss: 0.9240 - val_accuracy: 0.4989 - val_loss: 0.7583\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5364 - loss: 0.7157 - val_accuracy: 0.4989 - val_loss: 0.6915\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5371 - loss: 0.6944 - val_accuracy: 0.5448 - val_loss: 0.6914\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5502 - loss: 0.6808 - val_accuracy: 0.5356 - val_loss: 0.6903\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5745 - loss: 0.6772 - val_accuracy: 0.5471 - val_loss: 0.6879\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6087 - loss: 0.6576 - val_accuracy: 0.5218 - val_loss: 0.6843\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5883 - loss: 0.6639 - val_accuracy: 0.5563 - val_loss: 0.6801\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6218 - loss: 0.6389 - val_accuracy: 0.5540 - val_loss: 0.6752\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6389 - loss: 0.6292 - val_accuracy: 0.5701 - val_loss: 0.6726\n",
      "Epoch 10/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6921 - loss: 0.5847 - val_accuracy: 0.5517 - val_loss: 0.6710\n",
      "Epoch 11/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.7012 - loss: 0.5667 - val_accuracy: 0.5494 - val_loss: 0.6680\n",
      "Epoch 12/20\n",
      "12/12 - 0s - 28ms/step - accuracy: 0.7518 - loss: 0.5106 - val_accuracy: 0.5816 - val_loss: 0.6672\n",
      "Epoch 13/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7781 - loss: 0.4759 - val_accuracy: 0.6023 - val_loss: 0.6610\n",
      "Epoch 14/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7997 - loss: 0.4388 - val_accuracy: 0.5977 - val_loss: 0.6513\n",
      "Epoch 15/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.8332 - loss: 0.3916 - val_accuracy: 0.5931 - val_loss: 0.6664\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "    > Twitter :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 91ms/step - accuracy: 0.5148 - loss: 0.7800 - val_accuracy: 0.5402 - val_loss: 0.6887\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5785 - loss: 0.6688 - val_accuracy: 0.5149 - val_loss: 0.6884\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5968 - loss: 0.6528 - val_accuracy: 0.5310 - val_loss: 0.6847\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6592 - loss: 0.6197 - val_accuracy: 0.5563 - val_loss: 0.6718\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7006 - loss: 0.5745 - val_accuracy: 0.5655 - val_loss: 0.6662\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.7525 - loss: 0.5100 - val_accuracy: 0.5747 - val_loss: 0.6724\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 93ms/step - accuracy: 0.4990 - loss: 0.7785 - val_accuracy: 0.5264 - val_loss: 0.6916\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5640 - loss: 0.6832 - val_accuracy: 0.5149 - val_loss: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6257 - loss: 0.6579 - val_accuracy: 0.5540 - val_loss: 0.6818\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6402 - loss: 0.6276 - val_accuracy: 0.5724 - val_loss: 0.6666\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6940 - loss: 0.5697 - val_accuracy: 0.5862 - val_loss: 0.6590\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7590 - loss: 0.4984 - val_accuracy: 0.5908 - val_loss: 0.6503\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.8339 - loss: 0.4004 - val_accuracy: 0.5862 - val_loss: 0.6590\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 92ms/step - accuracy: 0.4918 - loss: 0.8673 - val_accuracy: 0.4989 - val_loss: 0.7059\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5568 - loss: 0.6905 - val_accuracy: 0.5333 - val_loss: 0.6914\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5962 - loss: 0.6692 - val_accuracy: 0.5379 - val_loss: 0.6856\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6395 - loss: 0.6329 - val_accuracy: 0.5471 - val_loss: 0.6749\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6986 - loss: 0.5929 - val_accuracy: 0.5494 - val_loss: 0.6650\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7380 - loss: 0.5303 - val_accuracy: 0.5747 - val_loss: 0.6722\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "    > Reddit :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 126ms/step - accuracy: 0.4892 - loss: 0.6995 - val_accuracy: 0.5126 - val_loss: 0.6912\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5115 - loss: 0.6943 - val_accuracy: 0.5586 - val_loss: 0.6886\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5437 - loss: 0.6906 - val_accuracy: 0.6092 - val_loss: 0.6875\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5660 - loss: 0.6799 - val_accuracy: 0.5494 - val_loss: 0.6879\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 124ms/step - accuracy: 0.5141 - loss: 0.6955 - val_accuracy: 0.5379 - val_loss: 0.6902\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5266 - loss: 0.6900 - val_accuracy: 0.5609 - val_loss: 0.6874\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5371 - loss: 0.6880 - val_accuracy: 0.5448 - val_loss: 0.6871\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5660 - loss: 0.6787 - val_accuracy: 0.5563 - val_loss: 0.6840\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5791 - loss: 0.6775 - val_accuracy: 0.5655 - val_loss: 0.6777\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5916 - loss: 0.6704 - val_accuracy: 0.5931 - val_loss: 0.6721\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6034 - loss: 0.6652 - val_accuracy: 0.5908 - val_loss: 0.6691\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 30ms/step - accuracy: 0.6257 - loss: 0.6505 - val_accuracy: 0.6207 - val_loss: 0.6638\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6290 - loss: 0.6487 - val_accuracy: 0.6046 - val_loss: 0.6560\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6513 - loss: 0.6340 - val_accuracy: 0.5839 - val_loss: 0.6593\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 156ms/step - accuracy: 0.5148 - loss: 0.7029 - val_accuracy: 0.4989 - val_loss: 0.6995\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.4997 - loss: 0.6961 - val_accuracy: 0.5057 - val_loss: 0.6932\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5496 - loss: 0.6858 - val_accuracy: 0.5494 - val_loss: 0.6893\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5364 - loss: 0.6841 - val_accuracy: 0.5517 - val_loss: 0.6892\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5693 - loss: 0.6760 - val_accuracy: 0.5540 - val_loss: 0.6884\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5706 - loss: 0.6730 - val_accuracy: 0.5839 - val_loss: 0.6801\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5896 - loss: 0.6673 - val_accuracy: 0.6046 - val_loss: 0.6750\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 30ms/step - accuracy: 0.6179 - loss: 0.6562 - val_accuracy: 0.6092 - val_loss: 0.6695\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6244 - loss: 0.6473 - val_accuracy: 0.6000 - val_loss: 0.6673\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6454 - loss: 0.6359 - val_accuracy: 0.6345 - val_loss: 0.6591\n",
      "Epoch 11/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6599 - loss: 0.6239 - val_accuracy: 0.6046 - val_loss: 0.6621\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "    > Reddit :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 139ms/step - accuracy: 0.4833 - loss: 0.6959 - val_accuracy: 0.5103 - val_loss: 0.6931\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5332 - loss: 0.6911 - val_accuracy: 0.5701 - val_loss: 0.6884\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5555 - loss: 0.6847 - val_accuracy: 0.5839 - val_loss: 0.6830\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6146 - loss: 0.6656 - val_accuracy: 0.5793 - val_loss: 0.6804\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6067 - loss: 0.6648 - val_accuracy: 0.6138 - val_loss: 0.6656\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6402 - loss: 0.6459 - val_accuracy: 0.6184 - val_loss: 0.6616\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6474 - loss: 0.6305 - val_accuracy: 0.6023 - val_loss: 0.6656\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 133ms/step - accuracy: 0.5095 - loss: 0.6940 - val_accuracy: 0.5103 - val_loss: 0.6915\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5502 - loss: 0.6856 - val_accuracy: 0.5471 - val_loss: 0.6855\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5798 - loss: 0.6734 - val_accuracy: 0.5908 - val_loss: 0.6779\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6067 - loss: 0.6629 - val_accuracy: 0.5931 - val_loss: 0.6699\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6159 - loss: 0.6537 - val_accuracy: 0.5816 - val_loss: 0.6662\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6290 - loss: 0.6459 - val_accuracy: 0.5793 - val_loss: 0.6645\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6559 - loss: 0.6232 - val_accuracy: 0.5954 - val_loss: 0.6616\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.6651 - loss: 0.6066 - val_accuracy: 0.6092 - val_loss: 0.6504\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6960 - loss: 0.5776 - val_accuracy: 0.6115 - val_loss: 0.6690\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 142ms/step - accuracy: 0.5076 - loss: 0.6988 - val_accuracy: 0.5126 - val_loss: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5588 - loss: 0.6846 - val_accuracy: 0.5770 - val_loss: 0.6878\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5607 - loss: 0.6778 - val_accuracy: 0.5931 - val_loss: 0.6825\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5863 - loss: 0.6682 - val_accuracy: 0.6138 - val_loss: 0.6753\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6172 - loss: 0.6518 - val_accuracy: 0.6069 - val_loss: 0.6548\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6573 - loss: 0.6215 - val_accuracy: 0.6368 - val_loss: 0.6424\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6940 - loss: 0.5949 - val_accuracy: 0.5655 - val_loss: 0.7194\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "    > Reddit :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 135ms/step - accuracy: 0.5043 - loss: 0.6960 - val_accuracy: 0.4966 - val_loss: 0.6919\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 30ms/step - accuracy: 0.5082 - loss: 0.6919 - val_accuracy: 0.5448 - val_loss: 0.6923\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.4951 - loss: 0.6952 - val_accuracy: 0.5218 - val_loss: 0.6929\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5076 - loss: 0.6918 - val_accuracy: 0.5425 - val_loss: 0.6918\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5148 - loss: 0.6919 - val_accuracy: 0.4989 - val_loss: 0.6906\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5529 - loss: 0.6897 - val_accuracy: 0.5770 - val_loss: 0.6878\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5640 - loss: 0.6853 - val_accuracy: 0.5724 - val_loss: 0.6811\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5936 - loss: 0.6740 - val_accuracy: 0.5632 - val_loss: 0.6898\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 135ms/step - accuracy: 0.4905 - loss: 0.6949 - val_accuracy: 0.4989 - val_loss: 0.6933\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.4918 - loss: 0.6940 - val_accuracy: 0.5172 - val_loss: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5332 - loss: 0.6911 - val_accuracy: 0.5310 - val_loss: 0.6920\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5246 - loss: 0.6906 - val_accuracy: 0.5241 - val_loss: 0.6917\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5279 - loss: 0.6875 - val_accuracy: 0.5632 - val_loss: 0.6885\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5410 - loss: 0.6818 - val_accuracy: 0.5747 - val_loss: 0.6808\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5620 - loss: 0.6783 - val_accuracy: 0.5793 - val_loss: 0.6727\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.6021 - loss: 0.6643 - val_accuracy: 0.5862 - val_loss: 0.6828\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "    > Reddit :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 143ms/step - accuracy: 0.5207 - loss: 0.6937 - val_accuracy: 0.5770 - val_loss: 0.6911\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.5476 - loss: 0.6875 - val_accuracy: 0.5678 - val_loss: 0.6867\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.5712 - loss: 0.6788 - val_accuracy: 0.5747 - val_loss: 0.6741\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5949 - loss: 0.6662 - val_accuracy: 0.5816 - val_loss: 0.6734\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6271 - loss: 0.6460 - val_accuracy: 0.6092 - val_loss: 0.6637\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 42ms/step - accuracy: 0.6645 - loss: 0.6177 - val_accuracy: 0.6161 - val_loss: 0.6528\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 53ms/step - accuracy: 0.6835 - loss: 0.5872 - val_accuracy: 0.6253 - val_loss: 0.6418\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7209 - loss: 0.5537 - val_accuracy: 0.6345 - val_loss: 0.6543\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 147ms/step - accuracy: 0.4741 - loss: 0.6960 - val_accuracy: 0.5149 - val_loss: 0.6920\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5325 - loss: 0.6911 - val_accuracy: 0.5287 - val_loss: 0.6911\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5601 - loss: 0.6873 - val_accuracy: 0.5724 - val_loss: 0.6866\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5778 - loss: 0.6769 - val_accuracy: 0.6023 - val_loss: 0.6712\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5929 - loss: 0.6680 - val_accuracy: 0.5862 - val_loss: 0.6747\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 4s - 299ms/step - accuracy: 0.5213 - loss: 0.6923 - val_accuracy: 0.5011 - val_loss: 0.6930\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 49ms/step - accuracy: 0.5148 - loss: 0.6918 - val_accuracy: 0.5103 - val_loss: 0.6918\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5430 - loss: 0.6867 - val_accuracy: 0.5586 - val_loss: 0.6871\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5693 - loss: 0.6772 - val_accuracy: 0.5931 - val_loss: 0.6746\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5936 - loss: 0.6663 - val_accuracy: 0.5977 - val_loss: 0.6628\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6408 - loss: 0.6323 - val_accuracy: 0.6023 - val_loss: 0.6535\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6619 - loss: 0.6102 - val_accuracy: 0.6046 - val_loss: 0.6468\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6822 - loss: 0.5845 - val_accuracy: 0.6115 - val_loss: 0.6611\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "    > Reddit :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5089 - loss: 0.7909 - val_accuracy: 0.5034 - val_loss: 0.6921\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5305 - loss: 0.6959 - val_accuracy: 0.5471 - val_loss: 0.6868\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5351 - loss: 0.6945 - val_accuracy: 0.5724 - val_loss: 0.6837\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5732 - loss: 0.6785 - val_accuracy: 0.5770 - val_loss: 0.6840\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.4721 - loss: 0.8738 - val_accuracy: 0.5149 - val_loss: 0.6976\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5338 - loss: 0.6990 - val_accuracy: 0.4851 - val_loss: 0.6937\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5076 - loss: 0.6954 - val_accuracy: 0.5241 - val_loss: 0.6932\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5384 - loss: 0.6886 - val_accuracy: 0.5218 - val_loss: 0.6926\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5601 - loss: 0.6839 - val_accuracy: 0.5379 - val_loss: 0.6916\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5581 - loss: 0.6854 - val_accuracy: 0.5540 - val_loss: 0.6868\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6074 - loss: 0.6633 - val_accuracy: 0.5770 - val_loss: 0.6786\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6113 - loss: 0.6664 - val_accuracy: 0.5839 - val_loss: 0.6765\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6428 - loss: 0.6413 - val_accuracy: 0.5793 - val_loss: 0.6740\n",
      "Epoch 10/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6441 - loss: 0.6188 - val_accuracy: 0.6069 - val_loss: 0.6637\n",
      "Epoch 11/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6783 - loss: 0.5994 - val_accuracy: 0.6299 - val_loss: 0.6582\n",
      "Epoch 12/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7124 - loss: 0.5621 - val_accuracy: 0.6391 - val_loss: 0.6464\n",
      "Epoch 13/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7347 - loss: 0.5393 - val_accuracy: 0.6207 - val_loss: 0.6546\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 80ms/step - accuracy: 0.4944 - loss: 0.8645 - val_accuracy: 0.4989 - val_loss: 0.7180\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5489 - loss: 0.7073 - val_accuracy: 0.5333 - val_loss: 0.6902\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5483 - loss: 0.6933 - val_accuracy: 0.5333 - val_loss: 0.6891\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5634 - loss: 0.6782 - val_accuracy: 0.5632 - val_loss: 0.6852\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5614 - loss: 0.6797 - val_accuracy: 0.6092 - val_loss: 0.6813\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6126 - loss: 0.6624 - val_accuracy: 0.6046 - val_loss: 0.6751\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5929 - loss: 0.6582 - val_accuracy: 0.5701 - val_loss: 0.6717\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6336 - loss: 0.6419 - val_accuracy: 0.5931 - val_loss: 0.6686\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6605 - loss: 0.6196 - val_accuracy: 0.5770 - val_loss: 0.6685\n",
      "Epoch 10/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6855 - loss: 0.5928 - val_accuracy: 0.5839 - val_loss: 0.6614\n",
      "Epoch 11/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7131 - loss: 0.5631 - val_accuracy: 0.5839 - val_loss: 0.6605\n",
      "Epoch 12/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7610 - loss: 0.5136 - val_accuracy: 0.5862 - val_loss: 0.6690\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "    > Reddit :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 97ms/step - accuracy: 0.5154 - loss: 0.7766 - val_accuracy: 0.5517 - val_loss: 0.6901\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5837 - loss: 0.6738 - val_accuracy: 0.5540 - val_loss: 0.6886\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5942 - loss: 0.6642 - val_accuracy: 0.5793 - val_loss: 0.6802\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6664 - loss: 0.6281 - val_accuracy: 0.5954 - val_loss: 0.6647\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.7124 - loss: 0.5805 - val_accuracy: 0.6207 - val_loss: 0.6530\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7479 - loss: 0.5209 - val_accuracy: 0.6414 - val_loss: 0.6517\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8037 - loss: 0.4324 - val_accuracy: 0.6138 - val_loss: 0.6564\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 93ms/step - accuracy: 0.4924 - loss: 0.7798 - val_accuracy: 0.5310 - val_loss: 0.6896\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5739 - loss: 0.6758 - val_accuracy: 0.5586 - val_loss: 0.6870\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5824 - loss: 0.6677 - val_accuracy: 0.5862 - val_loss: 0.6838\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6487 - loss: 0.6402 - val_accuracy: 0.5954 - val_loss: 0.6721\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6868 - loss: 0.5993 - val_accuracy: 0.6161 - val_loss: 0.6647\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7328 - loss: 0.5390 - val_accuracy: 0.6184 - val_loss: 0.6566\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8207 - loss: 0.4395 - val_accuracy: 0.6299 - val_loss: 0.6591\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 104ms/step - accuracy: 0.4898 - loss: 0.8235 - val_accuracy: 0.5241 - val_loss: 0.7052\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5660 - loss: 0.6820 - val_accuracy: 0.5195 - val_loss: 0.6898\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6120 - loss: 0.6581 - val_accuracy: 0.5770 - val_loss: 0.6791\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6671 - loss: 0.6170 - val_accuracy: 0.5816 - val_loss: 0.6693\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6967 - loss: 0.5774 - val_accuracy: 0.5977 - val_loss: 0.6571\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.7774 - loss: 0.5006 - val_accuracy: 0.5977 - val_loss: 0.6574\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "    > News :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 127ms/step - accuracy: 0.4944 - loss: 0.6998 - val_accuracy: 0.6207 - val_loss: 0.6842\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5588 - loss: 0.6837 - val_accuracy: 0.6368 - val_loss: 0.6740\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6008 - loss: 0.6678 - val_accuracy: 0.6897 - val_loss: 0.6522\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6796 - loss: 0.6258 - val_accuracy: 0.7172 - val_loss: 0.5769\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7295 - loss: 0.5688 - val_accuracy: 0.7172 - val_loss: 0.5421\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7518 - loss: 0.5073 - val_accuracy: 0.7172 - val_loss: 0.5389\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.7899 - loss: 0.4612 - val_accuracy: 0.7632 - val_loss: 0.5013\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.8234 - loss: 0.4226 - val_accuracy: 0.7586 - val_loss: 0.5147\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 127ms/step - accuracy: 0.5364 - loss: 0.6904 - val_accuracy: 0.5862 - val_loss: 0.6845\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5496 - loss: 0.6812 - val_accuracy: 0.6529 - val_loss: 0.6700\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6428 - loss: 0.6512 - val_accuracy: 0.6598 - val_loss: 0.6354\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6855 - loss: 0.6039 - val_accuracy: 0.6621 - val_loss: 0.6102\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7446 - loss: 0.5457 - val_accuracy: 0.6713 - val_loss: 0.5976\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7649 - loss: 0.4992 - val_accuracy: 0.7034 - val_loss: 0.5654\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7912 - loss: 0.4513 - val_accuracy: 0.7011 - val_loss: 0.5752\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 123ms/step - accuracy: 0.4944 - loss: 0.7027 - val_accuracy: 0.4989 - val_loss: 0.6910\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5489 - loss: 0.6865 - val_accuracy: 0.5172 - val_loss: 0.6826\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5831 - loss: 0.6756 - val_accuracy: 0.6276 - val_loss: 0.6672\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6349 - loss: 0.6579 - val_accuracy: 0.7011 - val_loss: 0.6434\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6651 - loss: 0.6267 - val_accuracy: 0.7034 - val_loss: 0.6143\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7196 - loss: 0.5778 - val_accuracy: 0.7494 - val_loss: 0.5533\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7420 - loss: 0.5273 - val_accuracy: 0.7379 - val_loss: 0.5249\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 30ms/step - accuracy: 0.7702 - loss: 0.4830 - val_accuracy: 0.7609 - val_loss: 0.5204\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.8050 - loss: 0.4500 - val_accuracy: 0.7724 - val_loss: 0.5011\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8050 - loss: 0.4370 - val_accuracy: 0.7724 - val_loss: 0.4984\n",
      "Epoch 11/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.8319 - loss: 0.3973 - val_accuracy: 0.7885 - val_loss: 0.5041\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "    > News :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 133ms/step - accuracy: 0.5056 - loss: 0.6917 - val_accuracy: 0.5977 - val_loss: 0.6821\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6001 - loss: 0.6685 - val_accuracy: 0.6345 - val_loss: 0.6573\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.6881 - loss: 0.6248 - val_accuracy: 0.7103 - val_loss: 0.6011\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.7617 - loss: 0.5329 - val_accuracy: 0.7218 - val_loss: 0.5652\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.8056 - loss: 0.4524 - val_accuracy: 0.7425 - val_loss: 0.5400\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.8332 - loss: 0.3823 - val_accuracy: 0.7609 - val_loss: 0.5035\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.8654 - loss: 0.3327 - val_accuracy: 0.7333 - val_loss: 0.5714\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 134ms/step - accuracy: 0.5272 - loss: 0.6887 - val_accuracy: 0.6253 - val_loss: 0.6815\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6257 - loss: 0.6618 - val_accuracy: 0.6989 - val_loss: 0.6440\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.7308 - loss: 0.5835 - val_accuracy: 0.6207 - val_loss: 0.6491\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 135ms/step - accuracy: 0.5391 - loss: 0.6929 - val_accuracy: 0.5701 - val_loss: 0.6768\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.6573 - loss: 0.6554 - val_accuracy: 0.6989 - val_loss: 0.6412\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.7157 - loss: 0.5883 - val_accuracy: 0.6874 - val_loss: 0.5776\n",
      "Epoch 4/20\n",
      "12/12 - 2s - 172ms/step - accuracy: 0.7886 - loss: 0.4837 - val_accuracy: 0.7471 - val_loss: 0.5229\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.8306 - loss: 0.4189 - val_accuracy: 0.7379 - val_loss: 0.5245\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "    > News :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 142ms/step - accuracy: 0.5187 - loss: 0.6925 - val_accuracy: 0.5632 - val_loss: 0.6869\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5791 - loss: 0.6842 - val_accuracy: 0.6667 - val_loss: 0.6638\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.6573 - loss: 0.6475 - val_accuracy: 0.7172 - val_loss: 0.5948\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7229 - loss: 0.5535 - val_accuracy: 0.7402 - val_loss: 0.5265\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7754 - loss: 0.5034 - val_accuracy: 0.7540 - val_loss: 0.5302\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 145ms/step - accuracy: 0.5003 - loss: 0.6933 - val_accuracy: 0.6460 - val_loss: 0.6894\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5778 - loss: 0.6851 - val_accuracy: 0.6529 - val_loss: 0.6806\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5968 - loss: 0.6716 - val_accuracy: 0.6874 - val_loss: 0.6547\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.6691 - loss: 0.6319 - val_accuracy: 0.7195 - val_loss: 0.6012\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.7177 - loss: 0.5709 - val_accuracy: 0.6920 - val_loss: 0.5850\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.7689 - loss: 0.5056 - val_accuracy: 0.7448 - val_loss: 0.5106\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.7925 - loss: 0.4679 - val_accuracy: 0.7540 - val_loss: 0.5104\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.8168 - loss: 0.4490 - val_accuracy: 0.7356 - val_loss: 0.5488\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 137ms/step - accuracy: 0.5135 - loss: 0.6923 - val_accuracy: 0.5356 - val_loss: 0.6895\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5443 - loss: 0.6877 - val_accuracy: 0.6483 - val_loss: 0.6800\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.6277 - loss: 0.6646 - val_accuracy: 0.6943 - val_loss: 0.6420\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6697 - loss: 0.6140 - val_accuracy: 0.7287 - val_loss: 0.5624\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.7597 - loss: 0.5310 - val_accuracy: 0.7310 - val_loss: 0.5413\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.7754 - loss: 0.4830 - val_accuracy: 0.7287 - val_loss: 0.5407\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.8050 - loss: 0.4481 - val_accuracy: 0.7563 - val_loss: 0.5169\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.8253 - loss: 0.4165 - val_accuracy: 0.7448 - val_loss: 0.5423\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "    > News :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 145ms/step - accuracy: 0.5272 - loss: 0.6907 - val_accuracy: 0.6253 - val_loss: 0.6839\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6290 - loss: 0.6649 - val_accuracy: 0.6897 - val_loss: 0.6343\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.7085 - loss: 0.5930 - val_accuracy: 0.7218 - val_loss: 0.5421\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.7853 - loss: 0.4854 - val_accuracy: 0.7448 - val_loss: 0.5049\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 47ms/step - accuracy: 0.8273 - loss: 0.4022 - val_accuracy: 0.7563 - val_loss: 0.5026\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8720 - loss: 0.3364 - val_accuracy: 0.7448 - val_loss: 0.5286\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 146ms/step - accuracy: 0.5082 - loss: 0.6920 - val_accuracy: 0.6460 - val_loss: 0.6850\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6389 - loss: 0.6737 - val_accuracy: 0.7103 - val_loss: 0.6498\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6993 - loss: 0.6090 - val_accuracy: 0.7218 - val_loss: 0.5609\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7787 - loss: 0.5042 - val_accuracy: 0.7310 - val_loss: 0.5473\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8155 - loss: 0.4266 - val_accuracy: 0.7264 - val_loss: 0.5670\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 147ms/step - accuracy: 0.5463 - loss: 0.6891 - val_accuracy: 0.6276 - val_loss: 0.6823\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6152 - loss: 0.6639 - val_accuracy: 0.6989 - val_loss: 0.6420\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6953 - loss: 0.6019 - val_accuracy: 0.7402 - val_loss: 0.5662\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7754 - loss: 0.4990 - val_accuracy: 0.7609 - val_loss: 0.5253\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8299 - loss: 0.4121 - val_accuracy: 0.7517 - val_loss: 0.5349\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "    > News :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.5049 - loss: 0.7985 - val_accuracy: 0.5632 - val_loss: 0.6812\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5949 - loss: 0.6646 - val_accuracy: 0.6506 - val_loss: 0.6701\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6146 - loss: 0.6534 - val_accuracy: 0.6736 - val_loss: 0.6462\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6343 - loss: 0.6340 - val_accuracy: 0.6621 - val_loss: 0.6202\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6737 - loss: 0.5899 - val_accuracy: 0.6828 - val_loss: 0.5981\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7269 - loss: 0.5281 - val_accuracy: 0.6897 - val_loss: 0.5765\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7787 - loss: 0.4734 - val_accuracy: 0.7126 - val_loss: 0.5549\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.8076 - loss: 0.4224 - val_accuracy: 0.7149 - val_loss: 0.5420\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.8398 - loss: 0.3733 - val_accuracy: 0.7333 - val_loss: 0.5276\n",
      "Epoch 10/20\n",
      "12/12 - 0s - 20ms/step - accuracy: 0.8838 - loss: 0.2967 - val_accuracy: 0.7356 - val_loss: 0.5211\n",
      "Epoch 11/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.8884 - loss: 0.2702 - val_accuracy: 0.7310 - val_loss: 0.5299\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5069 - loss: 0.8375 - val_accuracy: 0.5057 - val_loss: 0.6897\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5666 - loss: 0.6856 - val_accuracy: 0.5425 - val_loss: 0.6910\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 71ms/step - accuracy: 0.4885 - loss: 0.8929 - val_accuracy: 0.5333 - val_loss: 0.6859\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5857 - loss: 0.6862 - val_accuracy: 0.6575 - val_loss: 0.6717\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6120 - loss: 0.6582 - val_accuracy: 0.6690 - val_loss: 0.6697\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6520 - loss: 0.6286 - val_accuracy: 0.6966 - val_loss: 0.6404\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6802 - loss: 0.6073 - val_accuracy: 0.6966 - val_loss: 0.6067\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7183 - loss: 0.5533 - val_accuracy: 0.7080 - val_loss: 0.5759\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 22ms/step - accuracy: 0.7354 - loss: 0.5281 - val_accuracy: 0.7011 - val_loss: 0.5562\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 21ms/step - accuracy: 0.7833 - loss: 0.4769 - val_accuracy: 0.7126 - val_loss: 0.5413\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 20ms/step - accuracy: 0.8175 - loss: 0.4178 - val_accuracy: 0.7172 - val_loss: 0.5438\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "    > News :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 95ms/step - accuracy: 0.5318 - loss: 0.7693 - val_accuracy: 0.6253 - val_loss: 0.6635\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6546 - loss: 0.6203 - val_accuracy: 0.6276 - val_loss: 0.6433\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6907 - loss: 0.5770 - val_accuracy: 0.6989 - val_loss: 0.5813\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7781 - loss: 0.4795 - val_accuracy: 0.7264 - val_loss: 0.5371\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8411 - loss: 0.3891 - val_accuracy: 0.7494 - val_loss: 0.5056\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8936 - loss: 0.2795 - val_accuracy: 0.7494 - val_loss: 0.4983\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.9192 - loss: 0.2122 - val_accuracy: 0.7701 - val_loss: 0.4874\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.9468 - loss: 0.1552 - val_accuracy: 0.7724 - val_loss: 0.4767\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.9685 - loss: 0.1109 - val_accuracy: 0.7678 - val_loss: 0.4976\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 95ms/step - accuracy: 0.5069 - loss: 0.7754 - val_accuracy: 0.6667 - val_loss: 0.6676\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6500 - loss: 0.6270 - val_accuracy: 0.6690 - val_loss: 0.6533\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 47ms/step - accuracy: 0.7242 - loss: 0.5642 - val_accuracy: 0.7057 - val_loss: 0.5915\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7702 - loss: 0.4764 - val_accuracy: 0.7241 - val_loss: 0.5440\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8240 - loss: 0.3903 - val_accuracy: 0.7402 - val_loss: 0.5212\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8851 - loss: 0.2795 - val_accuracy: 0.7356 - val_loss: 0.5256\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5246 - loss: 0.8293 - val_accuracy: 0.5632 - val_loss: 0.6696\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6395 - loss: 0.6411 - val_accuracy: 0.6138 - val_loss: 0.6557\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7137 - loss: 0.5725 - val_accuracy: 0.7057 - val_loss: 0.5970\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.7761 - loss: 0.4766 - val_accuracy: 0.7310 - val_loss: 0.5433\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8148 - loss: 0.4070 - val_accuracy: 0.7494 - val_loss: 0.5165\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8798 - loss: 0.3018 - val_accuracy: 0.7609 - val_loss: 0.5029\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.9041 - loss: 0.2447 - val_accuracy: 0.7586 - val_loss: 0.4993\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.9350 - loss: 0.1964 - val_accuracy: 0.7586 - val_loss: 0.5074\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\n",
      "===== Balanced variant seed=1 =====\n",
      "  Twitter: balanced 50/50 → 2176 rows\n",
      "  Reddit: balanced 50/50 → 898548 rows\n",
      "  News: balanced 50/50 → 23294 rows\n",
      "  Equalized cap across domains → N per domain = 2176 (each class 1088)\n",
      "    > Twitter :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 4s - 315ms/step - accuracy: 0.4879 - loss: 0.7067 - val_accuracy: 0.5172 - val_loss: 0.6921\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5095 - loss: 0.6934 - val_accuracy: 0.5517 - val_loss: 0.6909\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5286 - loss: 0.6895 - val_accuracy: 0.5126 - val_loss: 0.6915\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 125ms/step - accuracy: 0.5016 - loss: 0.7003 - val_accuracy: 0.5080 - val_loss: 0.6941\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5069 - loss: 0.6932 - val_accuracy: 0.5218 - val_loss: 0.6926\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5049 - loss: 0.6952 - val_accuracy: 0.5402 - val_loss: 0.6905\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5345 - loss: 0.6909 - val_accuracy: 0.5241 - val_loss: 0.6896\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5443 - loss: 0.6868 - val_accuracy: 0.5287 - val_loss: 0.6881\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5292 - loss: 0.6883 - val_accuracy: 0.5655 - val_loss: 0.6872\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5712 - loss: 0.6833 - val_accuracy: 0.5563 - val_loss: 0.6849\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5561 - loss: 0.6801 - val_accuracy: 0.5816 - val_loss: 0.6851\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 125ms/step - accuracy: 0.4800 - loss: 0.7094 - val_accuracy: 0.4966 - val_loss: 0.6969\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5121 - loss: 0.6933 - val_accuracy: 0.5034 - val_loss: 0.6919\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5305 - loss: 0.6916 - val_accuracy: 0.5218 - val_loss: 0.6914\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5102 - loss: 0.6930 - val_accuracy: 0.5310 - val_loss: 0.6912\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5167 - loss: 0.6922 - val_accuracy: 0.5310 - val_loss: 0.6864\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5410 - loss: 0.6893 - val_accuracy: 0.5379 - val_loss: 0.6869\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "    > Twitter :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 135ms/step - accuracy: 0.5003 - loss: 0.6992 - val_accuracy: 0.5149 - val_loss: 0.6908\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5397 - loss: 0.6903 - val_accuracy: 0.5471 - val_loss: 0.6891\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5338 - loss: 0.6877 - val_accuracy: 0.5333 - val_loss: 0.6860\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5660 - loss: 0.6812 - val_accuracy: 0.5931 - val_loss: 0.6803\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.5968 - loss: 0.6699 - val_accuracy: 0.6046 - val_loss: 0.6732\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.5982 - loss: 0.6638 - val_accuracy: 0.5517 - val_loss: 0.6773\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 132ms/step - accuracy: 0.5049 - loss: 0.6962 - val_accuracy: 0.5563 - val_loss: 0.6878\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5227 - loss: 0.6893 - val_accuracy: 0.5678 - val_loss: 0.6857\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5831 - loss: 0.6786 - val_accuracy: 0.5747 - val_loss: 0.6802\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5719 - loss: 0.6763 - val_accuracy: 0.5747 - val_loss: 0.6744\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5942 - loss: 0.6680 - val_accuracy: 0.5701 - val_loss: 0.6701\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6323 - loss: 0.6472 - val_accuracy: 0.5655 - val_loss: 0.6730\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 137ms/step - accuracy: 0.4813 - loss: 0.7046 - val_accuracy: 0.5241 - val_loss: 0.6936\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5430 - loss: 0.6905 - val_accuracy: 0.5356 - val_loss: 0.6889\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.5489 - loss: 0.6838 - val_accuracy: 0.5563 - val_loss: 0.6854\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.5515 - loss: 0.6829 - val_accuracy: 0.5632 - val_loss: 0.6824\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.5837 - loss: 0.6726 - val_accuracy: 0.5517 - val_loss: 0.6765\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.6074 - loss: 0.6580 - val_accuracy: 0.5816 - val_loss: 0.6747\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.6323 - loss: 0.6462 - val_accuracy: 0.5770 - val_loss: 0.6711\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6704 - loss: 0.6188 - val_accuracy: 0.5724 - val_loss: 0.6798\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "    > Twitter :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 141ms/step - accuracy: 0.4885 - loss: 0.6988 - val_accuracy: 0.5126 - val_loss: 0.6930\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5102 - loss: 0.6924 - val_accuracy: 0.5471 - val_loss: 0.6921\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.5535 - loss: 0.6883 - val_accuracy: 0.5333 - val_loss: 0.6910\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5286 - loss: 0.6924 - val_accuracy: 0.5678 - val_loss: 0.6901\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5548 - loss: 0.6864 - val_accuracy: 0.5747 - val_loss: 0.6897\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5561 - loss: 0.6828 - val_accuracy: 0.5747 - val_loss: 0.6867\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5883 - loss: 0.6799 - val_accuracy: 0.5471 - val_loss: 0.6819\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6074 - loss: 0.6584 - val_accuracy: 0.5471 - val_loss: 0.6923\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 143ms/step - accuracy: 0.4905 - loss: 0.6954 - val_accuracy: 0.5103 - val_loss: 0.6930\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5200 - loss: 0.6921 - val_accuracy: 0.4966 - val_loss: 0.6930\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.4997 - loss: 0.6933 - val_accuracy: 0.4989 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5272 - loss: 0.6920 - val_accuracy: 0.5494 - val_loss: 0.6917\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5220 - loss: 0.6914 - val_accuracy: 0.5471 - val_loss: 0.6899\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5575 - loss: 0.6877 - val_accuracy: 0.5678 - val_loss: 0.6890\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5739 - loss: 0.6832 - val_accuracy: 0.5448 - val_loss: 0.6910\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.5233 - loss: 0.6921 - val_accuracy: 0.5149 - val_loss: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5121 - loss: 0.6963 - val_accuracy: 0.5333 - val_loss: 0.6917\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5220 - loss: 0.6917 - val_accuracy: 0.5448 - val_loss: 0.6916\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5561 - loss: 0.6899 - val_accuracy: 0.5494 - val_loss: 0.6905\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5555 - loss: 0.6886 - val_accuracy: 0.5471 - val_loss: 0.6879\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5509 - loss: 0.6855 - val_accuracy: 0.5379 - val_loss: 0.6865\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5778 - loss: 0.6799 - val_accuracy: 0.5701 - val_loss: 0.6844\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5949 - loss: 0.6682 - val_accuracy: 0.5724 - val_loss: 0.6886\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "    > Twitter :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 147ms/step - accuracy: 0.5016 - loss: 0.6966 - val_accuracy: 0.4989 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5056 - loss: 0.6924 - val_accuracy: 0.5172 - val_loss: 0.6921\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5535 - loss: 0.6903 - val_accuracy: 0.5908 - val_loss: 0.6906\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5758 - loss: 0.6863 - val_accuracy: 0.5517 - val_loss: 0.6876\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6001 - loss: 0.6766 - val_accuracy: 0.5655 - val_loss: 0.6837\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5995 - loss: 0.6678 - val_accuracy: 0.5563 - val_loss: 0.6845\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 153ms/step - accuracy: 0.4957 - loss: 0.6954 - val_accuracy: 0.4989 - val_loss: 0.6936\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5115 - loss: 0.6935 - val_accuracy: 0.5402 - val_loss: 0.6926\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5712 - loss: 0.6899 - val_accuracy: 0.5333 - val_loss: 0.6909\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5706 - loss: 0.6865 - val_accuracy: 0.5494 - val_loss: 0.6884\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6074 - loss: 0.6711 - val_accuracy: 0.5609 - val_loss: 0.6899\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 144ms/step - accuracy: 0.4865 - loss: 0.6919 - val_accuracy: 0.5517 - val_loss: 0.6922\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5266 - loss: 0.6914 - val_accuracy: 0.5379 - val_loss: 0.6909\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5345 - loss: 0.6905 - val_accuracy: 0.5678 - val_loss: 0.6895\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5850 - loss: 0.6810 - val_accuracy: 0.5609 - val_loss: 0.6847\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6054 - loss: 0.6675 - val_accuracy: 0.5724 - val_loss: 0.6826\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6343 - loss: 0.6478 - val_accuracy: 0.5747 - val_loss: 0.6885\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "    > Twitter :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 72ms/step - accuracy: 0.5102 - loss: 0.7974 - val_accuracy: 0.4989 - val_loss: 0.7060\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 21ms/step - accuracy: 0.5640 - loss: 0.6934 - val_accuracy: 0.5195 - val_loss: 0.6925\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5391 - loss: 0.6954 - val_accuracy: 0.5011 - val_loss: 0.6933\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 75ms/step - accuracy: 0.5049 - loss: 0.8463 - val_accuracy: 0.4828 - val_loss: 0.6942\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5194 - loss: 0.7114 - val_accuracy: 0.4943 - val_loss: 0.6971\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.4924 - loss: 0.8972 - val_accuracy: 0.4989 - val_loss: 0.7574\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5325 - loss: 0.7256 - val_accuracy: 0.5540 - val_loss: 0.6903\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5417 - loss: 0.6870 - val_accuracy: 0.5126 - val_loss: 0.6908\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "    > Twitter :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5148 - loss: 0.7855 - val_accuracy: 0.5103 - val_loss: 0.7062\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5857 - loss: 0.6743 - val_accuracy: 0.5402 - val_loss: 0.6893\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5785 - loss: 0.6685 - val_accuracy: 0.5310 - val_loss: 0.6862\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6284 - loss: 0.6355 - val_accuracy: 0.5701 - val_loss: 0.6776\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6967 - loss: 0.5859 - val_accuracy: 0.5379 - val_loss: 0.6816\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5076 - loss: 0.7641 - val_accuracy: 0.5264 - val_loss: 0.6908\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5496 - loss: 0.6898 - val_accuracy: 0.5195 - val_loss: 0.6940\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 97ms/step - accuracy: 0.4944 - loss: 0.8498 - val_accuracy: 0.5011 - val_loss: 0.7287\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5968 - loss: 0.6821 - val_accuracy: 0.5264 - val_loss: 0.6871\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5962 - loss: 0.6614 - val_accuracy: 0.5402 - val_loss: 0.6805\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6697 - loss: 0.6116 - val_accuracy: 0.5540 - val_loss: 0.6756\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6894 - loss: 0.5769 - val_accuracy: 0.5448 - val_loss: 0.6862\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "    > Reddit :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 130ms/step - accuracy: 0.5154 - loss: 0.6982 - val_accuracy: 0.5172 - val_loss: 0.6897\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 28ms/step - accuracy: 0.5312 - loss: 0.6877 - val_accuracy: 0.5195 - val_loss: 0.6899\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 125ms/step - accuracy: 0.5036 - loss: 0.7010 - val_accuracy: 0.5103 - val_loss: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 30ms/step - accuracy: 0.5076 - loss: 0.6914 - val_accuracy: 0.5310 - val_loss: 0.6918\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5272 - loss: 0.6895 - val_accuracy: 0.5057 - val_loss: 0.6926\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 128ms/step - accuracy: 0.4826 - loss: 0.7090 - val_accuracy: 0.5103 - val_loss: 0.6967\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5286 - loss: 0.6904 - val_accuracy: 0.5149 - val_loss: 0.6905\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5424 - loss: 0.6837 - val_accuracy: 0.5034 - val_loss: 0.6892\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5391 - loss: 0.6823 - val_accuracy: 0.4897 - val_loss: 0.6893\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "    > Reddit :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 132ms/step - accuracy: 0.4911 - loss: 0.6957 - val_accuracy: 0.5264 - val_loss: 0.6903\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5141 - loss: 0.6900 - val_accuracy: 0.5563 - val_loss: 0.6893\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.5338 - loss: 0.6824 - val_accuracy: 0.4943 - val_loss: 0.6898\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 135ms/step - accuracy: 0.5062 - loss: 0.6955 - val_accuracy: 0.5471 - val_loss: 0.6894\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.5371 - loss: 0.6867 - val_accuracy: 0.5586 - val_loss: 0.6869\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.5601 - loss: 0.6757 - val_accuracy: 0.5149 - val_loss: 0.6862\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.5653 - loss: 0.6708 - val_accuracy: 0.5310 - val_loss: 0.6822\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.5942 - loss: 0.6634 - val_accuracy: 0.5356 - val_loss: 0.6803\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.6415 - loss: 0.6401 - val_accuracy: 0.5195 - val_loss: 0.6891\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.4813 - loss: 0.7040 - val_accuracy: 0.5057 - val_loss: 0.6941\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5443 - loss: 0.6847 - val_accuracy: 0.5172 - val_loss: 0.6895\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.5561 - loss: 0.6809 - val_accuracy: 0.5471 - val_loss: 0.6854\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5798 - loss: 0.6699 - val_accuracy: 0.5011 - val_loss: 0.6848\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6133 - loss: 0.6559 - val_accuracy: 0.5425 - val_loss: 0.6843\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6211 - loss: 0.6442 - val_accuracy: 0.5195 - val_loss: 0.6900\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "    > Reddit :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 137ms/step - accuracy: 0.4839 - loss: 0.6950 - val_accuracy: 0.5057 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5338 - loss: 0.6902 - val_accuracy: 0.4874 - val_loss: 0.6933\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 140ms/step - accuracy: 0.5076 - loss: 0.6955 - val_accuracy: 0.4759 - val_loss: 0.6931\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5200 - loss: 0.6927 - val_accuracy: 0.4989 - val_loss: 0.6933\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 137ms/step - accuracy: 0.4833 - loss: 0.6934 - val_accuracy: 0.5195 - val_loss: 0.6929\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5272 - loss: 0.6913 - val_accuracy: 0.4920 - val_loss: 0.6928\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.4984 - loss: 0.6927 - val_accuracy: 0.5080 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5299 - loss: 0.6899 - val_accuracy: 0.5034 - val_loss: 0.6929\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "    > Reddit :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 181ms/step - accuracy: 0.5076 - loss: 0.6912 - val_accuracy: 0.5103 - val_loss: 0.6933\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5384 - loss: 0.6860 - val_accuracy: 0.5172 - val_loss: 0.6956\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 147ms/step - accuracy: 0.4898 - loss: 0.6956 - val_accuracy: 0.4943 - val_loss: 0.6935\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5062 - loss: 0.6914 - val_accuracy: 0.5057 - val_loss: 0.6932\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5555 - loss: 0.6862 - val_accuracy: 0.5126 - val_loss: 0.6940\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 147ms/step - accuracy: 0.4984 - loss: 0.6940 - val_accuracy: 0.5103 - val_loss: 0.6935\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5115 - loss: 0.6912 - val_accuracy: 0.5057 - val_loss: 0.6930\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5259 - loss: 0.6906 - val_accuracy: 0.5241 - val_loss: 0.6935\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "    > Reddit :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 127ms/step - accuracy: 0.5128 - loss: 0.7720 - val_accuracy: 0.4920 - val_loss: 0.7165\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 21ms/step - accuracy: 0.5286 - loss: 0.7085 - val_accuracy: 0.5471 - val_loss: 0.6917\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5286 - loss: 0.6956 - val_accuracy: 0.5034 - val_loss: 0.6926\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 73ms/step - accuracy: 0.5016 - loss: 0.8409 - val_accuracy: 0.5402 - val_loss: 0.6924\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5049 - loss: 0.7121 - val_accuracy: 0.5149 - val_loss: 0.6929\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.4865 - loss: 0.8590 - val_accuracy: 0.4966 - val_loss: 0.7629\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5489 - loss: 0.7249 - val_accuracy: 0.4828 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5391 - loss: 0.6910 - val_accuracy: 0.4989 - val_loss: 0.6942\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "    > Reddit :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 95ms/step - accuracy: 0.4957 - loss: 0.7779 - val_accuracy: 0.4943 - val_loss: 0.7004\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5798 - loss: 0.6782 - val_accuracy: 0.5034 - val_loss: 0.6981\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5949 - loss: 0.6675 - val_accuracy: 0.5218 - val_loss: 0.6943\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6415 - loss: 0.6363 - val_accuracy: 0.5034 - val_loss: 0.6941\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6822 - loss: 0.5849 - val_accuracy: 0.5379 - val_loss: 0.6980\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5148 - loss: 0.7599 - val_accuracy: 0.4828 - val_loss: 0.6945\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5515 - loss: 0.6849 - val_accuracy: 0.5195 - val_loss: 0.6939\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6362 - loss: 0.6509 - val_accuracy: 0.5241 - val_loss: 0.6932\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6717 - loss: 0.6216 - val_accuracy: 0.5149 - val_loss: 0.7026\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.4977 - loss: 0.8191 - val_accuracy: 0.4966 - val_loss: 0.7358\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5614 - loss: 0.6936 - val_accuracy: 0.5057 - val_loss: 0.6915\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6034 - loss: 0.6519 - val_accuracy: 0.4782 - val_loss: 0.6919\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "    > News :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 126ms/step - accuracy: 0.5095 - loss: 0.6964 - val_accuracy: 0.5724 - val_loss: 0.6829\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5581 - loss: 0.6800 - val_accuracy: 0.6368 - val_loss: 0.6718\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6185 - loss: 0.6623 - val_accuracy: 0.6943 - val_loss: 0.6534\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6625 - loss: 0.6360 - val_accuracy: 0.6966 - val_loss: 0.6242\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7118 - loss: 0.5886 - val_accuracy: 0.7103 - val_loss: 0.5943\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7282 - loss: 0.5594 - val_accuracy: 0.7149 - val_loss: 0.5824\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7544 - loss: 0.5248 - val_accuracy: 0.6897 - val_loss: 0.5714\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.7689 - loss: 0.4928 - val_accuracy: 0.7103 - val_loss: 0.5503\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 48ms/step - accuracy: 0.8004 - loss: 0.4431 - val_accuracy: 0.7149 - val_loss: 0.5537\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 125ms/step - accuracy: 0.5056 - loss: 0.6946 - val_accuracy: 0.5632 - val_loss: 0.6859\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 27ms/step - accuracy: 0.5719 - loss: 0.6802 - val_accuracy: 0.6299 - val_loss: 0.6757\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6349 - loss: 0.6584 - val_accuracy: 0.6184 - val_loss: 0.6446\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6710 - loss: 0.6135 - val_accuracy: 0.6920 - val_loss: 0.6108\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7275 - loss: 0.5621 - val_accuracy: 0.6920 - val_loss: 0.5842\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7577 - loss: 0.5140 - val_accuracy: 0.7080 - val_loss: 0.5786\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7846 - loss: 0.4627 - val_accuracy: 0.7310 - val_loss: 0.5550\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.8201 - loss: 0.4265 - val_accuracy: 0.7264 - val_loss: 0.5639\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 124ms/step - accuracy: 0.4865 - loss: 0.7122 - val_accuracy: 0.4989 - val_loss: 0.6919\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5338 - loss: 0.6868 - val_accuracy: 0.5356 - val_loss: 0.6844\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6106 - loss: 0.6699 - val_accuracy: 0.6483 - val_loss: 0.6721\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6362 - loss: 0.6546 - val_accuracy: 0.6782 - val_loss: 0.6542\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.6592 - loss: 0.6285 - val_accuracy: 0.6920 - val_loss: 0.6220\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7012 - loss: 0.5851 - val_accuracy: 0.6644 - val_loss: 0.6151\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.7072 - loss: 0.5729 - val_accuracy: 0.6943 - val_loss: 0.5865\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.7091 - loss: 0.5601 - val_accuracy: 0.6552 - val_loss: 0.6117\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "    > News :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 132ms/step - accuracy: 0.5227 - loss: 0.6906 - val_accuracy: 0.5908 - val_loss: 0.6832\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6139 - loss: 0.6698 - val_accuracy: 0.6552 - val_loss: 0.6605\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.6829 - loss: 0.6217 - val_accuracy: 0.7333 - val_loss: 0.5938\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.7472 - loss: 0.5440 - val_accuracy: 0.7172 - val_loss: 0.5573\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.7912 - loss: 0.4636 - val_accuracy: 0.7287 - val_loss: 0.5498\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.8194 - loss: 0.3964 - val_accuracy: 0.7356 - val_loss: 0.5870\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 133ms/step - accuracy: 0.5213 - loss: 0.6916 - val_accuracy: 0.5885 - val_loss: 0.6832\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.6028 - loss: 0.6698 - val_accuracy: 0.6391 - val_loss: 0.6672\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.6815 - loss: 0.6260 - val_accuracy: 0.7057 - val_loss: 0.6019\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.7288 - loss: 0.5352 - val_accuracy: 0.7172 - val_loss: 0.5551\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.7787 - loss: 0.4769 - val_accuracy: 0.7149 - val_loss: 0.6028\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.5207 - loss: 0.6957 - val_accuracy: 0.5586 - val_loss: 0.6798\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.6362 - loss: 0.6537 - val_accuracy: 0.6667 - val_loss: 0.6472\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.7039 - loss: 0.5837 - val_accuracy: 0.6644 - val_loss: 0.5968\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 48ms/step - accuracy: 0.7571 - loss: 0.5143 - val_accuracy: 0.6897 - val_loss: 0.5849\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.8017 - loss: 0.4450 - val_accuracy: 0.7034 - val_loss: 0.5758\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.8188 - loss: 0.4220 - val_accuracy: 0.6897 - val_loss: 0.6011\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "    > News :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 138ms/step - accuracy: 0.5102 - loss: 0.6936 - val_accuracy: 0.5977 - val_loss: 0.6893\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5962 - loss: 0.6821 - val_accuracy: 0.5747 - val_loss: 0.6721\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.6402 - loss: 0.6456 - val_accuracy: 0.6368 - val_loss: 0.6276\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7150 - loss: 0.5759 - val_accuracy: 0.6897 - val_loss: 0.5936\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.7551 - loss: 0.5287 - val_accuracy: 0.6943 - val_loss: 0.5911\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.7807 - loss: 0.4836 - val_accuracy: 0.7080 - val_loss: 0.5841\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.7951 - loss: 0.4646 - val_accuracy: 0.6966 - val_loss: 0.5784\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.8234 - loss: 0.4216 - val_accuracy: 0.7195 - val_loss: 0.5822\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 140ms/step - accuracy: 0.4879 - loss: 0.6942 - val_accuracy: 0.4966 - val_loss: 0.6920\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5404 - loss: 0.6886 - val_accuracy: 0.6046 - val_loss: 0.6868\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.5811 - loss: 0.6789 - val_accuracy: 0.6092 - val_loss: 0.6740\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6303 - loss: 0.6469 - val_accuracy: 0.6138 - val_loss: 0.6443\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.6645 - loss: 0.6190 - val_accuracy: 0.6575 - val_loss: 0.6158\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.6888 - loss: 0.5983 - val_accuracy: 0.5770 - val_loss: 0.6653\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 144ms/step - accuracy: 0.5220 - loss: 0.6918 - val_accuracy: 0.5862 - val_loss: 0.6896\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5594 - loss: 0.6864 - val_accuracy: 0.6000 - val_loss: 0.6804\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5896 - loss: 0.6762 - val_accuracy: 0.6184 - val_loss: 0.6614\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.6356 - loss: 0.6524 - val_accuracy: 0.6667 - val_loss: 0.6302\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.6796 - loss: 0.6018 - val_accuracy: 0.6644 - val_loss: 0.6172\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.7098 - loss: 0.5684 - val_accuracy: 0.6920 - val_loss: 0.5891\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7249 - loss: 0.5526 - val_accuracy: 0.6897 - val_loss: 0.5838\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.7669 - loss: 0.5037 - val_accuracy: 0.7057 - val_loss: 0.5838\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "    > News :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 146ms/step - accuracy: 0.5535 - loss: 0.6888 - val_accuracy: 0.6046 - val_loss: 0.6803\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6323 - loss: 0.6553 - val_accuracy: 0.6322 - val_loss: 0.6367\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6980 - loss: 0.5914 - val_accuracy: 0.6759 - val_loss: 0.6012\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7636 - loss: 0.4922 - val_accuracy: 0.7310 - val_loss: 0.5361\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8116 - loss: 0.4262 - val_accuracy: 0.7264 - val_loss: 0.5532\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 147ms/step - accuracy: 0.5049 - loss: 0.6932 - val_accuracy: 0.5770 - val_loss: 0.6898\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.6218 - loss: 0.6828 - val_accuracy: 0.6920 - val_loss: 0.6727\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6730 - loss: 0.6346 - val_accuracy: 0.6989 - val_loss: 0.6052\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7249 - loss: 0.5554 - val_accuracy: 0.7379 - val_loss: 0.5633\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7735 - loss: 0.5103 - val_accuracy: 0.6759 - val_loss: 0.5904\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 148ms/step - accuracy: 0.5272 - loss: 0.6907 - val_accuracy: 0.6000 - val_loss: 0.6846\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6185 - loss: 0.6701 - val_accuracy: 0.6644 - val_loss: 0.6540\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6697 - loss: 0.6195 - val_accuracy: 0.6667 - val_loss: 0.6110\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.7163 - loss: 0.5535 - val_accuracy: 0.6805 - val_loss: 0.5811\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 47ms/step - accuracy: 0.7669 - loss: 0.5028 - val_accuracy: 0.7218 - val_loss: 0.5563\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 49ms/step - accuracy: 0.8109 - loss: 0.4416 - val_accuracy: 0.7264 - val_loss: 0.5691\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "    > News :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5253 - loss: 0.7829 - val_accuracy: 0.5011 - val_loss: 0.6936\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5620 - loss: 0.6844 - val_accuracy: 0.5770 - val_loss: 0.6843\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5831 - loss: 0.6691 - val_accuracy: 0.6483 - val_loss: 0.6731\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6303 - loss: 0.6414 - val_accuracy: 0.6345 - val_loss: 0.6468\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.7019 - loss: 0.5909 - val_accuracy: 0.6207 - val_loss: 0.6285\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.7085 - loss: 0.5614 - val_accuracy: 0.6161 - val_loss: 0.6188\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7446 - loss: 0.5140 - val_accuracy: 0.5977 - val_loss: 0.6358\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.4977 - loss: 0.8370 - val_accuracy: 0.5103 - val_loss: 0.6915\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5391 - loss: 0.6994 - val_accuracy: 0.5563 - val_loss: 0.6882\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5968 - loss: 0.6674 - val_accuracy: 0.6023 - val_loss: 0.6836\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6165 - loss: 0.6528 - val_accuracy: 0.6345 - val_loss: 0.6697\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6323 - loss: 0.6302 - val_accuracy: 0.6368 - val_loss: 0.6415\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6789 - loss: 0.6015 - val_accuracy: 0.6161 - val_loss: 0.6307\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 20ms/step - accuracy: 0.7032 - loss: 0.5597 - val_accuracy: 0.6161 - val_loss: 0.6274\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.7249 - loss: 0.5389 - val_accuracy: 0.6207 - val_loss: 0.6294\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5023 - loss: 0.8636 - val_accuracy: 0.4989 - val_loss: 0.7475\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5594 - loss: 0.7137 - val_accuracy: 0.6184 - val_loss: 0.6773\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5982 - loss: 0.6611 - val_accuracy: 0.6253 - val_loss: 0.6786\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "    > News :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 92ms/step - accuracy: 0.5115 - loss: 0.7876 - val_accuracy: 0.6299 - val_loss: 0.6729\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6244 - loss: 0.6411 - val_accuracy: 0.5793 - val_loss: 0.6687\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6934 - loss: 0.5901 - val_accuracy: 0.7080 - val_loss: 0.6136\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7367 - loss: 0.5174 - val_accuracy: 0.7034 - val_loss: 0.5677\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8116 - loss: 0.4208 - val_accuracy: 0.7080 - val_loss: 0.5593\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8628 - loss: 0.3298 - val_accuracy: 0.7057 - val_loss: 0.5530\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8969 - loss: 0.2552 - val_accuracy: 0.7195 - val_loss: 0.5695\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 95ms/step - accuracy: 0.5647 - loss: 0.7364 - val_accuracy: 0.6483 - val_loss: 0.6672\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6376 - loss: 0.6410 - val_accuracy: 0.5517 - val_loss: 0.6611\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7229 - loss: 0.5622 - val_accuracy: 0.5931 - val_loss: 0.6458\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7682 - loss: 0.4846 - val_accuracy: 0.6345 - val_loss: 0.6265\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8313 - loss: 0.3938 - val_accuracy: 0.6345 - val_loss: 0.6440\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5253 - loss: 0.7937 - val_accuracy: 0.5218 - val_loss: 0.6910\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6271 - loss: 0.6276 - val_accuracy: 0.6575 - val_loss: 0.6499\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7203 - loss: 0.5716 - val_accuracy: 0.6759 - val_loss: 0.6026\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7702 - loss: 0.4871 - val_accuracy: 0.7310 - val_loss: 0.5606\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 47ms/step - accuracy: 0.8024 - loss: 0.4242 - val_accuracy: 0.7356 - val_loss: 0.5470\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.8588 - loss: 0.3385 - val_accuracy: 0.7402 - val_loss: 0.5336\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8825 - loss: 0.2801 - val_accuracy: 0.7379 - val_loss: 0.5194\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 55ms/step - accuracy: 0.9028 - loss: 0.2451 - val_accuracy: 0.6874 - val_loss: 0.6554\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\n",
      "===== Balanced variant seed=2 =====\n",
      "  Twitter: balanced 50/50 → 2176 rows\n",
      "  Reddit: balanced 50/50 → 898548 rows\n",
      "  News: balanced 50/50 → 23294 rows\n",
      "  Equalized cap across domains → N per domain = 2176 (each class 1088)\n",
      "    > Twitter :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 129ms/step - accuracy: 0.5167 - loss: 0.6978 - val_accuracy: 0.5126 - val_loss: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5213 - loss: 0.6930 - val_accuracy: 0.5379 - val_loss: 0.6882\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5174 - loss: 0.6900 - val_accuracy: 0.5448 - val_loss: 0.6880\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5233 - loss: 0.6890 - val_accuracy: 0.5678 - val_loss: 0.6869\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5509 - loss: 0.6806 - val_accuracy: 0.5724 - val_loss: 0.6830\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5660 - loss: 0.6789 - val_accuracy: 0.5770 - val_loss: 0.6806\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5824 - loss: 0.6737 - val_accuracy: 0.5517 - val_loss: 0.6853\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 123ms/step - accuracy: 0.5167 - loss: 0.6952 - val_accuracy: 0.5333 - val_loss: 0.6899\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5575 - loss: 0.6875 - val_accuracy: 0.5379 - val_loss: 0.6900\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 6s - 494ms/step - accuracy: 0.4957 - loss: 0.7081 - val_accuracy: 0.5011 - val_loss: 0.6959\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.4846 - loss: 0.6958 - val_accuracy: 0.5241 - val_loss: 0.6916\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5417 - loss: 0.6899 - val_accuracy: 0.5333 - val_loss: 0.6886\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5312 - loss: 0.6887 - val_accuracy: 0.5379 - val_loss: 0.6865\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5515 - loss: 0.6866 - val_accuracy: 0.5678 - val_loss: 0.6836\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5706 - loss: 0.6787 - val_accuracy: 0.5494 - val_loss: 0.6809\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5640 - loss: 0.6776 - val_accuracy: 0.5448 - val_loss: 0.6791\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5968 - loss: 0.6615 - val_accuracy: 0.5586 - val_loss: 0.7006\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "    > Twitter :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.5174 - loss: 0.6935 - val_accuracy: 0.5540 - val_loss: 0.6868\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5588 - loss: 0.6850 - val_accuracy: 0.6000 - val_loss: 0.6811\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5522 - loss: 0.6841 - val_accuracy: 0.6046 - val_loss: 0.6777\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5923 - loss: 0.6713 - val_accuracy: 0.6046 - val_loss: 0.6739\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5890 - loss: 0.6639 - val_accuracy: 0.5931 - val_loss: 0.6738\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6113 - loss: 0.6566 - val_accuracy: 0.5655 - val_loss: 0.6712\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6422 - loss: 0.6446 - val_accuracy: 0.6000 - val_loss: 0.6618\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.6717 - loss: 0.6200 - val_accuracy: 0.6000 - val_loss: 0.6665\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 135ms/step - accuracy: 0.5266 - loss: 0.6932 - val_accuracy: 0.5655 - val_loss: 0.6850\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5555 - loss: 0.6845 - val_accuracy: 0.5632 - val_loss: 0.6832\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5607 - loss: 0.6803 - val_accuracy: 0.5747 - val_loss: 0.6795\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.5706 - loss: 0.6726 - val_accuracy: 0.5885 - val_loss: 0.6753\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.6192 - loss: 0.6555 - val_accuracy: 0.5816 - val_loss: 0.6710\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6487 - loss: 0.6295 - val_accuracy: 0.5816 - val_loss: 0.6711\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 139ms/step - accuracy: 0.4964 - loss: 0.6986 - val_accuracy: 0.5218 - val_loss: 0.6921\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5135 - loss: 0.6937 - val_accuracy: 0.5793 - val_loss: 0.6848\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 38ms/step - accuracy: 0.5397 - loss: 0.6850 - val_accuracy: 0.5839 - val_loss: 0.6835\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5653 - loss: 0.6779 - val_accuracy: 0.5816 - val_loss: 0.6788\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.5883 - loss: 0.6631 - val_accuracy: 0.5379 - val_loss: 0.6814\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "    > Twitter :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 143ms/step - accuracy: 0.5128 - loss: 0.6938 - val_accuracy: 0.4989 - val_loss: 0.6932\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5049 - loss: 0.6941 - val_accuracy: 0.5379 - val_loss: 0.6923\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5601 - loss: 0.6907 - val_accuracy: 0.5517 - val_loss: 0.6912\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5575 - loss: 0.6870 - val_accuracy: 0.5402 - val_loss: 0.6882\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5666 - loss: 0.6859 - val_accuracy: 0.5425 - val_loss: 0.6899\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 138ms/step - accuracy: 0.5062 - loss: 0.6939 - val_accuracy: 0.4943 - val_loss: 0.6930\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5030 - loss: 0.6927 - val_accuracy: 0.5632 - val_loss: 0.6928\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5443 - loss: 0.6906 - val_accuracy: 0.5333 - val_loss: 0.6919\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5463 - loss: 0.6889 - val_accuracy: 0.5425 - val_loss: 0.6900\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5437 - loss: 0.6876 - val_accuracy: 0.5494 - val_loss: 0.6875\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5627 - loss: 0.6841 - val_accuracy: 0.5379 - val_loss: 0.6893\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 137ms/step - accuracy: 0.4819 - loss: 0.6955 - val_accuracy: 0.5310 - val_loss: 0.6918\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5167 - loss: 0.6929 - val_accuracy: 0.5402 - val_loss: 0.6914\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5325 - loss: 0.6914 - val_accuracy: 0.5379 - val_loss: 0.6905\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5561 - loss: 0.6863 - val_accuracy: 0.5563 - val_loss: 0.6873\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5739 - loss: 0.6791 - val_accuracy: 0.5747 - val_loss: 0.6851\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5732 - loss: 0.6765 - val_accuracy: 0.5839 - val_loss: 0.6824\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5857 - loss: 0.6722 - val_accuracy: 0.5770 - val_loss: 0.6817\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.6001 - loss: 0.6657 - val_accuracy: 0.5678 - val_loss: 0.6798\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 51ms/step - accuracy: 0.6179 - loss: 0.6566 - val_accuracy: 0.5678 - val_loss: 0.6834\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "    > Twitter :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 146ms/step - accuracy: 0.5227 - loss: 0.6937 - val_accuracy: 0.5310 - val_loss: 0.6932\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5450 - loss: 0.6898 - val_accuracy: 0.5402 - val_loss: 0.6900\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5739 - loss: 0.6807 - val_accuracy: 0.5425 - val_loss: 0.6894\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5693 - loss: 0.6805 - val_accuracy: 0.5586 - val_loss: 0.6889\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5726 - loss: 0.6722 - val_accuracy: 0.5885 - val_loss: 0.6809\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6205 - loss: 0.6467 - val_accuracy: 0.5816 - val_loss: 0.6815\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 146ms/step - accuracy: 0.4833 - loss: 0.6950 - val_accuracy: 0.5034 - val_loss: 0.6926\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5266 - loss: 0.6920 - val_accuracy: 0.5678 - val_loss: 0.6913\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 52ms/step - accuracy: 0.5634 - loss: 0.6850 - val_accuracy: 0.5540 - val_loss: 0.6875\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5666 - loss: 0.6857 - val_accuracy: 0.5586 - val_loss: 0.6872\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.5758 - loss: 0.6797 - val_accuracy: 0.5839 - val_loss: 0.6823\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6310 - loss: 0.6609 - val_accuracy: 0.5908 - val_loss: 0.6727\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6474 - loss: 0.6371 - val_accuracy: 0.5793 - val_loss: 0.6671\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6724 - loss: 0.6021 - val_accuracy: 0.5816 - val_loss: 0.6832\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 149ms/step - accuracy: 0.5378 - loss: 0.6927 - val_accuracy: 0.5264 - val_loss: 0.6914\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5417 - loss: 0.6887 - val_accuracy: 0.5425 - val_loss: 0.6885\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5555 - loss: 0.6860 - val_accuracy: 0.5494 - val_loss: 0.6862\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5680 - loss: 0.6782 - val_accuracy: 0.5655 - val_loss: 0.6853\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6159 - loss: 0.6631 - val_accuracy: 0.5770 - val_loss: 0.6833\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6290 - loss: 0.6376 - val_accuracy: 0.5678 - val_loss: 0.6973\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "    > Twitter :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 71ms/step - accuracy: 0.5023 - loss: 0.7838 - val_accuracy: 0.5103 - val_loss: 0.7002\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5299 - loss: 0.6971 - val_accuracy: 0.5080 - val_loss: 0.6929\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5456 - loss: 0.6973 - val_accuracy: 0.5264 - val_loss: 0.6897\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5732 - loss: 0.6805 - val_accuracy: 0.5839 - val_loss: 0.6865\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5752 - loss: 0.6803 - val_accuracy: 0.5379 - val_loss: 0.6859\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5817 - loss: 0.6696 - val_accuracy: 0.5241 - val_loss: 0.6819\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6126 - loss: 0.6503 - val_accuracy: 0.5517 - val_loss: 0.6791\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6356 - loss: 0.6399 - val_accuracy: 0.5241 - val_loss: 0.6920\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5010 - loss: 0.8487 - val_accuracy: 0.5080 - val_loss: 0.6914\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5049 - loss: 0.7156 - val_accuracy: 0.5080 - val_loss: 0.6950\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.5135 - loss: 0.8833 - val_accuracy: 0.4989 - val_loss: 0.7636\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5312 - loss: 0.7234 - val_accuracy: 0.5103 - val_loss: 0.6898\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5502 - loss: 0.6844 - val_accuracy: 0.5287 - val_loss: 0.6894\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5666 - loss: 0.6828 - val_accuracy: 0.5333 - val_loss: 0.6884\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5877 - loss: 0.6666 - val_accuracy: 0.5310 - val_loss: 0.6865\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5982 - loss: 0.6675 - val_accuracy: 0.5333 - val_loss: 0.6862\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 20ms/step - accuracy: 0.6060 - loss: 0.6563 - val_accuracy: 0.5310 - val_loss: 0.6845\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6619 - loss: 0.6185 - val_accuracy: 0.5494 - val_loss: 0.6760\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6855 - loss: 0.5904 - val_accuracy: 0.5517 - val_loss: 0.6738\n",
      "Epoch 10/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6861 - loss: 0.5720 - val_accuracy: 0.5770 - val_loss: 0.6855\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "    > Twitter :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5089 - loss: 0.7854 - val_accuracy: 0.5034 - val_loss: 0.6944\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5804 - loss: 0.6787 - val_accuracy: 0.5517 - val_loss: 0.6848\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 47ms/step - accuracy: 0.6074 - loss: 0.6627 - val_accuracy: 0.5494 - val_loss: 0.6800\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6330 - loss: 0.6273 - val_accuracy: 0.5540 - val_loss: 0.6780\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7078 - loss: 0.5690 - val_accuracy: 0.5494 - val_loss: 0.6860\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5148 - loss: 0.7590 - val_accuracy: 0.5241 - val_loss: 0.6885\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5817 - loss: 0.6777 - val_accuracy: 0.5425 - val_loss: 0.6853\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5995 - loss: 0.6549 - val_accuracy: 0.5402 - val_loss: 0.6806\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6802 - loss: 0.6059 - val_accuracy: 0.5333 - val_loss: 0.6964\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 97ms/step - accuracy: 0.5200 - loss: 0.8453 - val_accuracy: 0.5057 - val_loss: 0.7125\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5857 - loss: 0.6781 - val_accuracy: 0.5126 - val_loss: 0.6876\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6106 - loss: 0.6532 - val_accuracy: 0.5494 - val_loss: 0.6795\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6559 - loss: 0.6204 - val_accuracy: 0.5609 - val_loss: 0.6717\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7032 - loss: 0.5722 - val_accuracy: 0.5770 - val_loss: 0.6695\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 47ms/step - accuracy: 0.7472 - loss: 0.5088 - val_accuracy: 0.5793 - val_loss: 0.6594\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8043 - loss: 0.4401 - val_accuracy: 0.5885 - val_loss: 0.6720\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "    > Reddit :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 130ms/step - accuracy: 0.5148 - loss: 0.6952 - val_accuracy: 0.5011 - val_loss: 0.6919\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5404 - loss: 0.6896 - val_accuracy: 0.5816 - val_loss: 0.6849\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5364 - loss: 0.6881 - val_accuracy: 0.5977 - val_loss: 0.6796\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5660 - loss: 0.6815 - val_accuracy: 0.6115 - val_loss: 0.6737\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5804 - loss: 0.6697 - val_accuracy: 0.5770 - val_loss: 0.6709\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 27ms/step - accuracy: 0.5955 - loss: 0.6623 - val_accuracy: 0.5724 - val_loss: 0.6666\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 27ms/step - accuracy: 0.6047 - loss: 0.6551 - val_accuracy: 0.5977 - val_loss: 0.6545\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.6303 - loss: 0.6416 - val_accuracy: 0.5333 - val_loss: 0.6890\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 124ms/step - accuracy: 0.5266 - loss: 0.6960 - val_accuracy: 0.5609 - val_loss: 0.6872\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5378 - loss: 0.6877 - val_accuracy: 0.5448 - val_loss: 0.6854\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5561 - loss: 0.6822 - val_accuracy: 0.5816 - val_loss: 0.6772\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 26ms/step - accuracy: 0.5561 - loss: 0.6805 - val_accuracy: 0.5747 - val_loss: 0.6740\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5824 - loss: 0.6738 - val_accuracy: 0.5954 - val_loss: 0.6681\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5968 - loss: 0.6666 - val_accuracy: 0.5885 - val_loss: 0.6629\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6146 - loss: 0.6535 - val_accuracy: 0.5862 - val_loss: 0.6658\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 124ms/step - accuracy: 0.4938 - loss: 0.7048 - val_accuracy: 0.5034 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5259 - loss: 0.6910 - val_accuracy: 0.5747 - val_loss: 0.6862\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5364 - loss: 0.6885 - val_accuracy: 0.5862 - val_loss: 0.6803\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5647 - loss: 0.6774 - val_accuracy: 0.5701 - val_loss: 0.6741\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5791 - loss: 0.6698 - val_accuracy: 0.5586 - val_loss: 0.6666\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5785 - loss: 0.6647 - val_accuracy: 0.5540 - val_loss: 0.6637\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5909 - loss: 0.6634 - val_accuracy: 0.5655 - val_loss: 0.6612\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.6120 - loss: 0.6488 - val_accuracy: 0.5655 - val_loss: 0.6574\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6277 - loss: 0.6367 - val_accuracy: 0.5931 - val_loss: 0.6571\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6467 - loss: 0.6297 - val_accuracy: 0.5908 - val_loss: 0.6563\n",
      "Epoch 11/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.6494 - loss: 0.6241 - val_accuracy: 0.5977 - val_loss: 0.6517\n",
      "Epoch 12/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6704 - loss: 0.6018 - val_accuracy: 0.6138 - val_loss: 0.6538\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "    > Reddit :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 133ms/step - accuracy: 0.5036 - loss: 0.6936 - val_accuracy: 0.5402 - val_loss: 0.6882\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5673 - loss: 0.6822 - val_accuracy: 0.5747 - val_loss: 0.6766\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5693 - loss: 0.6782 - val_accuracy: 0.5632 - val_loss: 0.6696\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6120 - loss: 0.6596 - val_accuracy: 0.5701 - val_loss: 0.6623\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6369 - loss: 0.6401 - val_accuracy: 0.5655 - val_loss: 0.6532\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6500 - loss: 0.6263 - val_accuracy: 0.5954 - val_loss: 0.6579\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 131ms/step - accuracy: 0.5049 - loss: 0.6975 - val_accuracy: 0.5402 - val_loss: 0.6863\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5476 - loss: 0.6835 - val_accuracy: 0.5747 - val_loss: 0.6796\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5791 - loss: 0.6767 - val_accuracy: 0.5908 - val_loss: 0.6695\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.5975 - loss: 0.6582 - val_accuracy: 0.5816 - val_loss: 0.6619\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6330 - loss: 0.6441 - val_accuracy: 0.5770 - val_loss: 0.6594\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6467 - loss: 0.6287 - val_accuracy: 0.5471 - val_loss: 0.6706\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 138ms/step - accuracy: 0.5292 - loss: 0.6914 - val_accuracy: 0.5471 - val_loss: 0.6860\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5305 - loss: 0.6884 - val_accuracy: 0.5839 - val_loss: 0.6760\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5719 - loss: 0.6751 - val_accuracy: 0.6161 - val_loss: 0.6671\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5995 - loss: 0.6610 - val_accuracy: 0.5885 - val_loss: 0.6571\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.6211 - loss: 0.6402 - val_accuracy: 0.5954 - val_loss: 0.6488\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6586 - loss: 0.6220 - val_accuracy: 0.5954 - val_loss: 0.6510\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "    > Reddit :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.5220 - loss: 0.6939 - val_accuracy: 0.5126 - val_loss: 0.6922\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.4990 - loss: 0.6931 - val_accuracy: 0.5563 - val_loss: 0.6884\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5555 - loss: 0.6881 - val_accuracy: 0.5747 - val_loss: 0.6855\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5745 - loss: 0.6810 - val_accuracy: 0.5793 - val_loss: 0.6792\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5870 - loss: 0.6669 - val_accuracy: 0.5977 - val_loss: 0.6764\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.6303 - loss: 0.6455 - val_accuracy: 0.5655 - val_loss: 0.6790\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.4859 - loss: 0.6952 - val_accuracy: 0.5011 - val_loss: 0.6923\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.5095 - loss: 0.6919 - val_accuracy: 0.5632 - val_loss: 0.6922\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5154 - loss: 0.6926 - val_accuracy: 0.5793 - val_loss: 0.6910\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5437 - loss: 0.6904 - val_accuracy: 0.5586 - val_loss: 0.6900\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.5424 - loss: 0.6866 - val_accuracy: 0.5517 - val_loss: 0.6866\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.5719 - loss: 0.6791 - val_accuracy: 0.5586 - val_loss: 0.6940\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 137ms/step - accuracy: 0.5082 - loss: 0.6941 - val_accuracy: 0.5379 - val_loss: 0.6918\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5095 - loss: 0.6917 - val_accuracy: 0.5379 - val_loss: 0.6885\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 34ms/step - accuracy: 0.5358 - loss: 0.6913 - val_accuracy: 0.5747 - val_loss: 0.6874\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5522 - loss: 0.6876 - val_accuracy: 0.5494 - val_loss: 0.6836\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5469 - loss: 0.6845 - val_accuracy: 0.5793 - val_loss: 0.6781\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.5758 - loss: 0.6748 - val_accuracy: 0.5425 - val_loss: 0.6727\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.5890 - loss: 0.6703 - val_accuracy: 0.5540 - val_loss: 0.6737\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "    > Reddit :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 147ms/step - accuracy: 0.5174 - loss: 0.6922 - val_accuracy: 0.5609 - val_loss: 0.6884\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5456 - loss: 0.6893 - val_accuracy: 0.5770 - val_loss: 0.6847\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.5712 - loss: 0.6764 - val_accuracy: 0.5701 - val_loss: 0.6714\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5909 - loss: 0.6599 - val_accuracy: 0.5885 - val_loss: 0.6641\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.6566 - loss: 0.6224 - val_accuracy: 0.6230 - val_loss: 0.6680\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 142ms/step - accuracy: 0.4773 - loss: 0.6949 - val_accuracy: 0.5080 - val_loss: 0.6912\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 40ms/step - accuracy: 0.5233 - loss: 0.6916 - val_accuracy: 0.5655 - val_loss: 0.6898\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.5660 - loss: 0.6879 - val_accuracy: 0.5632 - val_loss: 0.6855\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 42ms/step - accuracy: 0.6133 - loss: 0.6672 - val_accuracy: 0.5954 - val_loss: 0.6661\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6402 - loss: 0.6344 - val_accuracy: 0.6069 - val_loss: 0.6583\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6822 - loss: 0.6068 - val_accuracy: 0.6023 - val_loss: 0.6619\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 144ms/step - accuracy: 0.5108 - loss: 0.6941 - val_accuracy: 0.5264 - val_loss: 0.6920\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.5384 - loss: 0.6911 - val_accuracy: 0.5379 - val_loss: 0.6893\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5397 - loss: 0.6860 - val_accuracy: 0.5655 - val_loss: 0.6847\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5745 - loss: 0.6763 - val_accuracy: 0.5885 - val_loss: 0.6730\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6054 - loss: 0.6585 - val_accuracy: 0.6046 - val_loss: 0.6640\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6507 - loss: 0.6255 - val_accuracy: 0.6138 - val_loss: 0.6551\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6802 - loss: 0.6025 - val_accuracy: 0.6230 - val_loss: 0.6557\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "    > Reddit :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5069 - loss: 0.7834 - val_accuracy: 0.5126 - val_loss: 0.7001\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5338 - loss: 0.6965 - val_accuracy: 0.5586 - val_loss: 0.6893\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5345 - loss: 0.6934 - val_accuracy: 0.5218 - val_loss: 0.6905\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.5043 - loss: 0.8189 - val_accuracy: 0.5103 - val_loss: 0.6968\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5095 - loss: 0.7166 - val_accuracy: 0.4966 - val_loss: 0.6945\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5272 - loss: 0.6905 - val_accuracy: 0.4897 - val_loss: 0.6935\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5620 - loss: 0.6810 - val_accuracy: 0.5080 - val_loss: 0.6928\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5758 - loss: 0.6736 - val_accuracy: 0.5333 - val_loss: 0.6884\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5739 - loss: 0.6726 - val_accuracy: 0.5310 - val_loss: 0.6853\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6152 - loss: 0.6524 - val_accuracy: 0.5885 - val_loss: 0.6755\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6349 - loss: 0.6336 - val_accuracy: 0.5632 - val_loss: 0.6762\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.4892 - loss: 0.8692 - val_accuracy: 0.4966 - val_loss: 0.7107\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5548 - loss: 0.7078 - val_accuracy: 0.5494 - val_loss: 0.6866\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.5410 - loss: 0.6851 - val_accuracy: 0.5080 - val_loss: 0.6901\n",
      "Epoch 3: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "    > Reddit :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5082 - loss: 0.7740 - val_accuracy: 0.5126 - val_loss: 0.6942\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5732 - loss: 0.6748 - val_accuracy: 0.5770 - val_loss: 0.6825\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.5909 - loss: 0.6583 - val_accuracy: 0.5471 - val_loss: 0.6822\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6435 - loss: 0.6221 - val_accuracy: 0.5494 - val_loss: 0.6819\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7275 - loss: 0.5669 - val_accuracy: 0.5724 - val_loss: 0.6797\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7577 - loss: 0.5094 - val_accuracy: 0.5816 - val_loss: 0.6863\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5141 - loss: 0.7516 - val_accuracy: 0.5149 - val_loss: 0.6918\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5752 - loss: 0.6747 - val_accuracy: 0.5241 - val_loss: 0.6891\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6284 - loss: 0.6500 - val_accuracy: 0.5586 - val_loss: 0.6848\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6724 - loss: 0.6142 - val_accuracy: 0.5724 - val_loss: 0.6764\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7104 - loss: 0.5737 - val_accuracy: 0.5425 - val_loss: 0.6947\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 94ms/step - accuracy: 0.5076 - loss: 0.8134 - val_accuracy: 0.5103 - val_loss: 0.7099\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.5844 - loss: 0.6776 - val_accuracy: 0.5172 - val_loss: 0.6912\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6349 - loss: 0.6467 - val_accuracy: 0.5609 - val_loss: 0.6843\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 49ms/step - accuracy: 0.6638 - loss: 0.6194 - val_accuracy: 0.5724 - val_loss: 0.6714\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.7282 - loss: 0.5528 - val_accuracy: 0.5816 - val_loss: 0.6662\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7557 - loss: 0.5049 - val_accuracy: 0.5908 - val_loss: 0.6793\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "    > News :: BiLSTM_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 131ms/step - accuracy: 0.5200 - loss: 0.6936 - val_accuracy: 0.5678 - val_loss: 0.6844\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 24ms/step - accuracy: 0.5712 - loss: 0.6788 - val_accuracy: 0.6161 - val_loss: 0.6703\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6225 - loss: 0.6609 - val_accuracy: 0.6943 - val_loss: 0.6479\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6717 - loss: 0.6183 - val_accuracy: 0.6529 - val_loss: 0.5992\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7072 - loss: 0.5740 - val_accuracy: 0.6621 - val_loss: 0.5857\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7433 - loss: 0.5293 - val_accuracy: 0.7034 - val_loss: 0.5419\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7649 - loss: 0.4982 - val_accuracy: 0.7264 - val_loss: 0.5277\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 29ms/step - accuracy: 0.7938 - loss: 0.4540 - val_accuracy: 0.7632 - val_loss: 0.4936\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8247 - loss: 0.4200 - val_accuracy: 0.7678 - val_loss: 0.4790\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8214 - loss: 0.4031 - val_accuracy: 0.7425 - val_loss: 0.4909\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 124ms/step - accuracy: 0.5259 - loss: 0.6940 - val_accuracy: 0.5563 - val_loss: 0.6834\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5883 - loss: 0.6765 - val_accuracy: 0.6529 - val_loss: 0.6677\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6435 - loss: 0.6452 - val_accuracy: 0.6046 - val_loss: 0.6576\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 28ms/step - accuracy: 0.6822 - loss: 0.6100 - val_accuracy: 0.6460 - val_loss: 0.6275\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6921 - loss: 0.5843 - val_accuracy: 0.7080 - val_loss: 0.5781\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 30ms/step - accuracy: 0.7249 - loss: 0.5519 - val_accuracy: 0.7011 - val_loss: 0.5615\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 49ms/step - accuracy: 0.7617 - loss: 0.5114 - val_accuracy: 0.7126 - val_loss: 0.5442\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 39ms/step - accuracy: 0.7971 - loss: 0.4638 - val_accuracy: 0.7241 - val_loss: 0.5440\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8135 - loss: 0.4334 - val_accuracy: 0.7494 - val_loss: 0.5238\n",
      "Epoch 10/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8339 - loss: 0.3966 - val_accuracy: 0.7471 - val_loss: 0.5407\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 8s - 654ms/step - accuracy: 0.5194 - loss: 0.7006 - val_accuracy: 0.5080 - val_loss: 0.6884\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.5456 - loss: 0.6871 - val_accuracy: 0.5816 - val_loss: 0.6774\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6126 - loss: 0.6634 - val_accuracy: 0.7126 - val_loss: 0.6552\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.6612 - loss: 0.6372 - val_accuracy: 0.7195 - val_loss: 0.6181\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7058 - loss: 0.5823 - val_accuracy: 0.6874 - val_loss: 0.5784\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7452 - loss: 0.5372 - val_accuracy: 0.6874 - val_loss: 0.5591\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 25ms/step - accuracy: 0.7656 - loss: 0.4910 - val_accuracy: 0.7333 - val_loss: 0.5271\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.7781 - loss: 0.4558 - val_accuracy: 0.7586 - val_loss: 0.5072\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8148 - loss: 0.4167 - val_accuracy: 0.7655 - val_loss: 0.5109\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "    > News :: BiLSTM_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 136ms/step - accuracy: 0.5476 - loss: 0.6872 - val_accuracy: 0.6046 - val_loss: 0.6817\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6323 - loss: 0.6594 - val_accuracy: 0.6552 - val_loss: 0.6540\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.7012 - loss: 0.6106 - val_accuracy: 0.6851 - val_loss: 0.5983\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.7689 - loss: 0.5148 - val_accuracy: 0.7195 - val_loss: 0.5443\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.8030 - loss: 0.4438 - val_accuracy: 0.7218 - val_loss: 0.5219\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.8273 - loss: 0.3927 - val_accuracy: 0.7379 - val_loss: 0.5157\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.8654 - loss: 0.3537 - val_accuracy: 0.7471 - val_loss: 0.5174\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 139ms/step - accuracy: 0.5259 - loss: 0.6907 - val_accuracy: 0.5563 - val_loss: 0.6824\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6211 - loss: 0.6639 - val_accuracy: 0.6483 - val_loss: 0.6583\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.6907 - loss: 0.6064 - val_accuracy: 0.7103 - val_loss: 0.5752\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.7439 - loss: 0.5232 - val_accuracy: 0.7310 - val_loss: 0.5602\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.7905 - loss: 0.4698 - val_accuracy: 0.7448 - val_loss: 0.5338\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.8240 - loss: 0.4116 - val_accuracy: 0.7172 - val_loss: 0.6589\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 161ms/step - accuracy: 0.5522 - loss: 0.6899 - val_accuracy: 0.5264 - val_loss: 0.6796\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.6448 - loss: 0.6474 - val_accuracy: 0.6598 - val_loss: 0.6131\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.7433 - loss: 0.5519 - val_accuracy: 0.7356 - val_loss: 0.5495\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 36ms/step - accuracy: 0.8142 - loss: 0.4581 - val_accuracy: 0.7540 - val_loss: 0.5396\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 37ms/step - accuracy: 0.8529 - loss: 0.3958 - val_accuracy: 0.7356 - val_loss: 0.5408\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "    > News :: BiLSTM_Attn_100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 138ms/step - accuracy: 0.5345 - loss: 0.6912 - val_accuracy: 0.6115 - val_loss: 0.6878\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.5942 - loss: 0.6799 - val_accuracy: 0.6828 - val_loss: 0.6651\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.6520 - loss: 0.6406 - val_accuracy: 0.6851 - val_loss: 0.5971\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7216 - loss: 0.5675 - val_accuracy: 0.7034 - val_loss: 0.5624\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7584 - loss: 0.5244 - val_accuracy: 0.7218 - val_loss: 0.5372\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7978 - loss: 0.4726 - val_accuracy: 0.7425 - val_loss: 0.5200\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.8148 - loss: 0.4462 - val_accuracy: 0.7632 - val_loss: 0.5007\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.8359 - loss: 0.4112 - val_accuracy: 0.7632 - val_loss: 0.4998\n",
      "Epoch 9/20\n",
      "12/12 - 1s - 49ms/step - accuracy: 0.8615 - loss: 0.3696 - val_accuracy: 0.7632 - val_loss: 0.5273\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 142ms/step - accuracy: 0.4898 - loss: 0.6941 - val_accuracy: 0.5057 - val_loss: 0.6922\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5489 - loss: 0.6908 - val_accuracy: 0.5701 - val_loss: 0.6893\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.6244 - loss: 0.6749 - val_accuracy: 0.6506 - val_loss: 0.6628\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.6684 - loss: 0.6227 - val_accuracy: 0.6828 - val_loss: 0.5906\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.7360 - loss: 0.5520 - val_accuracy: 0.7379 - val_loss: 0.5432\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7571 - loss: 0.5108 - val_accuracy: 0.7563 - val_loss: 0.5228\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7938 - loss: 0.4595 - val_accuracy: 0.7540 - val_loss: 0.5170\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 35ms/step - accuracy: 0.8260 - loss: 0.4243 - val_accuracy: 0.7448 - val_loss: 0.5348\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 138ms/step - accuracy: 0.5036 - loss: 0.6930 - val_accuracy: 0.5471 - val_loss: 0.6899\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 31ms/step - accuracy: 0.5817 - loss: 0.6837 - val_accuracy: 0.6253 - val_loss: 0.6761\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.6408 - loss: 0.6526 - val_accuracy: 0.6437 - val_loss: 0.6317\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7144 - loss: 0.5816 - val_accuracy: 0.6943 - val_loss: 0.5851\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.7623 - loss: 0.5127 - val_accuracy: 0.7080 - val_loss: 0.5343\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 32ms/step - accuracy: 0.7991 - loss: 0.4723 - val_accuracy: 0.7402 - val_loss: 0.5319\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 33ms/step - accuracy: 0.8116 - loss: 0.4332 - val_accuracy: 0.7448 - val_loss: 0.5322\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "    > News :: BiLSTM_Attn_300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 153ms/step - accuracy: 0.5272 - loss: 0.6912 - val_accuracy: 0.6276 - val_loss: 0.6862\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6527 - loss: 0.6667 - val_accuracy: 0.6736 - val_loss: 0.6447\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.7085 - loss: 0.5857 - val_accuracy: 0.7149 - val_loss: 0.5620\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.7787 - loss: 0.4868 - val_accuracy: 0.7632 - val_loss: 0.5070\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.8273 - loss: 0.4069 - val_accuracy: 0.7793 - val_loss: 0.5001\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8772 - loss: 0.3428 - val_accuracy: 0.7862 - val_loss: 0.4983\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8890 - loss: 0.2971 - val_accuracy: 0.7816 - val_loss: 0.5019\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 145ms/step - accuracy: 0.5246 - loss: 0.6927 - val_accuracy: 0.5080 - val_loss: 0.6895\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.6179 - loss: 0.6749 - val_accuracy: 0.6621 - val_loss: 0.6586\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7118 - loss: 0.5888 - val_accuracy: 0.7057 - val_loss: 0.5761\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7932 - loss: 0.4838 - val_accuracy: 0.7333 - val_loss: 0.5174\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8398 - loss: 0.4067 - val_accuracy: 0.7540 - val_loss: 0.5322\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 2s - 149ms/step - accuracy: 0.5358 - loss: 0.6883 - val_accuracy: 0.6345 - val_loss: 0.6844\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 41ms/step - accuracy: 0.6271 - loss: 0.6654 - val_accuracy: 0.6943 - val_loss: 0.6480\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.6914 - loss: 0.6055 - val_accuracy: 0.7080 - val_loss: 0.5832\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.7682 - loss: 0.5092 - val_accuracy: 0.7517 - val_loss: 0.5273\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8162 - loss: 0.4269 - val_accuracy: 0.7494 - val_loss: 0.5175\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.8628 - loss: 0.3508 - val_accuracy: 0.7471 - val_loss: 0.5342\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "    > News :: CNN_GloVe100\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5108 - loss: 0.7833 - val_accuracy: 0.5356 - val_loss: 0.6896\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5949 - loss: 0.6661 - val_accuracy: 0.6161 - val_loss: 0.6773\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.6047 - loss: 0.6615 - val_accuracy: 0.6414 - val_loss: 0.6507\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6605 - loss: 0.6256 - val_accuracy: 0.6529 - val_loss: 0.6382\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6986 - loss: 0.5670 - val_accuracy: 0.6667 - val_loss: 0.6040\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7571 - loss: 0.5131 - val_accuracy: 0.6874 - val_loss: 0.5781\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7820 - loss: 0.4723 - val_accuracy: 0.7103 - val_loss: 0.5591\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.8116 - loss: 0.4115 - val_accuracy: 0.7241 - val_loss: 0.5518\n",
      "Epoch 9/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.8588 - loss: 0.3589 - val_accuracy: 0.7195 - val_loss: 0.5675\n",
      "Epoch 9: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 70ms/step - accuracy: 0.5135 - loss: 0.8010 - val_accuracy: 0.5724 - val_loss: 0.6814\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5634 - loss: 0.6914 - val_accuracy: 0.5333 - val_loss: 0.6828\n",
      "Epoch 2: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 69ms/step - accuracy: 0.5023 - loss: 0.8535 - val_accuracy: 0.4966 - val_loss: 0.7329\n",
      "Epoch 2/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.5988 - loss: 0.6737 - val_accuracy: 0.6161 - val_loss: 0.6780\n",
      "Epoch 3/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6133 - loss: 0.6519 - val_accuracy: 0.5977 - val_loss: 0.6730\n",
      "Epoch 4/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.6605 - loss: 0.6188 - val_accuracy: 0.6575 - val_loss: 0.6502\n",
      "Epoch 5/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.7039 - loss: 0.5757 - val_accuracy: 0.6897 - val_loss: 0.6151\n",
      "Epoch 6/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7380 - loss: 0.5285 - val_accuracy: 0.6897 - val_loss: 0.5904\n",
      "Epoch 7/20\n",
      "12/12 - 0s - 19ms/step - accuracy: 0.7695 - loss: 0.4857 - val_accuracy: 0.6920 - val_loss: 0.5793\n",
      "Epoch 8/20\n",
      "12/12 - 0s - 18ms/step - accuracy: 0.8050 - loss: 0.4269 - val_accuracy: 0.6989 - val_loss: 0.5796\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "    > News :: CNN_GloVe300\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 93ms/step - accuracy: 0.5338 - loss: 0.7675 - val_accuracy: 0.5379 - val_loss: 0.6729\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6513 - loss: 0.6234 - val_accuracy: 0.6575 - val_loss: 0.6395\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 46ms/step - accuracy: 0.7104 - loss: 0.5641 - val_accuracy: 0.7149 - val_loss: 0.5913\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.7617 - loss: 0.4836 - val_accuracy: 0.7425 - val_loss: 0.5500\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8247 - loss: 0.3922 - val_accuracy: 0.7448 - val_loss: 0.5210\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8844 - loss: 0.2980 - val_accuracy: 0.7586 - val_loss: 0.5030\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.9232 - loss: 0.2174 - val_accuracy: 0.7517 - val_loss: 0.5140\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 95ms/step - accuracy: 0.5483 - loss: 0.7372 - val_accuracy: 0.6023 - val_loss: 0.6738\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.6343 - loss: 0.6308 - val_accuracy: 0.6460 - val_loss: 0.6507\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 42ms/step - accuracy: 0.7131 - loss: 0.5619 - val_accuracy: 0.6920 - val_loss: 0.6028\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7774 - loss: 0.4762 - val_accuracy: 0.6989 - val_loss: 0.5691\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8280 - loss: 0.3919 - val_accuracy: 0.7241 - val_loss: 0.5579\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8785 - loss: 0.3099 - val_accuracy: 0.7356 - val_loss: 0.5323\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.9028 - loss: 0.2476 - val_accuracy: 0.7379 - val_loss: 0.5337\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 1s - 95ms/step - accuracy: 0.5161 - loss: 0.8108 - val_accuracy: 0.5264 - val_loss: 0.6822\n",
      "Epoch 2/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.6487 - loss: 0.6298 - val_accuracy: 0.6667 - val_loss: 0.6494\n",
      "Epoch 3/20\n",
      "12/12 - 1s - 45ms/step - accuracy: 0.7315 - loss: 0.5493 - val_accuracy: 0.6920 - val_loss: 0.6080\n",
      "Epoch 4/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.7807 - loss: 0.4728 - val_accuracy: 0.7011 - val_loss: 0.5666\n",
      "Epoch 5/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.8345 - loss: 0.3734 - val_accuracy: 0.7149 - val_loss: 0.5489\n",
      "Epoch 6/20\n",
      "12/12 - 1s - 43ms/step - accuracy: 0.8917 - loss: 0.2955 - val_accuracy: 0.7471 - val_loss: 0.5259\n",
      "Epoch 7/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.9225 - loss: 0.2280 - val_accuracy: 0.7356 - val_loss: 0.5204\n",
      "Epoch 8/20\n",
      "12/12 - 1s - 44ms/step - accuracy: 0.9534 - loss: 0.1513 - val_accuracy: 0.7425 - val_loss: 0.5462\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\n",
      "=== Classic DL — Per-domain summary (mean ± std) ===\n",
      " domain           model  f1_mean  f1_std  acc_mean  acc_std  prec_mean  prec_std  rec_mean  rec_std  auprc_mean  auprc_std  lat_ms_mean  lat_ms_std  ece_mean  ece_std  brier_mean  brier_std  mis_total  mis_neg  mis_excl  mis_hyp  mis_over  runs\n",
      "   News BiLSTM_Attn_300    0.760   0.035     0.761    0.037      0.770     0.059     0.757    0.076       0.828      0.072        1.154       0.012     0.317    0.040       0.167      0.025        469       41         5       17         1     9\n",
      "   News BiLSTM_GloVe300    0.741   0.037     0.730    0.048      0.720     0.065     0.770    0.052       0.799      0.062        1.100       0.065     0.294    0.032       0.190      0.035        529       48         7       17         2     9\n",
      "   News BiLSTM_Attn_100    0.733   0.067     0.742    0.052      0.758     0.059     0.722    0.125       0.817      0.061        1.143       0.041     0.303    0.040       0.175      0.030        506       40         6       12         1     9\n",
      "   News BiLSTM_GloVe100    0.732   0.054     0.742    0.043      0.763     0.056     0.712    0.097       0.810      0.056        1.074       0.080     0.297    0.041       0.175      0.022        506       37         3       13         2     9\n",
      "   News    CNN_GloVe300    0.718   0.102     0.740    0.060      0.775     0.068     0.694    0.157       0.805      0.055        0.531       0.019     0.313    0.046       0.178      0.030        511       53         4       15         3     9\n",
      "   News    CNN_GloVe100    0.646   0.102     0.652    0.095      0.702     0.137     0.662    0.212       0.710      0.112        0.436       0.008     0.212    0.123       0.212      0.037        682       69         4       20         3     9\n",
      " Reddit BiLSTM_Attn_100    0.629   0.053     0.564    0.043      0.558     0.052     0.761    0.181       0.594      0.073        2.604       4.441     0.069    0.047       0.243      0.008        855      188        83       38         9     9\n",
      " Reddit BiLSTM_GloVe300    0.591   0.039     0.602    0.030      0.611     0.047     0.579    0.081       0.675      0.051        1.072       0.020     0.125    0.029       0.233      0.010        781      183        73       46        14     9\n",
      " Reddit BiLSTM_GloVe100    0.590   0.050     0.599    0.036      0.611     0.052     0.587    0.113       0.662      0.055        1.062       0.071     0.107    0.045       0.236      0.011        787      191        72       50         8     9\n",
      " Reddit BiLSTM_Attn_300    0.553   0.096     0.588    0.040      0.604     0.052     0.535    0.162       0.649      0.062        1.176       0.037     0.110    0.046       0.236      0.014        809      179        86       56        11     9\n",
      " Reddit    CNN_GloVe300    0.549   0.113     0.564    0.048      0.573     0.046     0.568    0.204       0.634      0.062        0.546       0.075     0.099    0.039       0.240      0.010        855      204        85       51        13     9\n",
      " Reddit    CNN_GloVe100    0.548   0.122     0.530    0.059      0.525     0.071     0.606    0.203       0.574      0.064        0.447       0.027     0.060    0.039       0.247      0.004        922      226        91       47        15     9\n",
      "Twitter    CNN_GloVe100    0.620   0.044     0.536    0.053      0.533     0.045     0.769    0.157       0.590      0.071        0.435       0.009     0.081    0.050       0.246      0.012        910      232       123       95        10     9\n",
      "Twitter    CNN_GloVe300    0.589   0.050     0.553    0.059      0.554     0.060     0.649    0.128       0.630      0.060        0.525       0.010     0.108    0.039       0.240      0.011        877      228       113      103        12     9\n",
      "Twitter BiLSTM_GloVe300    0.589   0.045     0.579    0.035      0.581     0.044     0.612    0.106       0.664      0.030        1.074       0.053     0.124    0.027       0.239      0.009        826      236       116       98         7     9\n",
      "Twitter BiLSTM_GloVe100    0.572   0.081     0.550    0.040      0.543     0.042     0.619    0.149       0.612      0.054        1.106       0.186     0.074    0.033       0.245      0.007        882      247       116      101        10     9\n",
      "Twitter BiLSTM_Attn_100    0.550   0.091     0.543    0.045      0.562     0.073     0.596    0.214       0.569      0.040        1.121       0.017     0.072    0.029       0.249      0.006        896      223       123      113         6     9\n",
      "Twitter BiLSTM_Attn_300    0.535   0.082     0.549    0.047      0.574     0.083     0.544    0.175       0.594      0.037        3.483       6.960     0.099    0.046       0.249      0.007        884      231       120      115         7     9\n",
      "\n",
      "=== Classic DL — Overall (grand mean over domains × variants × seeds) ===\n",
      "          model  f1_mean  f1_std  acc_mean  acc_std  prec_mean  prec_std  rec_mean  rec_std  auprc_mean  auprc_std  lat_ms_mean  lat_ms_std  ece_mean  ece_std  brier_mean  brier_std  mis_total  mis_neg  mis_excl  mis_hyp  mis_over  runs\n",
      "BiLSTM_GloVe300    0.640   0.083     0.637    0.077      0.637     0.079     0.653    0.116       0.713      0.078        1.082       0.049     0.181    0.086       0.220      0.030       2136      467       196      161        23    27\n",
      "BiLSTM_Attn_100    0.637   0.103     0.617    0.101      0.626     0.112     0.693    0.185       0.660      0.127        1.623       2.563     0.148    0.118       0.222      0.039       2257      451       212      163        16    27\n",
      "BiLSTM_GloVe100    0.631   0.095     0.630    0.091      0.639     0.106     0.639    0.129       0.695      0.101        1.081       0.120     0.159    0.107       0.219      0.035       2175      475       191      164        20    27\n",
      "   CNN_GloVe300    0.619   0.115     0.619    0.102      0.634     0.116     0.637    0.168       0.690      0.101        0.534       0.044     0.174    0.108       0.219      0.035       2243      485       202      169        28    27\n",
      "BiLSTM_Attn_300    0.616   0.127     0.633    0.102      0.649     0.108     0.612    0.174       0.690      0.117        1.938       4.018     0.175    0.111       0.217      0.040       2162      451       211      188        19    27\n",
      "   CNN_GloVe100    0.604   0.101     0.573    0.089      0.587     0.122     0.679    0.197       0.624      0.102        0.439       0.017     0.118    0.103       0.235      0.027       2514      527       218      162        28    27\n",
      "\n",
      "All artifacts saved under: /Users/evelinaivanova/Dissertation/sarcasm_balanced_runs_v3_equal_classic/run_20250830_053556\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Classic DL Suite — Per-Domain Balanced & Equalized (Reddit/Twitter/News)\n",
    "# - For each seed: balance each domain 50/50, cap all domains to same N\n",
    "# - 70/20/10 stratified split per domain/seed\n",
    "# - Models: BiLSTM, BiLSTM+Attention, CNN with GloVe (100d, 300d)\n",
    "# - 3 seeds, early stopping (patience=1), best weights\n",
    "# - Full artifacts + per-domain and overall aggregates (mean ± std)\n",
    "# ===========================================\n",
    "import os, json, time, random, re, math, datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Embedding, Bidirectional, LSTM, Dense, Dropout,\n",
    "                                     GlobalMaxPooling1D, Conv1D, Input, Concatenate)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, brier_score_loss,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "DATA_CSV       = \"df_filtered.csv\"   # expects columns: text, label, source\n",
    "TEXT_COL       = \"text\"\n",
    "LABEL_COL      = \"label\"             # 0 = non-sarc, 1 = sarc\n",
    "SOURCE_COL     = \"source\"            # values like Twitter / Reddit / News\n",
    "\n",
    "DOMAINS        = [\"Twitter\", \"Reddit\", \"News\"]\n",
    "SEEDS_BAL      = [0, 1, 2]\n",
    "RNG_BASE       = 42\n",
    "\n",
    "MAX_LEN        = 40       # classic DL token length (EDA-backed)\n",
    "BATCH_SIZE     = 128\n",
    "MAX_EPOCHS     = 20\n",
    "PATIENCE       = 1\n",
    "\n",
    "N_CALIB_BINS   = 10\n",
    "BOOT_ITERS     = 500\n",
    "\n",
    "GLOVE_100_PATH = \"glove.6B.100d.txt\"\n",
    "GLOVE_300_PATH = \"glove.6B.300d.txt\"\n",
    "\n",
    "RUN_TAG   = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_BASE  = Path(\"./sarcasm_balanced_runs_v3_equal_classic\")\n",
    "OUT_DIR   = OUT_BASE / f\"run_{RUN_TAG}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Repro\n",
    "# -------------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# -------------------------------\n",
    "# Heuristic flags (for misclass analysis)\n",
    "# -------------------------------\n",
    "_NEGATION_RE   = re.compile(r\"\\b(no|not|never|n't|cannot|can't|won't|don'?t)\\b\", re.IGNORECASE)\n",
    "_EXCLAM_RE     = re.compile(r\"!{1,}\")\n",
    "_HYPERBOLE_RE  = re.compile(r\"\\b(always|never|literally|absolutely|everyone|no one|best|worst|totally|completely)\\b\", re.IGNORECASE)\n",
    "_OVERCONF_RE   = re.compile(r\"\\b(of course|obviously|clearly|as everyone knows|without a doubt)\\b\", re.IGNORECASE)\n",
    "def pattern_flags(txt: str):\n",
    "    return {\n",
    "        \"negation\": bool(_NEGATION_RE.search(txt or \"\")),\n",
    "        \"exclamation\": bool(_EXCLAM_RE.search(txt or \"\")),\n",
    "        \"hyperbole\": bool(_HYPERBOLE_RE.search(txt or \"\")),\n",
    "        \"overconfidence\": bool(_OVERCONF_RE.search(txt or \"\")),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# Calibration + PR utilities\n",
    "# -------------------------------\n",
    "def ece_score_bin(probs, labels, n_bins=10):\n",
    "    probs = np.asarray(probs, dtype=float)\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    rows, ece = [], 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        mask = (probs >= lo) & (probs < hi) if i < n_bins-1 else (probs >= lo) & (probs <= hi)\n",
    "        if not np.any(mask):\n",
    "            rows.append((float(lo), float(hi), 0, np.nan, np.nan)); continue\n",
    "        p = probs[mask]; y = labels[mask]\n",
    "        conf = p.mean()\n",
    "        acc  = ((p >= 0.5).astype(int) == y).mean()\n",
    "        w    = mask.mean()\n",
    "        ece += w * abs(acc - conf)\n",
    "        rows.append((float(lo), float(hi), int(mask.sum()), float(acc), float(conf)))\n",
    "    return float(ece), pd.DataFrame(rows, columns=[\"bin_lo\",\"bin_hi\",\"count\",\"bin_acc\",\"bin_conf\"])\n",
    "\n",
    "def pr_curve_and_auprc(y_true, p_pos, out_png, title=\"PR Curve\"):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, p_pos)\n",
    "    auprc = average_precision_score(y_true, p_pos)\n",
    "    plt.figure(figsize=(4.6,4.0))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{title} (AUPRC={auprc:.3f})\")\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=160); plt.close()\n",
    "    return float(auprc)\n",
    "\n",
    "def bootstrap_metric_ci(y_true, y_pred, p_pos, iters=500, alpha=0.05, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    n = len(y_true)\n",
    "    def once(idx):\n",
    "        yt, yp, pp = y_true[idx], y_pred[idx], p_pos[idx]\n",
    "        return (f1_score(yt, yp, zero_division=0),\n",
    "                accuracy_score(yt, yp),\n",
    "                precision_score(yt, yp, zero_division=0),\n",
    "                recall_score(yt, yp, zero_division=0),\n",
    "                average_precision_score(yt, pp))\n",
    "    f1s, accs, precs, recs, auprcs = [], [], [], [], []\n",
    "    for _ in range(iters):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        f1, acc, prc, rec, au = once(idx)\n",
    "        f1s.append(f1); accs.append(acc); precs.append(prc); recs.append(rec); auprcs.append(au)\n",
    "    def ci(arr):\n",
    "        arr = np.sort(arr); lo = int((alpha/2)*iters); hi = int((1-alpha/2)*iters)-1\n",
    "        return float(arr[lo]), float(arr[hi])\n",
    "    return {\"f1_ci\":ci(f1s),\"acc_ci\":ci(accs),\"prec_ci\":ci(precs),\"rec_ci\":ci(recs),\"auprc_ci\":ci(auprcs)}\n",
    "\n",
    "# -------------------------------\n",
    "# Balancing + equalisation\n",
    "# -------------------------------\n",
    "def downsample_balance(df_src: pd.DataFrame, label_col: str, seed: int) -> pd.DataFrame:\n",
    "    df0 = df_src[df_src[label_col] == 0]\n",
    "    df1 = df_src[df_src[label_col] == 1]\n",
    "    if len(df0)==0 or len(df1)==0:\n",
    "        raise ValueError(\"One class empty in source; cannot balance.\")\n",
    "    minority = min(len(df0), len(df1))\n",
    "    if len(df0) > len(df1):\n",
    "        df0_bal = df0.sample(n=minority, random_state=seed, replace=False); df1_bal = df1\n",
    "    else:\n",
    "        df1_bal = df1.sample(n=minority, random_state=seed, replace=False); df0_bal = df0\n",
    "    return (pd.concat([df0_bal, df1_bal])\n",
    "              .sample(frac=1.0, random_state=seed).reset_index(drop=True))\n",
    "\n",
    "def equalize_domains_by_cap(balanced_by_domain: dict, seed: int):\n",
    "    totals = {d: len(df) for d, df in balanced_by_domain.items()}\n",
    "    cap = min(totals.values())\n",
    "    if cap % 2 == 1: cap -= 1\n",
    "    out = {}\n",
    "    for d, df in balanced_by_domain.items():\n",
    "        per_class = cap // 2\n",
    "        df0 = df[df[LABEL_COL]==0].sample(n=per_class, random_state=seed, replace=False)\n",
    "        df1 = df[df[LABEL_COL]==1].sample(n=per_class, random_state=seed, replace=False)\n",
    "        out[d] = (pd.concat([df0, df1])\n",
    "                    .sample(frac=1.0, random_state=seed).reset_index(drop=True))\n",
    "    return out, cap\n",
    "\n",
    "def split_70_20_10(df_bal: pd.DataFrame, seed: int):\n",
    "    X = df_bal[TEXT_COL].astype(str).tolist()\n",
    "    y = df_bal[LABEL_COL].astype(int).to_numpy()\n",
    "    X_tr, X_tmp, y_tr, y_tmp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=seed)\n",
    "    X_va, X_te, y_va, y_te = train_test_split(X_tmp, y_tmp, test_size=(1/3), stratify=y_tmp, random_state=seed)\n",
    "    return X_tr, y_tr, X_va, y_va, X_te, y_te\n",
    "\n",
    "# -------------------------------\n",
    "# GloVe loading (once)\n",
    "# -------------------------------\n",
    "def load_glove(path):\n",
    "    emb_index = {}\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            vals = line.rstrip().split(\" \")\n",
    "            emb_index[vals[0]] = np.asarray(vals[1:], dtype=\"float32\")\n",
    "    return emb_index\n",
    "\n",
    "print(\"[info] Loading GloVe ...\")\n",
    "GLOVE100 = load_glove(GLOVE_100_PATH)\n",
    "GLOVE300 = load_glove(GLOVE_300_PATH)\n",
    "print(\"[info] GloVe loaded.\")\n",
    "\n",
    "def build_embedding_matrix(tokenizer, emb_index, emb_dim):\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "    matrix = np.zeros((vocab_size, emb_dim), dtype=\"float32\")\n",
    "    for w, i in word_index.items():\n",
    "        vec = emb_index.get(w)\n",
    "        if vec is not None: matrix[i] = vec\n",
    "    return matrix, vocab_size\n",
    "\n",
    "# -------------------------------\n",
    "# Model builders (frozen embeddings)\n",
    "# -------------------------------\n",
    "def build_bilstm(emb_matrix, max_len):\n",
    "    emb_dim = emb_matrix.shape[1]\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=emb_matrix.shape[0], output_dim=emb_dim,\n",
    "                  weights=[emb_matrix], input_length=max_len, trainable=False),\n",
    "        Bidirectional(LSTM(64, return_sequences=False)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class AdditiveAttention(layers.Layer):\n",
    "    def __init__(self, units=64, **kwargs): super().__init__(**kwargs); self.W = layers.Dense(units, activation='tanh'); self.v = layers.Dense(1, use_bias=False)\n",
    "    def call(self, h, mask=None):\n",
    "        score = self.v(self.W(h))\n",
    "        weights = tf.nn.softmax(score, axis=1)\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask[:, :, tf.newaxis], tf.float32)\n",
    "            weights = weights * mask\n",
    "            weights = weights / (tf.reduce_sum(weights, axis=1, keepdims=True) + 1e-8)\n",
    "        return tf.reduce_sum(weights * h, axis=1)\n",
    "\n",
    "def build_bilstm_attention(emb_matrix, max_len):\n",
    "    emb_dim = emb_matrix.shape[1]\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    emb = Embedding(input_dim=emb_matrix.shape[0], output_dim=emb_dim,\n",
    "                    weights=[emb_matrix], input_length=max_len, trainable=False, mask_zero=False)(inp)\n",
    "    h = Bidirectional(LSTM(64, return_sequences=True))(emb)\n",
    "    att = AdditiveAttention(64)(h)\n",
    "    x = Dropout(0.5)(att); x = Dense(32, activation='relu')(x); x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn(emb_matrix, max_len):\n",
    "    emb_dim = emb_matrix.shape[1]\n",
    "    inp = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    emb = Embedding(input_dim=emb_matrix.shape[0], output_dim=emb_dim,\n",
    "                    weights=[emb_matrix], input_length=max_len, trainable=False)(inp)\n",
    "    convs = []\n",
    "    for k in (3,4,5):\n",
    "        c = Conv1D(filters=128, kernel_size=k, activation='relu', padding='valid')(emb)\n",
    "        p = GlobalMaxPooling1D()(c); convs.append(p)\n",
    "    x = Concatenate()(convs)\n",
    "    x = Dropout(0.5)(x); x = Dense(64, activation='relu')(x); x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "MODEL_SPECS = [\n",
    "    (\"BiLSTM_GloVe100\",      build_bilstm,           100),\n",
    "    (\"BiLSTM_GloVe300\",      build_bilstm,           300),\n",
    "    (\"BiLSTM_Attn_100\",      build_bilstm_attention, 100),\n",
    "    (\"BiLSTM_Attn_300\",      build_bilstm_attention, 300),\n",
    "    (\"CNN_GloVe100\",         build_cnn,              100),\n",
    "    (\"CNN_GloVe300\",         build_cnn,              300),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Load & normalize sources\n",
    "# -------------------------------\n",
    "df_full = pd.read_csv(DATA_CSV)\n",
    "assert {TEXT_COL, LABEL_COL, SOURCE_COL}.issubset(df_full.columns), \\\n",
    "    f\"CSV must have columns: {TEXT_COL}, {LABEL_COL}, {SOURCE_COL}\"\n",
    "\n",
    "df_full[SOURCE_COL] = (\n",
    "    df_full[SOURCE_COL].astype(str).str.strip().str.lower()\n",
    "      .str.replace(r\"[\\s\\-]+\", \"_\", regex=True)\n",
    ")\n",
    "\n",
    "SOURCE_MAP = {\n",
    "    \"twitter\":\"Twitter\",\"tw\":\"Twitter\",\n",
    "    \"reddit\":\"Reddit\",\n",
    "    \"news\":\"News\",\"headline\":\"News\",\n",
    "    \"news_headline\":\"News\",\"news_headlines\":\"News\",\n",
    "    \"newsheadline\":\"News\",\"newsheadlines\":\"News\",\n",
    "}\n",
    "df_full[SOURCE_COL] = df_full[SOURCE_COL].map(SOURCE_MAP).fillna(df_full[SOURCE_COL].str.title())\n",
    "\n",
    "print(\"Domain counts before balancing:\")\n",
    "print(df_full[SOURCE_COL].value_counts())\n",
    "\n",
    "with open(OUT_DIR/\"_run_info.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"data_csv\": DATA_CSV, \"domains\": DOMAINS, \"seeds_bal\": SEEDS_BAL,\n",
    "        \"models\": [m for m,_,_ in MODEL_SPECS],\n",
    "        \"max_len\": MAX_LEN, \"batch_size\": BATCH_SIZE, \"max_epochs\": MAX_EPOCHS,\n",
    "        \"patience\": PATIENCE, \"n_calib_bins\": N_CALIB_BINS,\n",
    "        \"bootstrap_iters\": BOOT_ITERS,\n",
    "        \"equalize_strategy\": \"per seed: cap each domain to min(balanced totals); per-class downsample to cap/2\"\n",
    "    }, f, indent=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Main loop (balanced/equalized per seed/domain)\n",
    "# -------------------------------\n",
    "rows_all = []\n",
    "for bal_seed in SEEDS_BAL:\n",
    "    print(f\"\\n===== Balanced variant seed={bal_seed} =====\")\n",
    "    # 1) 50/50 within-domain\n",
    "    balanced_raw = {}\n",
    "    for domain in DOMAINS:\n",
    "        dfd = df_full[df_full[SOURCE_COL]==domain].copy()\n",
    "        if dfd.empty:\n",
    "            print(f\"[warn] No rows for domain '{domain}'. Skipping.\")\n",
    "            continue\n",
    "        balanced_raw[domain] = downsample_balance(dfd, LABEL_COL, seed=bal_seed)\n",
    "        print(f\"  {domain}: balanced 50/50 → {len(balanced_raw[domain])} rows\")\n",
    "\n",
    "    # 2) Equalize to same total N across domains\n",
    "    balanced_equal, cap_total = equalize_domains_by_cap(balanced_raw, seed=RNG_BASE+bal_seed)\n",
    "    print(f\"  Equalized cap across domains → N per domain = {cap_total} (each class {cap_total//2})\")\n",
    "\n",
    "    # 3) Per domain → split, tokenize(train-only), embed, train each model across seeds\n",
    "    seed_dir = OUT_DIR / f\"bal_seed_{bal_seed}\"\n",
    "    seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save equalized CSVs\n",
    "    for domain, df_eq in balanced_equal.items():\n",
    "        df_eq.to_csv(seed_dir / f\"{domain}_equalized_50_50.csv\", index=False)\n",
    "\n",
    "    for domain, df_eq in balanced_equal.items():\n",
    "        X_tr, y_tr, X_va, y_va, X_te, y_te = split_70_20_10(df_eq, seed=RNG_BASE+bal_seed)\n",
    "        dom_dir = seed_dir / f\"domain_{domain}\"\n",
    "        dom_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"split\": [\"train\"]*len(X_tr) + [\"val\"]*len(X_va) + [\"test\"]*len(X_te),\n",
    "            \"text\":  X_tr + X_va + X_te,\n",
    "            \"label\": list(y_tr) + list(y_va) + list(y_te)\n",
    "        }).to_csv(dom_dir/\"data_split.csv\", index=False)\n",
    "\n",
    "        # Fit tokenizer on TRAIN only to avoid leakage\n",
    "        tok = Tokenizer(oov_token=\"<OOV>\")\n",
    "        tok.fit_on_texts(X_tr)\n",
    "        def to_seq(texts): return pad_sequences(tok.texts_to_sequences(texts), maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "        Xtr = to_seq(X_tr); Xva = to_seq(X_va); Xte = to_seq(X_te)\n",
    "\n",
    "        # Build embeddings per embedding size (based on the tokenizer's vocab)\n",
    "        emb100, _ = build_embedding_matrix(tok, GLOVE100, 100)\n",
    "        emb300, _ = build_embedding_matrix(tok, GLOVE300, 300)\n",
    "\n",
    "        # Small helper: select matrix by spec\n",
    "        def get_emb(dim): return emb100 if dim==100 else emb300\n",
    "\n",
    "        for model_name, builder, emb_dim in MODEL_SPECS:\n",
    "            print(f\"    > {domain} :: {model_name}\")\n",
    "            mdl_dir = dom_dir / model_name\n",
    "            mdl_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            per_seed_rows = []\n",
    "            for seed in [13,17,23]:\n",
    "                set_all_seeds(seed)\n",
    "                run_dir = mdl_dir / f\"seed_{seed}\"\n",
    "                run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                model = builder(get_emb(emb_dim), MAX_LEN)\n",
    "                es = EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True, verbose=1)\n",
    "\n",
    "                t0 = time.time()\n",
    "                hist = model.fit(Xtr, y_tr, validation_data=(Xva, y_va),\n",
    "                                 epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, verbose=2, callbacks=[es])\n",
    "                train_time = time.time() - t0\n",
    "\n",
    "                # Predict test\n",
    "                t1 = time.time()\n",
    "                probs_test = model.predict(Xte, batch_size=BATCH_SIZE).reshape(-1)\n",
    "                infer_time = time.time() - t1\n",
    "                preds_test = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "                # Metrics\n",
    "                acc  = accuracy_score(y_te, preds_test)\n",
    "                prec = precision_score(y_te, preds_test, zero_division=0)\n",
    "                rec  = recall_score(y_te, preds_test, zero_division=0)\n",
    "                f1   = f1_score(y_te, preds_test, zero_division=0)\n",
    "                auprc = average_precision_score(y_te, probs_test)\n",
    "                brier = brier_score_loss(y_te, probs_test)\n",
    "                ece, ece_table = ece_score_bin(probs_test, y_te, n_bins=N_CALIB_BINS)\n",
    "\n",
    "                # PR curve\n",
    "                pr_png = run_dir/\"pr_curve.png\"\n",
    "                auprc_plot = pr_curve_and_auprc(y_te, probs_test, pr_png, title=f\"{domain}-{model_name} PR\")\n",
    "\n",
    "                # Confusion matrix & report\n",
    "                cm = confusion_matrix(y_te, preds_test)\n",
    "                report_df = pd.DataFrame(classification_report(y_te, preds_test, output_dict=True, zero_division=0)).T\n",
    "\n",
    "                # Misclassified + heuristic tags\n",
    "                mis_mask = (preds_test != y_te)\n",
    "                texts_test = np.array(X_te, dtype=object)  # sequences only; also want raw text:\n",
    "                # map back raw text for test:\n",
    "                # ( already saved split CSV; read raw text from there)\n",
    "                raw_te = df_eq.iloc[len(X_tr)+len(X_va):][\"text\"].tolist()  # aligns with split order\n",
    "                df_miss = pd.DataFrame({\n",
    "                    \"text\": np.array(raw_te, dtype=object)[mis_mask],\n",
    "                    \"label_true\": np.array(y_te)[mis_mask],\n",
    "                    \"prob_pred\": probs_test[mis_mask],\n",
    "                    \"pred_label\": preds_test[mis_mask]\n",
    "                }).reset_index(drop=True)\n",
    "                for k in [\"negation\",\"exclamation\",\"hyperbole\",\"overconfidence\"]:\n",
    "                    df_miss[k] = df_miss[\"text\"].apply(lambda t, kk=k: pattern_flags(t)[kk])\n",
    "\n",
    "                # Bootstrap CIs\n",
    "                ci = bootstrap_metric_ci(np.array(y_te), preds_test, probs_test, iters=BOOT_ITERS, rng=seed)\n",
    "\n",
    "                # Save artifacts\n",
    "                np.save(run_dir/\"probs_test.npy\", probs_test)\n",
    "                np.save(run_dir/\"preds_test.npy\", preds_test)\n",
    "                np.save(run_dir/\"y_test.npy\",     np.array(y_te))\n",
    "\n",
    "                report_df.to_csv(run_dir/\"classification_report.csv\")\n",
    "                pd.DataFrame(cm, index=[0,1], columns=[0,1]).to_csv(run_dir/\"confusion_matrix.csv\")\n",
    "                ece_table.to_csv(run_dir/\"calibration_bins.csv\", index=False)\n",
    "                df_miss.to_csv(run_dir/\"misclassified.csv\", index=False)\n",
    "\n",
    "                meta = {\n",
    "                    \"domain\": domain, \"model\": model_name, \"seed\": seed,\n",
    "                    \"epochs_run\": int(len(hist.history.get(\"loss\", []))),\n",
    "                    \"train_time_sec\": float(train_time),\n",
    "                    \"infer_time_sec\": float(infer_time),\n",
    "                    \"infer_avg_ms_per_sample\": float((infer_time / len(y_te)) * 1000.0),\n",
    "                    \"acc\": float(acc), \"precision\": float(prec), \"recall\": float(rec), \"f1\": float(f1),\n",
    "                    \"pr_auc\": float(auprc), \"brier\": float(brier), \"ece_10bin\": float(ece),\n",
    "                    \"bootstrap_ci\": ci\n",
    "                }\n",
    "                with open(run_dir/\"meta.json\",\"w\") as f: json.dump(meta, f, indent=2)\n",
    "                per_seed_rows.append(meta)\n",
    "\n",
    "                # Row for global aggregation\n",
    "                rows_all.append({\n",
    "                    \"bal_seed\": bal_seed, \"domain\": domain, \"model\": model_name, \"seed\": seed,\n",
    "                    \"f1\": meta[\"f1\"], \"acc\": meta[\"acc\"], \"prec\": meta[\"precision\"], \"rec\": meta[\"recall\"],\n",
    "                    \"auprc\": meta[\"pr_auc\"], \"latency_ms\": meta[\"infer_avg_ms_per_sample\"],\n",
    "                    \"size_mb\": np.nan,  # (Keras .h5 not saved here; size optional for classic DL)\n",
    "                    \"ece\": meta[\"ece_10bin\"], \"brier\": meta[\"brier\"],\n",
    "                    \"mis_total\": int((preds_test != y_te).sum()),\n",
    "                    \"mis_neg\": int(((preds_test != y_te) & np.array([pattern_flags(t)[\"negation\"] for t in raw_te])).sum()),\n",
    "                    \"mis_excl\": int(((preds_test != y_te) & np.array([pattern_flags(t)[\"exclamation\"] for t in raw_te])).sum()),\n",
    "                    \"mis_hyp\": int(((preds_test != y_te) & np.array([pattern_flags(t)[\"hyperbole\"] for t in raw_te])).sum()),\n",
    "                    \"mis_over\": int(((preds_test != y_te) & np.array([pattern_flags(t)[\"overconfidence\"] for t in raw_te])).sum()),\n",
    "                })\n",
    "\n",
    "            # Per-model (domain) seed summary\n",
    "            df_seeds = pd.DataFrame(per_seed_rows)\n",
    "            df_seeds.to_csv(mdl_dir/\"per_seed_metrics.csv\", index=False)\n",
    "\n",
    "            def mean_std(col):\n",
    "                return float(df_seeds[col].mean()), float(df_seeds[col].std(ddof=1)) if len(df_seeds)>1 else 0.0\n",
    "\n",
    "            agg = {}\n",
    "            for col in [\"acc\",\"precision\",\"recall\",\"f1\",\"pr_auc\",\"brier\",\"ece_10bin\",\"train_time_sec\",\"infer_time_sec\",\"infer_avg_ms_per_sample\"]:\n",
    "                m, s = mean_std(col)\n",
    "                agg[col] = {\"mean\": m, \"std\": s}\n",
    "            with open(mdl_dir/\"aggregate_mean_std.json\",\"w\") as f: json.dump(agg, f, indent=2)\n",
    "\n",
    "# -------------------------------\n",
    "# Aggregate summaries (all domains/seeds/models)\n",
    "# -------------------------------\n",
    "df_all = pd.DataFrame(rows_all)\n",
    "df_all.to_csv(OUT_DIR/\"runs_detailed_all_classic.csv\", index=False)\n",
    "\n",
    "def agg_mean_std(df, group_cols):\n",
    "    return (df.groupby(group_cols)\n",
    "              .agg(f1_mean=(\"f1\",\"mean\"), f1_std=(\"f1\",\"std\"),\n",
    "                   acc_mean=(\"acc\",\"mean\"), acc_std=(\"acc\",\"std\"),\n",
    "                   prec_mean=(\"prec\",\"mean\"), prec_std=(\"prec\",\"std\"),\n",
    "                   rec_mean=(\"rec\",\"mean\"), rec_std=(\"rec\",\"std\"),\n",
    "                   auprc_mean=(\"auprc\",\"mean\"), auprc_std=(\"auprc\",\"std\"),\n",
    "                   lat_ms_mean=(\"latency_ms\",\"mean\"), lat_ms_std=(\"latency_ms\",\"std\"),\n",
    "                   ece_mean=(\"ece\",\"mean\"), ece_std=(\"ece\",\"std\"),\n",
    "                   brier_mean=(\"brier\",\"mean\"), brier_std=(\"brier\",\"std\"),\n",
    "                   mis_total=(\"mis_total\",\"sum\"),\n",
    "                   mis_neg=(\"mis_neg\",\"sum\"),\n",
    "                   mis_excl=(\"mis_excl\",\"sum\"),\n",
    "                   mis_hyp=(\"mis_hyp\",\"sum\"),\n",
    "                   mis_over=(\"mis_over\",\"sum\"),\n",
    "                   runs=(\"f1\",\"count\"))\n",
    "              .reset_index())\n",
    "\n",
    "# Per-domain summary across seeds\n",
    "summary_dom = agg_mean_std(df_all, [\"domain\",\"model\"]).sort_values([\"domain\",\"f1_mean\"], ascending=[True, False])\n",
    "summary_dom.round(3).to_csv(OUT_DIR/\"summary_by_domain_classic.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Classic DL — Per-domain summary (mean ± std) ===\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(summary_dom.round(3).to_string(index=False))\n",
    "\n",
    "# Overall model means\n",
    "summary_model = agg_mean_std(df_all, [\"model\"]).sort_values(\"f1_mean\", ascending=False)\n",
    "summary_model.round(3).to_csv(OUT_DIR/\"summary_overall_by_model_classic.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Classic DL — Overall (grand mean over domains × variants × seeds) ===\")\n",
    "print(summary_model.round(3).to_string(index=False))\n",
    "\n",
    "print(f\"\\nAll artifacts saved under: {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121a3fa-7739-4dc1-bb77-64d64c3fe9b9",
   "metadata": {},
   "source": [
    "# 7. Linear Baselines — TF-IDF with Logistic Regression\n",
    "\n",
    "This section builds **strong linear baselines** to anchor the classic DL and Transformer results. I stick to a **transparent, reproducible** setup: TF-IDF features (1–2 grams, max 10k) with optional lightweight, hand-engineered cues (negation, exclamation, question marks). Models are trained on a **fixed 70/10/20 split**, repeated over **3 seeds**, with **validation-tuned thresholds** to maximize F1.\n",
    "\n",
    "> **Note on SVM (SVC):** A linear SVM with Platt scaling was initially implemented and explored. However, after early analyses I removed SVC from the reported metrics pipeline and from the final comparisons. **SVC is not part of the dissertation results** (kept only as exploratory code).\n",
    "\n",
    "---\n",
    "\n",
    "## 7.1 What I ran\n",
    "\n",
    "* **Features**\n",
    "\n",
    "  * **TF-IDF**: 1–2 grams, `max_features=10,000`, trained on the **training** partition only.\n",
    "  * **Extra signals (optional)**: `negation_count`, `exclamations`, `question_marks` (scaled and appended to the sparse matrix).\n",
    "\n",
    "* **Model**\n",
    "\n",
    "  * **Logistic Regression (LBFGS, max\\_iter=2000)**.\n",
    "  * Threshold **tuned on validation** to maximize F1 (reporting both default 0.5 and tuned).\n",
    "\n",
    "* **Protocol**\n",
    "\n",
    "  * 70/10/20 stratified **fixed split** shared across seeds.\n",
    "  * **3 seeds** {0,1,2}.\n",
    "  * Saved artifacts: per-seed classification reports, confusion matrices, misclassified samples with cue tags, calibration bins (ECE), and JSON meta.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2 Headline results — Fixed split (mean ± sd over 3 seeds)\n",
    "\n",
    "* **LogReg + TF-IDF**\n",
    "\n",
    "  * **F1**: **0.697 ± 0.000** (default 0.5 threshold)\n",
    "  * **F1 (tuned)**: **0.719 ± 0.000**\n",
    "  * **Acc**: 0.702 ± 0.000\n",
    "  * **Precision/Recall**: 0.721 / 0.675\n",
    "  * **Brier**: 0.194\n",
    "  * **ECE (15 bins)**: 0.199\n",
    "  * **Latency**: **\\~0.028 ms/sample** (very fast)\n",
    "\n",
    "**Takeaway:** A simple **Logistic Regression on TF-IDF** already lands near the classic DL models, and **threshold tuning** gives a clean F1 bump to **\\~0.72**—at a **tiny inference cost**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.3 Per-domain (balanced & equalized) results\n",
    "\n",
    "To isolate domain effects, I **balanced each source 50/50** and **equalized sample sizes** to the smallest source (per seed), then ran **LogReg + TF-IDF** with a 70/20/10 split inside each source.\n",
    "\n",
    "**Mean ± sd across 3 seeds:**\n",
    "\n",
    "* **News headlines:** **F1 = 0.768 ± 0.011**, **Acc = 0.760 ± 0.013**, **Brier = 0.180**\n",
    "* **Twitter:** **F1 = 0.604 ± 0.023**, **Acc = 0.617 ± 0.023**, **Brier = 0.225**\n",
    "* **Reddit:** **F1 = 0.598 ± 0.009**, **Acc = 0.586 ± 0.003**, **Brier = 0.238**\n",
    "\n",
    "**Reading it:**\n",
    "\n",
    "* **News** is consistently the easiest for linear signals (headlines are concise and topical).\n",
    "* **Reddit/Twitter** are tougher (implicit cues, slang, topic drift); still, LR remains competitive given its simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.4 Calibration & errors (quick glance)\n",
    "\n",
    "* **Calibration:** ECE \\~0.20 (fixed split), with **Brier \\~0.19–0.24** across domains—solid for a linear model; temperature scaling wasn’t required here thanks to LR’s native probabilities.\n",
    "* **Typical misses:**\n",
    "\n",
    "  * **Implicit sarcasm** with neutral vocabulary (few surface cues).\n",
    "  * **Hyperbole/exclamations** can trigger false positives; tuned thresholds help re-balance precision/recall.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.5 Why keep LR in the stack\n",
    "\n",
    "* **Speed & robustness:** Sub-millisecond inference and stable outputs make LR a **great production fallback** and a **trustworthy yardstick** for newer models.\n",
    "* **Interpretability:** TF-IDF weights and hand-engineered counts provide **transparent** signals when auditing behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## 7.6 One-liner for Results\n",
    "\n",
    "> **LogReg + TF-IDF** delivers a **strong, fast baseline** (F1≈**0.70**, **0.72** with tuned threshold); on balanced per-domain runs it peaks on **News (F1≈0.77)** and trails on social text (Reddit/Twitter **\\~0.60**). A previously explored **SVC** was **excluded** from final reporting and comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b004b24-3729-4518-bd22-49cba390a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LogReg_TFIDF ================\n",
      "\n",
      "\n",
      "=== LogReg_TFIDF — Aggregate (3 seeds) ===\n",
      "                   acc: 0.7023 ± 0.0000\n",
      "             precision: 0.7212 ± 0.0000\n",
      "                recall: 0.6747 ± 0.0000\n",
      "                    f1: 0.6972 ± 0.0000\n",
      "              f1_tuned: 0.7191 ± 0.0000\n",
      "                 brier: 0.1936 ± 0.0000\n",
      "                   ece: 0.1987 ± 0.0000\n",
      "            latency_ms: 0.0282 ± 0.0002\n",
      "        train_time_sec: 9.4566 ± 0.1575\n",
      "\n",
      "================ LinearSVM_TFIDF ================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Linear Baselines Suite — Same Protocol as DL/Transformers\n",
    "# Models:\n",
    "#   - Logistic Regression (LR)\n",
    "#   - Linear SVM with Platt scaling (SVC, probability=True)\n",
    "# Protocol:\n",
    "#   - TF-IDF (1-2 grams, max_features=10k) + optional engineered features\n",
    "#   - 70/10/20 stratified split (fixed indices for fairness)\n",
    "#   - Multi-seed runs {0,1,2}\n",
    "#   - Threshold tuning on validation (maximize F1)\n",
    "#   - Save: reports, CM, misclassified, calibration bins, per-domain metrics, meta\n",
    "#   - Aggregate mean±std across seeds per model\n",
    "# ===============================================\n",
    "\n",
    "import os, json, time, random, re, string, joblib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, brier_score_loss)\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "CFG = {\n",
    "    \"out_dir\": \"./out_fair/Baselines_LinearSuite\",\n",
    "    \"seeds\": [0, 1, 2],\n",
    "    \"max_features\": 10000,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"use_extra_features\": True,  # set False if you don't have them\n",
    "    \"extra_feature_cols\": [\"negation_count\", \"exclamations\", \"question_marks\"],\n",
    "    \"val_split_random_state\": 42,\n",
    "}\n",
    "\n",
    "OUT_ROOT = Path(CFG[\"out_dir\"])\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Assumptions: df_filtered exists with 'text','label', optional 'source'\n",
    "# -------------------------------\n",
    "assert 'df_filtered' in globals(), \"df_filtered must exist with columns: text,label,(optional)source,(optional) extra features\"\n",
    "assert {'text','label'}.issubset(df_filtered.columns), \"df_filtered must contain 'text' and 'label'\"\n",
    "\n",
    "HAS_SOURCE = 'source' in df_filtered.columns\n",
    "\n",
    "# -------------------------------\n",
    "# Utility: reproducibility\n",
    "# -------------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# -------------------------------\n",
    "# Text patterns for error tagging (like DL/Transformers)\n",
    "# -------------------------------\n",
    "_NEGATION_RE   = re.compile(r\"\\b(no|not|never|n't|cannot|can't|won't|don'?t)\\b\", re.IGNORECASE)\n",
    "_EXCLAM_RE     = re.compile(r\"!{1,}\")\n",
    "_HYPERBOLE_RE  = re.compile(r\"\\b(always|never|literally|absolutely|everyone|no one|best|worst|totally|completely)\\b\", re.IGNORECASE)\n",
    "_OVERCONF_RE   = re.compile(r\"\\b(of course|obviously|clearly|as everyone knows|without a doubt)\\b\", re.IGNORECASE)\n",
    "\n",
    "def pattern_flags(text):\n",
    "    return {\n",
    "        \"negation\": bool(_NEGATION_RE.search(text)),\n",
    "        \"exclamation\": bool(_EXCLAM_RE.search(text)),\n",
    "        \"hyperbole\": bool(_HYPERBOLE_RE.search(text)),\n",
    "        \"overconfidence\": bool(_OVERCONF_RE.search(text)),\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# ECE (binary) & threshold tuning\n",
    "# -------------------------------\n",
    "def ece_score(y_true, p_pos, n_bins=15):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    p_pos  = np.asarray(p_pos).astype(float)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    rows = []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        mask = (p_pos > lo) & (p_pos <= hi) if i > 0 else (p_pos >= lo) & (p_pos <= hi)\n",
    "        if not np.any(mask):\n",
    "            rows.append((float(lo), float(hi), 0, np.nan, np.nan))\n",
    "            continue\n",
    "        conf = p_pos[mask].mean()\n",
    "        acc  = (y_true[mask] == (p_pos[mask] >= 0.5)).mean()\n",
    "        w    = mask.mean()\n",
    "        ece += w * abs(acc - conf)\n",
    "        rows.append((float(lo), float(hi), int(mask.sum()), float(acc), float(conf)))\n",
    "    return float(ece), pd.DataFrame(rows, columns=[\"bin_lo\",\"bin_hi\",\"count\",\"bin_acc\",\"bin_conf\"])\n",
    "\n",
    "def threshold_tune(p_val, y_val):\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in np.linspace(0.05, 0.95, 19):\n",
    "        preds = (p_val >= t).astype(int)\n",
    "        f1 = f1_score(y_val, preds, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "# -------------------------------\n",
    "# Fixed split (70/10/20) shared by both models\n",
    "# -------------------------------\n",
    "texts_all = df_filtered['text'].astype(str).tolist()\n",
    "labels_all = df_filtered['label'].astype(int).to_numpy()\n",
    "sources_all = df_filtered['source'].astype(str).tolist() if HAS_SOURCE else [\"_\"]*len(texts_all)\n",
    "\n",
    "# Select/prepare extra features\n",
    "if CFG[\"use_extra_features\"] and all(col in df_filtered.columns for col in CFG[\"extra_feature_cols\"]):\n",
    "    X_extra_all = df_filtered[CFG[\"extra_feature_cols\"]].to_numpy(dtype=float)\n",
    "    scaler = StandardScaler(with_mean=False)  # sparse-friendly\n",
    "    X_extra_all = scaler.fit_transform(X_extra_all)\n",
    "else:\n",
    "    CFG[\"use_extra_features\"] = False\n",
    "    X_extra_all = None\n",
    "\n",
    "X_tmp_texts, X_test_texts, y_tmp, y_test, s_tmp, s_test, idx_tmp, idx_test = train_test_split(\n",
    "    texts_all, labels_all, sources_all, np.arange(len(labels_all)),\n",
    "    test_size=0.20, stratify=labels_all, random_state=CFG[\"val_split_random_state\"]\n",
    ")\n",
    "X_train_texts, X_val_texts, y_train, y_val, s_train, s_val, idx_train, idx_val = train_test_split(\n",
    "    X_tmp_texts, y_tmp, s_tmp, idx_tmp,\n",
    "    test_size=0.125, stratify=y_tmp, random_state=CFG[\"val_split_random_state\"]\n",
    ")\n",
    "\n",
    "# Split extra features on the same indices\n",
    "if CFG[\"use_extra_features\"]:\n",
    "    X_extra_train = csr_matrix(X_extra_all[idx_train])\n",
    "    X_extra_val   = csr_matrix(X_extra_all[idx_val])\n",
    "    X_extra_test  = csr_matrix(X_extra_all[idx_test])\n",
    "\n",
    "# Save split for reproducibility\n",
    "split_df = pd.DataFrame({\n",
    "    \"split\": ([\"train\"]*len(X_train_texts) + [\"val\"]*len(X_val_texts) + [\"test\"]*len(X_test_texts)),\n",
    "    \"text\":  X_train_texts + X_val_texts + X_test_texts,\n",
    "    \"label\": list(y_train) + list(y_val) + list(y_test),\n",
    "    \"source\": list(s_train) + list(s_val) + list(s_test),\n",
    "})\n",
    "split_df.to_csv(OUT_ROOT/\"data_splits.csv\", index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# TF-IDF fit only on training texts\n",
    "# -------------------------------\n",
    "vectorizer = TfidfVectorizer(max_features=CFG[\"max_features\"], ngram_range=CFG[\"ngram_range\"])\n",
    "X_tfidf_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_tfidf_val   = vectorizer.transform(X_val_texts)\n",
    "X_tfidf_test  = vectorizer.transform(X_test_texts)\n",
    "\n",
    "# Combine text + extra features\n",
    "def combine(X_text_sparse, X_extra_or_none):\n",
    "    if CFG[\"use_extra_features\"]:\n",
    "        return hstack([X_text_sparse, X_extra_or_none], format='csr')\n",
    "    return X_text_sparse\n",
    "\n",
    "X_train = combine(X_tfidf_train, X_extra_train if CFG[\"use_extra_features\"] else None)\n",
    "X_val   = combine(X_tfidf_val,   X_extra_val   if CFG[\"use_extra_features\"] else None)\n",
    "X_test  = combine(X_tfidf_test,  X_extra_test  if CFG[\"use_extra_features\"] else None)\n",
    "\n",
    "# Persist vectorizer & scaler (for deployment parity)\n",
    "joblib.dump(vectorizer, OUT_ROOT/\"tfidf_vectorizer.joblib\")\n",
    "if CFG[\"use_extra_features\"]:\n",
    "    joblib.dump(scaler, OUT_ROOT/\"extra_scaler.joblib\")\n",
    "\n",
    "# -------------------------------\n",
    "# Model builders\n",
    "# -------------------------------\n",
    "def build_logreg():\n",
    "    # liblinear or saga; saga handles l1/elastic, lbfgs good for l2; use lbfgs here\n",
    "    return LogisticRegression(max_iter=2000, solver=\"lbfgs\")\n",
    "\n",
    "def build_svc():\n",
    "    # Linear kernel, probability=True enables Platt scaling for calibrated probs\n",
    "    return SVC(kernel=\"linear\", C=1.0, probability=True)\n",
    "\n",
    "MODEL_SPECS = [\n",
    "    (\"LogReg_TFIDF\", build_logreg),\n",
    "    (\"LinearSVM_TFIDF\", build_svc),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Train & evaluate per model × seed\n",
    "# -------------------------------\n",
    "def mean_std(values):\n",
    "    if len(values) == 1:\n",
    "        return float(values[0]), 0.0\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    return float(arr.mean()), float(arr.std(ddof=1))\n",
    "\n",
    "all_model_aggregates = {}\n",
    "\n",
    "for model_name, builder in MODEL_SPECS:\n",
    "    print(f\"\\n================ {model_name} ================\\n\")\n",
    "    per_seed_rows = []\n",
    "    model_dir = OUT_ROOT / model_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for seed in CFG[\"seeds\"]:\n",
    "        set_all_seeds(seed)\n",
    "        run_dir = model_dir / f\"seed_{seed}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Build & train\n",
    "        model = builder()\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - t0\n",
    "\n",
    "        # Validation probs for threshold tuning\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs_val = model.predict_proba(X_val)[:, 1]\n",
    "            probs_test = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            # fallback using decision function → map to [0,1] via logistic\n",
    "            def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))\n",
    "            probs_val = sigmoid(model.decision_function(X_val))\n",
    "            probs_test = sigmoid(model.decision_function(X_test))\n",
    "\n",
    "        best_t, best_f1_val = threshold_tune(probs_val, y_val)\n",
    "\n",
    "        # Default-threshold preds (0.5) & tuned preds\n",
    "        preds_test_default = (probs_test >= 0.5).astype(int)\n",
    "        preds_test_tuned   = (probs_test >= best_t).astype(int)\n",
    "\n",
    "        # Metrics (default threshold)\n",
    "        acc  = accuracy_score(y_test, preds_test_default)\n",
    "        prec = precision_score(y_test, preds_test_default, zero_division=0)\n",
    "        rec  = recall_score(y_test, preds_test_default, zero_division=0)\n",
    "        f1   = f1_score(y_test, preds_test_default, zero_division=0)\n",
    "\n",
    "        # Calibration\n",
    "        ece, ece_table = ece_score(y_test, probs_test, n_bins=15)\n",
    "        brier = brier_score_loss(y_test, probs_test)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, preds_test_default)\n",
    "\n",
    "        # Misclassifications (+ tags)\n",
    "        texts_test_arr = np.array(X_test_texts, dtype=object)\n",
    "        mis_mask = preds_test_default != y_test\n",
    "        df_miss = pd.DataFrame({\n",
    "            \"text\": texts_test_arr[mis_mask],\n",
    "            \"label_true\": np.asarray(y_test)[mis_mask],\n",
    "            \"prob_pred\": probs_test[mis_mask],\n",
    "            \"pred_label\": preds_test_default[mis_mask]\n",
    "        }).reset_index(drop=True)\n",
    "        # Tagging patterns\n",
    "        for k in [\"negation\",\"exclamation\",\"hyperbole\",\"overconfidence\"]:\n",
    "            df_miss[k] = df_miss[\"text\"].apply(lambda t, kk=k: pattern_flags(t)[kk])\n",
    "\n",
    "        # Domain analysis\n",
    "        domain_metrics = {}\n",
    "        if HAS_SOURCE:\n",
    "            df_pred = pd.DataFrame({\n",
    "                \"source\": np.array(s_test),\n",
    "                \"y\": np.array(y_test),\n",
    "                \"pred\": preds_test_default,\n",
    "                \"p_pos\": probs_test,\n",
    "                \"text\": texts_test_arr\n",
    "            })\n",
    "            for src, g in df_pred.groupby(\"source\"):\n",
    "                m = {\n",
    "                    \"acc\": accuracy_score(g[\"y\"], g[\"pred\"]),\n",
    "                    \"prec\": precision_score(g[\"y\"], g[\"pred\"], zero_division=0),\n",
    "                    \"rec\": recall_score(g[\"y\"], g[\"pred\"], zero_division=0),\n",
    "                    \"f1\": f1_score(g[\"y\"], g[\"pred\"], zero_division=0),\n",
    "                }\n",
    "                e_src, _ = ece_score(g[\"y\"].values, g[\"p_pos\"].values, n_bins=10)\n",
    "                b_src = brier_score_loss(g[\"y\"].values, g[\"p_pos\"].values)\n",
    "                domain_metrics[src] = {**m, \"ece\": float(e_src), \"brier\": float(b_src), \"n\": int(len(g))}\n",
    "\n",
    "        # Latency (ms/sample): predict_proba on a single sample, repeated\n",
    "        sample_vec = X_tfidf_test[0]\n",
    "        if CFG[\"use_extra_features\"]:\n",
    "            sample_vec = hstack([sample_vec, X_extra_test[0]], format='csr')\n",
    "        reps = min(100, max(20, len(y_test)//10))\n",
    "        _ = model.predict_proba(sample_vec) if hasattr(model, \"predict_proba\") else model.decision_function(sample_vec)\n",
    "        t1 = time.time()\n",
    "        for _ in range(reps):\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                _ = model.predict_proba(sample_vec)\n",
    "            else:\n",
    "                _ = model.decision_function(sample_vec)\n",
    "        t2 = time.time()\n",
    "        latency_ms = (t2 - t1) / reps * 1000.0\n",
    "\n",
    "        # Save artifacts\n",
    "        joblib.dump(model, run_dir/\"model.joblib\")\n",
    "        with open(run_dir/\"meta.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"model\": model_name,\n",
    "                \"seed\": seed,\n",
    "                \"train_time_sec\": float(train_time),\n",
    "                \"acc\": float(acc),\n",
    "                \"precision\": float(prec),\n",
    "                \"recall\": float(rec),\n",
    "                \"f1\": float(f1),\n",
    "                \"brier\": float(brier),\n",
    "                \"ece_15bin\": float(ece),\n",
    "                \"best_threshold_val_f1\": float(best_t),\n",
    "                \"f1_at_best_threshold_test\": float(f1_score(y_test, preds_test_tuned, zero_division=0)),\n",
    "                \"latency_ms_per_sample\": float(latency_ms),\n",
    "            }, f, indent=2)\n",
    "\n",
    "        pd.DataFrame(confusion_matrix(y_test, preds_test_default),\n",
    "                     index=[0,1], columns=[0,1]).to_csv(run_dir/\"confusion_matrix.csv\")\n",
    "        ece_table.to_csv(run_dir/\"calibration_bins.csv\", index=False)\n",
    "        df_miss.to_csv(run_dir/\"misclassified.csv\", index=False)\n",
    "\n",
    "        report = classification_report(y_test, preds_test_default, output_dict=True, zero_division=0)\n",
    "        pd.DataFrame(report).T.to_csv(run_dir/\"classification_report.csv\")\n",
    "\n",
    "        # Per-seed row for aggregation\n",
    "        per_seed_rows.append({\n",
    "            \"acc\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1,\n",
    "            \"f1_tuned\": f1_score(y_test, preds_test_tuned, zero_division=0),\n",
    "            \"brier\": brier, \"ece\": ece, \"latency_ms\": latency_ms,\n",
    "            \"train_time_sec\": train_time\n",
    "        })\n",
    "\n",
    "        # Save domain metrics if any\n",
    "        if HAS_SOURCE:\n",
    "            with open(run_dir/\"domain_metrics.json\",\"w\") as f:\n",
    "                json.dump(domain_metrics, f, indent=2)\n",
    "\n",
    "    # Aggregate per model\n",
    "    df_seeds = pd.DataFrame(per_seed_rows)\n",
    "    df_seeds.to_csv(model_dir/\"per_seed_metrics.csv\", index=False)\n",
    "\n",
    "    agg = {}\n",
    "    for col in [\"acc\",\"precision\",\"recall\",\"f1\",\"f1_tuned\",\"brier\",\"ece\",\"latency_ms\",\"train_time_sec\"]:\n",
    "        mu, sd = mean_std(df_seeds[col].tolist())\n",
    "        agg[col] = {\"mean\": mu, \"std\": sd}\n",
    "    with open(model_dir/\"aggregate_mean_std.json\",\"w\") as f:\n",
    "        json.dump(agg, f, indent=2)\n",
    "\n",
    "    print(f\"\\n=== {model_name} — Aggregate (3 seeds) ===\")\n",
    "    for k, vs in agg.items():\n",
    "        prinat(f\"{k:>22}: {vs['mean']:.4f} ± {vs['std']:.4f}\")\n",
    "\n",
    "    all_model_aggregates[model_name] = agg\n",
    "\n",
    "# Save cross-model summary\n",
    "with open(OUT_ROOT/\"ALL_LINEAR_MODELS_aggregate_summary.json\",\"w\") as f:\n",
    "    json.dump(all_model_aggregates, f, indent=2)\n",
    "\n",
    "print(\"\\nAll done. Artifacts under:\", OUT_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e919d80e-d817-42da-9484-66bdeff0b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source counts (raw):\n",
      "source\n",
      "Reddit           919475\n",
      "news_headline     26597\n",
      "twitter            4622\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Seed 0 ===\n",
      "news_headline: balanced → 23294\n",
      "twitter: balanced → 2176\n",
      "Reddit: balanced → 898548\n",
      "news_headline: equalized to 2176\n",
      "twitter: equalized to 2176\n",
      "Reddit: equalized to 2176\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=news_headline | Seed=0 ===\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=twitter | Seed=0 ===\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=Reddit | Seed=0 ===\n",
      "\n",
      "=== Seed 1 ===\n",
      "news_headline: balanced → 23294\n",
      "twitter: balanced → 2176\n",
      "Reddit: balanced → 898548\n",
      "news_headline: equalized to 2176\n",
      "twitter: equalized to 2176\n",
      "Reddit: equalized to 2176\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=news_headline | Seed=1 ===\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=twitter | Seed=1 ===\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=Reddit | Seed=1 ===\n",
      "\n",
      "=== Seed 2 ===\n",
      "news_headline: balanced → 23294\n",
      "twitter: balanced → 2176\n",
      "Reddit: balanced → 898548\n",
      "news_headline: equalized to 2176\n",
      "twitter: equalized to 2176\n",
      "Reddit: equalized to 2176\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=news_headline | Seed=2 ===\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=twitter | Seed=2 ===\n",
      "\n",
      "=== Training LogReg_TFIDF | Source=Reddit | Seed=2 ===\n",
      "\n",
      "=== Logistic Regression — Per-source summary (mean ± std) ===\n",
      "       source  f1_mean  f1_std  acc_mean  acc_std  prec_mean  prec_std  rec_mean  rec_std  brier_mean  brier_std  n_runs\n",
      "       Reddit    0.598   0.009     0.586    0.003      0.581     0.002     0.616    0.021       0.238      0.002       3\n",
      "news_headline    0.768   0.011     0.760    0.013      0.743     0.016     0.795    0.005       0.180      0.003       3\n",
      "      twitter    0.604   0.023     0.617    0.023      0.626     0.028     0.586    0.033       0.225      0.002       3\n",
      "\n",
      "All artifacts saved under: /Users/evelinaivanova/Dissertation/sarcasm_lr_perdomain_runs/run_20250831_063542\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Logistic Regression — Per-Domain Balanced & Equalized (News/Reddit/Twitter)\n",
    "# - For each seed: balance each source 50/50, cap to smallest source\n",
    "# - Train/test split per source\n",
    "# - Logistic Regression with TF-IDF features\n",
    "# - Saves metrics, misclassifications, and summaries\n",
    "# ===========================================\n",
    "import os, json, random, datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, brier_score_loss\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "DATA_CSV = \"df_filtered.csv\"   # expects: text, label, source\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "SOURCE_COL = \"source\"\n",
    "\n",
    "SEEDS = [0, 1, 2]\n",
    "RUN_TAG = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_BASE = Path(\"./sarcasm_lr_perdomain_runs\")\n",
    "OUT_DIR = OUT_BASE / f\"run_{RUN_TAG}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Data\n",
    "# -------------------------------\n",
    "df_full = pd.read_csv(DATA_CSV)\n",
    "assert {TEXT_COL, LABEL_COL, SOURCE_COL}.issubset(df_full.columns)\n",
    "\n",
    "print(\"Source counts (raw):\")\n",
    "print(df_full[SOURCE_COL].value_counts())\n",
    "\n",
    "# -------------------------------\n",
    "# Balancing utils\n",
    "# -------------------------------\n",
    "def balance_source(df_src: pd.DataFrame, seed: int) -> pd.DataFrame:\n",
    "    df0 = df_src[df_src[LABEL_COL] == 0]\n",
    "    df1 = df_src[df_src[LABEL_COL] == 1]\n",
    "    n = min(len(df0), len(df1))\n",
    "    df0s = df0.sample(n=n, random_state=seed)\n",
    "    df1s = df1.sample(n=n, random_state=seed)\n",
    "    return pd.concat([df0s, df1s]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Main loop\n",
    "# -------------------------------\n",
    "rows_all = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n=== Seed {seed} ===\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Balance within each source\n",
    "    balanced = {}\n",
    "    for src in df_full[SOURCE_COL].unique():\n",
    "        df_src = df_full[df_full[SOURCE_COL] == src]\n",
    "        balanced[src] = balance_source(df_src, seed)\n",
    "        print(f\"{src}: balanced → {len(balanced[src])}\")\n",
    "\n",
    "    # Equalise counts across sources (cap to smallest balanced source)\n",
    "    cap = min(len(df) for df in balanced.values())\n",
    "    equalized = {}\n",
    "    for src, df_src in balanced.items():\n",
    "        df0 = df_src[df_src[LABEL_COL] == 0].sample(n=cap//2, random_state=seed)\n",
    "        df1 = df_src[df_src[LABEL_COL] == 1].sample(n=cap//2, random_state=seed)\n",
    "        equalized[src] = pd.concat([df0, df1]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "        print(f\"{src}: equalized to {len(equalized[src])}\")\n",
    "\n",
    "    # Train/test split + TF-IDF + Logistic Regression per source\n",
    "    for src_name, df_src in equalized.items():\n",
    "        print(f\"\\n=== Training LogReg_TFIDF | Source={src_name} | Seed={seed} ===\")\n",
    "\n",
    "        X = df_src[TEXT_COL].astype(str).tolist()\n",
    "        y = df_src[LABEL_COL].astype(int).to_numpy()\n",
    "\n",
    "        X_tr, X_tmp, y_tr, y_tmp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=seed)\n",
    "        X_va, X_te, y_va, y_te = train_test_split(X_tmp, y_tmp, test_size=(2/3), stratify=y_tmp, random_state=seed)\n",
    "\n",
    "        # Save split\n",
    "        src_dir = OUT_DIR / f\"{src_name}\"\n",
    "        src_dir.mkdir(parents=True, exist_ok=True)\n",
    "        split_df = pd.DataFrame({\n",
    "            \"split\": [\"train\"]*len(X_tr) + [\"val\"]*len(X_va) + [\"test\"]*len(X_te),\n",
    "            \"text\": X_tr + X_va + X_te,\n",
    "            \"label\": list(y_tr) + list(y_va) + list(y_te)\n",
    "        })\n",
    "        split_df.to_csv(src_dir/f\"data_split_seed_{seed}.csv\", index=False)\n",
    "\n",
    "        # Vectorize\n",
    "        tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "        Xtr = tfidf.fit_transform(X_tr)\n",
    "        Xva = tfidf.transform(X_va)\n",
    "        Xte = tfidf.transform(X_te)\n",
    "\n",
    "        # Train\n",
    "        clf = LogisticRegression(max_iter=1000)\n",
    "        clf.fit(Xtr, y_tr)\n",
    "\n",
    "        # Evaluate\n",
    "        preds = clf.predict(Xte)\n",
    "        prob_pos = clf.predict_proba(Xte)[:,1]\n",
    "\n",
    "        acc = accuracy_score(y_te, preds)\n",
    "        prec = precision_score(y_te, preds, zero_division=0)\n",
    "        rec = recall_score(y_te, preds, zero_division=0)\n",
    "        f1 = f1_score(y_te, preds, zero_division=0)\n",
    "        brier = brier_score_loss(y_te, prob_pos)\n",
    "\n",
    "        # Misclassified table (FIXED)\n",
    "        test_texts = split_df[split_df[\"split\"] == \"test\"][\"text\"].astype(str).tolist()\n",
    "        y_arr, preds_arr, prob_arr = np.asarray(y_te), np.asarray(preds), np.asarray(prob_pos)\n",
    "        mis_mask = preds_arr != y_arr\n",
    "        df_miss = pd.DataFrame({\n",
    "            \"text\": np.array(test_texts, dtype=object)[mis_mask],\n",
    "            \"label_true\": y_arr[mis_mask],\n",
    "            \"prob_pred\": prob_arr[mis_mask],\n",
    "            \"pred_label\": preds_arr[mis_mask],\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        run_dir = src_dir / f\"seed_{seed}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df_miss.to_csv(run_dir / \"misclassified.csv\", index=False)\n",
    "\n",
    "        # Also save preds for traceability\n",
    "        pd.DataFrame({\n",
    "            \"text\": test_texts,\n",
    "            \"y\": y_arr,\n",
    "            \"pred\": preds_arr,\n",
    "            \"p_pos\": prob_arr,\n",
    "            \"source\": src_name\n",
    "        }).to_csv(run_dir / \"preds_test.csv\", index=False)\n",
    "\n",
    "        # Save meta\n",
    "        meta = {\n",
    "            \"source\": src_name, \"model\": \"LogReg_TFIDF\", \"seed\": seed,\n",
    "            \"acc\": float(acc), \"precision\": float(prec), \"recall\": float(rec),\n",
    "            \"f1\": float(f1), \"brier\": float(brier),\n",
    "            \"n_test\": len(y_te)\n",
    "        }\n",
    "        with open(run_dir/\"meta.json\",\"w\") as f: json.dump(meta, f, indent=2)\n",
    "\n",
    "        rows_all.append(meta)\n",
    "\n",
    "# -------------------------------\n",
    "# Aggregate summaries\n",
    "# -------------------------------\n",
    "df_all = pd.DataFrame(rows_all)\n",
    "df_all.to_csv(OUT_DIR/\"runs_detailed_lr.csv\", index=False)\n",
    "\n",
    "summary_src = df_all.groupby(\"source\").agg(\n",
    "    f1_mean=(\"f1\",\"mean\"), f1_std=(\"f1\",\"std\"),\n",
    "    acc_mean=(\"acc\",\"mean\"), acc_std=(\"acc\",\"std\"),\n",
    "    prec_mean=(\"precision\",\"mean\"), prec_std=(\"precision\",\"std\"),\n",
    "    rec_mean=(\"recall\",\"mean\"), rec_std=(\"recall\",\"std\"),\n",
    "    brier_mean=(\"brier\",\"mean\"), brier_std=(\"brier\",\"std\"),\n",
    "    n_runs=(\"f1\",\"count\")\n",
    ").reset_index()\n",
    "\n",
    "summary_src.to_csv(OUT_DIR/\"summary_by_source.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Logistic Regression — Per-source summary (mean ± std) ===\")\n",
    "print(summary_src.round(3).to_string(index=False))\n",
    "\n",
    "print(f\"\\nAll artifacts saved under: {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685672e2-6a49-4f93-82c6-9ae3e71514c7",
   "metadata": {},
   "source": [
    "# 8. Error analysis: misclassification patterns (classic DL + linear baselines)\n",
    "\n",
    "## 8.1 What I measured\n",
    "\n",
    "Using the saved **misclassified.csv** files, I tagged each mistaken test example with lightweight heuristics (regex-based; *non-exclusive*):\n",
    "\n",
    "* **Negation** (`no/n't/not/never/...`)\n",
    "* **Exclamation** (`!`)\n",
    "* **Hyperbole/intensifiers** (e.g., *always, literally, totally*, elongated spellings like *soooo*, **ALL-CAPS**)\n",
    "* **Overconfidence** = confidently wrong (p≥0.90 for label 1 when true=0, or p≤0.10 when true=1)\n",
    "\n",
    "Counts below are **totals across the 3 seeds** per model (Classic DL “Same Split, 3 Seeds”) and for the **Linear TF-IDF baseline**. Categories can overlap.\n",
    "\n",
    "### Totals & tag rates (across seeds)\n",
    "\n",
    "| Model                | Total Errors | Negation % | Exclamation % | Hyperbole % | Overconfidence % |\n",
    "| -------------------- | -----------: | ---------: | ------------: | ----------: | ---------------: |\n",
    "| BiLSTM\\_Attn\\_100    |      165,432 |       22.0 |           9.1 |        18.7 |              2.9 |\n",
    "| BiLSTM\\_Attn\\_300    |      163,247 |       21.9 |           9.0 |        18.7 |              3.6 |\n",
    "| BiLSTM\\_GloVe100     |      165,358 |       21.9 |           9.1 |        18.8 |              3.3 |\n",
    "| **BiLSTM\\_GloVe300** |  **162,947** |       21.9 |           9.1 |        18.8 |              3.6 |\n",
    "| CNN\\_GloVe100        |      173,581 |       22.1 |           8.9 |        18.7 |          **1.6** |\n",
    "| CNN\\_GloVe300        |      170,553 |       22.0 |           8.9 |        18.6 |              2.1 |\n",
    "| LogReg\\_TFIDF        |      169,830 |       21.8 |           8.0 |        18.7 |              1.9 |\n",
    "\n",
    "*(Bolded = fewest total errors within Classic DL.)*\n",
    "\n",
    "## 8.2 Key takeaways\n",
    "\n",
    "* **Negation drives \\~22% of mistakes across *all* models.**\n",
    "  This is consistent with sarcasm’s polarity flips and implicit scope changes; even simple linear TF-IDF shows the same sensitivity.\n",
    "\n",
    "* **Hyperbole/“intensifier” cues appear in \\~18–19% of errors.**\n",
    "  Exaggeration (lexical intensifiers, stretched spellings, ALL-CAPS) remains a frequent failure mode, suggesting models often miss pragmatic intent when these cues are used non-literally.\n",
    "\n",
    "* **Overconfident wrongs are rare but non-trivial (≈1.6–3.6%).**\n",
    "  CNNs tend to be **less** overconfident when wrong (≈1.6–2.1%) than BiLSTMs (≈2.9–3.6%), aligning with their generally better calibration in our classic suite; LogReg is also cautious (≈1.9%).\n",
    "\n",
    "* **Which classic DL model errs least overall?**\n",
    "  **BiLSTM + GloVe (300d)** records the **fewest total mistakes** across seeds (162,947), while **CNN\\_GloVe100** has the most (173,581). The **LogReg TF-IDF** baseline sits in between (169,830).\n",
    "\n",
    "## 8.3 Implications & fixes\n",
    "\n",
    "* **Handle negation explicitly.**\n",
    "  Add simple **negation-scope features** (e.g., flip polarity inside a detected scope, or inject “NOT\\_” bigrams) for linear models; for DL, append a binary negation flag per token/time-step or fine-tune with **negation-focused augmentation** (paired “with/without not” edits).\n",
    "\n",
    "* **Model hyperbole signals.**\n",
    "  Include **character-level modules** (captures elongated spellings), keep **case information** for ALL-CAPS, and augment with **intensifier rewrites** (e.g., remove/replace *totally/literally/soooo*) to teach invariance.\n",
    "\n",
    "* **Reduce confident mistakes.**\n",
    "  Continue using **temperature scaling** (already in the transformer section) or isotonic calibration for classic/linear models at deployment time; monitor “high-confidence errors” separately.\n",
    "\n",
    "* **Data curation.**\n",
    "  Mine hard negatives from the error buckets above (negation + hyperbole intersections are especially valuable) and re-train with a small **hard-example upweighting**.\n",
    "\n",
    "> Note: these tags are heuristic and overlapping; they’re meant to **locate failure regions**, not to provide mutually exclusive attributions. Full CSVs and JSON roll-ups are saved alongside the runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df2c8d2-c7eb-4a1c-9e6a-51b27fa7ece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification analysis (TOTALS across seeds, per model):\n",
      "  BiLSTM_Attn_100: total_errors=165432 | negation=36428 | exclamation=15129 | hyperbole=31013 | overconfidence=4781 | seeds=3\n",
      "  BiLSTM_Attn_300: total_errors=163247 | negation=35793 | exclamation=14720 | hyperbole=30450 | overconfidence=5881 | seeds=3\n",
      "  BiLSTM_GloVe100: total_errors=165358 | negation=36257 | exclamation=14984 | hyperbole=31042 | overconfidence=5379 | seeds=3\n",
      "  BiLSTM_GloVe300: total_errors=162947 | negation=35729 | exclamation=14784 | hyperbole=30637 | overconfidence=5860 | seeds=3\n",
      "  CNN_GloVe100: total_errors=173581 | negation=38324 | exclamation=15420 | hyperbole=32496 | overconfidence=2708 | seeds=3\n",
      "  CNN_GloVe300: total_errors=170553 | negation=37493 | exclamation=15176 | hyperbole=31756 | overconfidence=3533 | seeds=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_seeds</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>negation</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>hyperbole</th>\n",
       "      <th>overconfidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiLSTM_Attn_100</td>\n",
       "      <td>3</td>\n",
       "      <td>165432</td>\n",
       "      <td>36428</td>\n",
       "      <td>15129</td>\n",
       "      <td>31013</td>\n",
       "      <td>4781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiLSTM_Attn_300</td>\n",
       "      <td>3</td>\n",
       "      <td>163247</td>\n",
       "      <td>35793</td>\n",
       "      <td>14720</td>\n",
       "      <td>30450</td>\n",
       "      <td>5881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BiLSTM_GloVe100</td>\n",
       "      <td>3</td>\n",
       "      <td>165358</td>\n",
       "      <td>36257</td>\n",
       "      <td>14984</td>\n",
       "      <td>31042</td>\n",
       "      <td>5379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM_GloVe300</td>\n",
       "      <td>3</td>\n",
       "      <td>162947</td>\n",
       "      <td>35729</td>\n",
       "      <td>14784</td>\n",
       "      <td>30637</td>\n",
       "      <td>5860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN_GloVe100</td>\n",
       "      <td>3</td>\n",
       "      <td>173581</td>\n",
       "      <td>38324</td>\n",
       "      <td>15420</td>\n",
       "      <td>32496</td>\n",
       "      <td>2708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN_GloVe300</td>\n",
       "      <td>3</td>\n",
       "      <td>170553</td>\n",
       "      <td>37493</td>\n",
       "      <td>15176</td>\n",
       "      <td>31756</td>\n",
       "      <td>3533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model  n_seeds  total_errors  negation  exclamation  hyperbole  \\\n",
       "0  BiLSTM_Attn_100        3        165432     36428        15129      31013   \n",
       "1  BiLSTM_Attn_300        3        163247     35793        14720      30450   \n",
       "2  BiLSTM_GloVe100        3        165358     36257        14984      31042   \n",
       "3  BiLSTM_GloVe300        3        162947     35729        14784      30637   \n",
       "4     CNN_GloVe100        3        173581     38324        15420      32496   \n",
       "5     CNN_GloVe300        3        170553     37493        15176      31756   \n",
       "\n",
       "   overconfidence  \n",
       "0            4781  \n",
       "1            5881  \n",
       "2            5379  \n",
       "3            5860  \n",
       "4            2708  \n",
       "5            3533  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Classic DL Suite: Misclassification totals across seeds (Transformer-style) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, json\n",
    "\n",
    "ROOT_DIR = Path(\"out_fair/Classic_DL_Suite\").expanduser().resolve()\n",
    "\n",
    "# Prefixed tag columns to avoid collisions\n",
    "NEGATIONS      = r\"\\b(?:not|n't|no|never|nothing|nowhere|hardly|barely|without)\\b\"\n",
    "INTENSIFIERS   = r\"\\b(?:always|never|literally|totally|completely|absolutely|insanely|ridiculously|so+|very|extremely|utterly|best|worst|everyone|no one|ever)\\b\"\n",
    "REPEAT_LETTERS = r\"(.)\\1{2,}\"\n",
    "ALL_CAPS_WORD  = r\"\\b[A-Z]{3,}\\b\"\n",
    "EXCLAMATION    = r\"!+\"\n",
    "TAG_COLS = [\"__negation\",\"__exclamation\",\"__hyperbole\",\"__overconfidence\"]\n",
    "\n",
    "def detect_categories(text: str):\n",
    "    t = text or \"\"\n",
    "    return {\n",
    "        \"__negation\": bool(re.search(NEGATIONS, t, flags=re.IGNORECASE)),\n",
    "        \"__exclamation\": bool(re.search(EXCLAMATION, t)),\n",
    "        \"__hyperbole\": (\n",
    "            bool(re.search(INTENSIFIERS, t, flags=re.IGNORECASE)) or\n",
    "            bool(re.search(REPEAT_LETTERS, t)) or\n",
    "            bool(re.search(ALL_CAPS_WORD, t))\n",
    "        ),\n",
    "    }\n",
    "\n",
    "def count_mis(df: pd.DataFrame) -> dict:\n",
    "    # Ensure missing prefixed cols exist\n",
    "    for c in TAG_COLS:\n",
    "        if c not in df:\n",
    "            df[c] = False\n",
    "    return {\n",
    "        \"total_errors\": int(len(df)),\n",
    "        \"negation\": int(df[\"__negation\"].astype(bool).sum()),\n",
    "        \"exclamation\": int(df[\"__exclamation\"].astype(bool).sum()),\n",
    "        \"hyperbole\": int(df[\"__hyperbole\"].astype(bool).sum()),\n",
    "        \"overconfidence\": int(df[\"__overconfidence\"].astype(bool).sum()),\n",
    "    }\n",
    "\n",
    "def load_seed_errors(seed_dir: Path) -> pd.DataFrame:\n",
    "    csv_path = seed_dir / \"misclassified.csv\"\n",
    "    if not csv_path.exists():\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # add heuristic tags\n",
    "    tags = df[\"text\"].astype(str).apply(detect_categories).apply(pd.Series)\n",
    "    df = pd.concat([df, tags], axis=1)\n",
    "    # overconfidence (prefixed) if probs + labels present\n",
    "    if {\"label_true\",\"prob_pred\"}.issubset(df.columns):\n",
    "        df[\"__overconfidence\"] = (\n",
    "            ((df[\"label_true\"] == 0) & (df[\"prob_pred\"] >= 0.90)) |\n",
    "            ((df[\"label_true\"] == 1) & (df[\"prob_pred\"] <= 0.10))\n",
    "        )\n",
    "    else:\n",
    "        df[\"__overconfidence\"] = False\n",
    "    return df\n",
    "\n",
    "rows = []\n",
    "per_model_totals = []\n",
    "\n",
    "for model_dir in sorted([d for d in ROOT_DIR.iterdir() if d.is_dir() and not d.name.startswith(\"_\")]):\n",
    "    # collect all seeds for this model\n",
    "    seed_dfs = []\n",
    "    for sd in sorted([d for d in model_dir.iterdir() if d.is_dir() and d.name.startswith(\"seed_\")]):\n",
    "        df = load_seed_errors(sd)\n",
    "        if not df.empty:\n",
    "            seed_dfs.append(df)\n",
    "\n",
    "    if not seed_dfs:\n",
    "        continue\n",
    "\n",
    "    # concatenate all seed errors → TOTALS across seeds (like transformers script)\n",
    "    df_all = pd.concat(seed_dfs, ignore_index=True)\n",
    "\n",
    "    cnt = count_mis(df_all)\n",
    "    per_model_totals.append({\n",
    "        \"model\": model_dir.name,\n",
    "        \"n_seeds\": len(seed_dfs),\n",
    "        **cnt\n",
    "    })\n",
    "\n",
    "per_model_df = pd.DataFrame(per_model_totals).sort_values(\"model\").reset_index(drop=True)\n",
    "\n",
    "# One-liners (TOTALS across seeds)\n",
    "print(\"Misclassification analysis (TOTALS across seeds, per model):\")\n",
    "for _, r in per_model_df.iterrows():\n",
    "    print(\n",
    "        f\"  {r['model']}: total_errors={int(r['total_errors'])} | \"\n",
    "        f\"negation={int(r['negation'])} | exclamation={int(r['exclamation'])} | \"\n",
    "        f\"hyperbole={int(r['hyperbole'])} | overconfidence={int(r['overconfidence'])} | \"\n",
    "        f\"seeds={int(r['n_seeds'])}\"\n",
    "    )\n",
    "\n",
    "# Save\n",
    "OUT_DIR = ROOT_DIR / \"_analysis_misclassified_totals\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "per_model_df.to_csv(OUT_DIR / \"per_model_seed_totals.csv\", index=False)\n",
    "with open(OUT_DIR / \"per_model_seed_totals.json\", \"w\") as f:\n",
    "    json.dump(per_model_totals, f, indent=2)\n",
    "\n",
    "per_model_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b136394-5826-435f-b5cd-a1bf48ceb113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification analysis (TOTALS across seeds, per model):\n",
      "  LogReg_TFIDF: total_errors=169830 | negation=37083 | exclamation=13560 | hyperbole=31689 | overconfidence=3207 | seeds=3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_seeds</th>\n",
       "      <th>total_errors</th>\n",
       "      <th>negation</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>hyperbole</th>\n",
       "      <th>overconfidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg_TFIDF</td>\n",
       "      <td>3</td>\n",
       "      <td>169830</td>\n",
       "      <td>37083</td>\n",
       "      <td>13560</td>\n",
       "      <td>31689</td>\n",
       "      <td>3207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  n_seeds  total_errors  negation  exclamation  hyperbole  \\\n",
       "0  LogReg_TFIDF        3        169830     37083        13560      31689   \n",
       "\n",
       "   overconfidence  \n",
       "0            3207  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Baselines_LinearSuite: Misclassification totals across seeds (Transformer-style) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re, json\n",
    "\n",
    "ROOT_DIR = Path(\"out_fair/Baselines_LinearSuite\").expanduser().resolve()\n",
    "\n",
    "NEGATIONS      = r\"\\b(?:not|n't|no|never|nothing|nowhere|hardly|barely|without)\\b\"\n",
    "INTENSIFIERS   = r\"\\b(?:always|never|literally|totally|completely|absolutely|insanely|ridiculously|so+|very|extremely|utterly|best|worst|everyone|no one|ever)\\b\"\n",
    "REPEAT_LETTERS = r\"(.)\\1{2,}\"\n",
    "ALL_CAPS_WORD  = r\"\\b[A-Z]{3,}\\b\"\n",
    "EXCLAMATION    = r\"!+\"\n",
    "TAG_COLS = [\"__negation\",\"__exclamation\",\"__hyperbole\",\"__overconfidence\"]\n",
    "\n",
    "def detect_categories(text: str):\n",
    "    t = text or \"\"\n",
    "    return {\n",
    "        \"__negation\": bool(re.search(NEGATIONS, t, flags=re.IGNORECASE)),\n",
    "        \"__exclamation\": bool(re.search(EXCLAMATION, t)),\n",
    "        \"__hyperbole\": (\n",
    "            bool(re.search(INTENSIFIERS, t, flags=re.IGNORECASE)) or\n",
    "            bool(re.search(REPEAT_LETTERS, t)) or\n",
    "            bool(re.search(ALL_CAPS_WORD, t))\n",
    "        ),\n",
    "    }\n",
    "\n",
    "def count_mis(df: pd.DataFrame) -> dict:\n",
    "    for c in TAG_COLS:\n",
    "        if c not in df:\n",
    "            df[c] = False\n",
    "    return {\n",
    "        \"total_errors\": int(len(df)),\n",
    "        \"negation\": int(df[\"__negation\"].astype(bool).sum()),\n",
    "        \"exclamation\": int(df[\"__exclamation\"].astype(bool).sum()),\n",
    "        \"hyperbole\": int(df[\"__hyperbole\"].astype(bool).sum()),\n",
    "        \"overconfidence\": int(df[\"__overconfidence\"].astype(bool).sum()),\n",
    "    }\n",
    "\n",
    "def load_seed_errors(seed_dir: Path) -> pd.DataFrame:\n",
    "    csv_path = seed_dir / \"misclassified.csv\"\n",
    "    if not csv_path.exists():\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    tags = df[\"text\"].astype(str).apply(detect_categories).apply(pd.Series)\n",
    "    df = pd.concat([df, tags], axis=1)\n",
    "    if {\"label_true\",\"prob_pred\"}.issubset(df.columns):\n",
    "        df[\"__overconfidence\"] = (\n",
    "            ((df[\"label_true\"] == 0) & (df[\"prob_pred\"] >= 0.90)) |\n",
    "            ((df[\"label_true\"] == 1) & (df[\"prob_pred\"] <= 0.10))\n",
    "        )\n",
    "    else:\n",
    "        df[\"__overconfidence\"] = False\n",
    "    return df\n",
    "\n",
    "per_model_totals = []\n",
    "for model_dir in sorted([d for d in ROOT_DIR.iterdir() if d.is_dir() and not d.name.startswith(\"_\")]):\n",
    "    seed_dfs = []\n",
    "    for sd in sorted([d for d in model_dir.iterdir() if d.is_dir() and d.name.startswith(\"seed_\")]):\n",
    "        df = load_seed_errors(sd)\n",
    "        if not df.empty:\n",
    "            seed_dfs.append(df)\n",
    "    if not seed_dfs:\n",
    "        continue\n",
    "\n",
    "    df_all = pd.concat(seed_dfs, ignore_index=True)  # TOTAL across seeds\n",
    "    cnt = count_mis(df_all)\n",
    "    per_model_totals.append({\n",
    "        \"model\": model_dir.name,\n",
    "        \"n_seeds\": len(seed_dfs),\n",
    "        **cnt\n",
    "    })\n",
    "\n",
    "per_model_df = pd.DataFrame(per_model_totals).sort_values(\"model\").reset_index(drop=True)\n",
    "\n",
    "print(\"Misclassification analysis (TOTALS across seeds, per model):\")\n",
    "for _, r in per_model_df.iterrows():\n",
    "    print(\n",
    "        f\"  {r['model']}: total_errors={int(r['total_errors'])} | \"\n",
    "        f\"negation={int(r['negation'])} | exclamation={int(r['exclamation'])} | \"\n",
    "        f\"hyperbole={int(r['hyperbole'])} | overconfidence={int(r['overconfidence'])} | \"\n",
    "        f\"seeds={int(r['n_seeds'])}\"\n",
    "    )\n",
    "\n",
    "OUT_DIR = ROOT_DIR / \"_analysis_misclassified_totals\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "per_model_df.to_csv(OUT_DIR / \"per_model_seed_totals.csv\", index=False)\n",
    "with open(OUT_DIR / \"per_model_seed_totals.json\", \"w\") as f:\n",
    "    json.dump(per_model_totals, f, indent=2)\n",
    "\n",
    "per_model_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "628958d3-cc57-4d59-ae34-0906a7fdad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv(\"df_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b6670f-6e43-4b2b-a596-8b0f6ed1a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-seed pairwise comparisons:\n",
      " seed         model_A         model_B     F1_A     F1_B  ΔF1(A-B)  95% CI low  95% CI high  bootstrap p  McNemar n01  McNemar n10  McNemar discordant     McNemar p\n",
      "   13 BiLSTM_GloVe100 BiLSTM_GloVe300 0.698096 0.699846 -0.001750   -0.003641     0.000100       0.0636        12938        13546               26484  1.914425e-04\n",
      "   13 BiLSTM_GloVe100 BiLSTM_Attn_100 0.698096 0.693317  0.004779    0.002958     0.006579       0.0000        12532        12415               24947  4.626890e-01\n",
      "   13 BiLSTM_GloVe100 BiLSTM_Attn_300 0.698096 0.697791  0.000304   -0.001581     0.002181       0.7644        13200        13897               27097  2.354088e-05\n",
      "   13 BiLSTM_GloVe100    CNN_GloVe100 0.698096 0.680502  0.017593    0.015618     0.019624       0.0000        16932        13407               30339  3.233263e-91\n",
      "   13 BiLSTM_GloVe100    CNN_GloVe300 0.698096 0.697477  0.000619   -0.001359     0.002619       0.5502        16879        15004               31883  8.806121e-26\n",
      "   13 BiLSTM_GloVe300 BiLSTM_Attn_100 0.699846 0.693317  0.006529    0.004596     0.008505       0.0000        14346        13621               27967  1.494425e-05\n",
      "   13 BiLSTM_GloVe300 BiLSTM_Attn_300 0.699846 0.697791  0.002054    0.000228     0.003905       0.0262        12751        12840               25591  5.822539e-01\n",
      "   13 BiLSTM_GloVe300    CNN_GloVe100 0.699846 0.680502  0.019344    0.017258     0.021463       0.0000        18986        14853               33839 5.180500e-112\n",
      "   13 BiLSTM_GloVe300    CNN_GloVe300 0.699846 0.697477  0.002369    0.000408     0.004389       0.0190        17325        14842               32167  1.352787e-43\n",
      "   13 BiLSTM_Attn_100 BiLSTM_Attn_300 0.693317 0.697791 -0.004475   -0.006371    -0.002558       0.0000        12736        13550               26286  5.306338e-07\n",
      "   13 BiLSTM_Attn_100    CNN_GloVe100 0.693317 0.680502  0.012814    0.010777     0.014907       0.0000        17462        14054               31516  3.045924e-82\n",
      "   13 BiLSTM_Attn_100    CNN_GloVe300 0.693317 0.697477 -0.004161   -0.006237    -0.002106       0.0000        17271        15513               32784  2.840604e-22\n",
      "   13 BiLSTM_Attn_300    CNN_GloVe100 0.697791 0.680502  0.017289    0.015200     0.019426       0.0000        18930        14708               33638 1.660633e-117\n",
      "   13 BiLSTM_Attn_300    CNN_GloVe300 0.697791 0.697477  0.000314   -0.001665     0.002329       0.7450        17126        14554               31680  2.412557e-47\n",
      "   13    CNN_GloVe100    CNN_GloVe300 0.680502 0.697477 -0.016975   -0.018800    -0.015159       0.0000        12900        14550               27450  2.377508e-23\n",
      "   17 BiLSTM_GloVe100 BiLSTM_GloVe300 0.702925 0.711186 -0.008261   -0.010089    -0.006421       0.0000        14122        14701               28823  6.625066e-04\n",
      "   17 BiLSTM_GloVe100 BiLSTM_Attn_100 0.702925 0.710767 -0.007842   -0.009613    -0.005981       0.0000        14341        14394               28735  7.590272e-01\n",
      "   17 BiLSTM_GloVe100 BiLSTM_Attn_300 0.702925 0.719147 -0.016221   -0.018056    -0.014362       0.0000        15280        15687               30967  2.104478e-02\n",
      "   17 BiLSTM_GloVe100    CNN_GloVe100 0.702925 0.695070  0.007855    0.005857     0.009817       0.0000        17686        15858               33544  1.905272e-23\n",
      "   17 BiLSTM_GloVe100    CNN_GloVe300 0.702925 0.695569  0.007357    0.005331     0.009414       0.0000        17719        16407               34126  1.269291e-12\n",
      "   17 BiLSTM_GloVe300 BiLSTM_Attn_100 0.711186 0.710767  0.000419   -0.001349     0.002215       0.6428        14776        14250               29026  2.058911e-03\n",
      "   17 BiLSTM_GloVe300 BiLSTM_Attn_300 0.711186 0.719147 -0.007960   -0.009641    -0.006255       0.0000        13571        13399               26970  2.977587e-01\n",
      "   17 BiLSTM_GloVe300    CNN_GloVe100 0.711186 0.695070  0.016116    0.014173     0.018132       0.0000        18413        16006               34419  1.719130e-38\n",
      "   17 BiLSTM_GloVe300    CNN_GloVe300 0.711186 0.695569  0.015617    0.013704     0.017603       0.0000        17018        15127               32145  5.381378e-26\n",
      "   17 BiLSTM_Attn_100 BiLSTM_Attn_300 0.710767 0.719147 -0.008380   -0.010072    -0.006667       0.0000        13547        13901               27448  3.311317e-02\n",
      "   17 BiLSTM_Attn_100    CNN_GloVe100 0.710767 0.695070  0.015697    0.013761     0.017659       0.0000        17492        15611               33103  4.858478e-25\n",
      "   17 BiLSTM_Attn_100    CNN_GloVe300 0.710767 0.695569  0.015198    0.013249     0.017208       0.0000        17577        16212               33789  1.159067e-13\n",
      "   17 BiLSTM_Attn_300    CNN_GloVe100 0.719147 0.695070  0.024077    0.022131     0.026053       0.0000        18648        16413               35061  7.789548e-33\n",
      "   17 BiLSTM_Attn_300    CNN_GloVe300 0.719147 0.695569  0.023578    0.021636     0.025485       0.0000        17241        15522               32763  2.232420e-21\n",
      "   17    CNN_GloVe100    CNN_GloVe300 0.695070 0.695569 -0.000499   -0.002300     0.001340       0.5818        13769        14285               28054  2.106244e-03\n",
      "   23 BiLSTM_GloVe100 BiLSTM_GloVe300 0.702858 0.710619 -0.007761   -0.009387    -0.006071       0.0000        11300        12524               23824  2.276851e-15\n",
      "   23 BiLSTM_GloVe100 BiLSTM_Attn_100 0.702858 0.694195  0.008663    0.006929     0.010381       0.0000        11953        11943               23896  9.535727e-01\n",
      "   23 BiLSTM_GloVe100 BiLSTM_Attn_300 0.702858 0.704671 -0.001813   -0.003594     0.000028       0.0540        12564        13571               26135  4.859939e-10\n",
      "   23 BiLSTM_GloVe100    CNN_GloVe100 0.702858 0.692363  0.010494    0.008615     0.012361       0.0000        16933        14063               30996  8.728896e-60\n",
      "   23 BiLSTM_GloVe100    CNN_GloVe300 0.702858 0.688758  0.014100    0.012198     0.016035       0.0000        16643        14635               31278  7.240853e-30\n",
      "   23 BiLSTM_GloVe300 BiLSTM_Attn_100 0.710619 0.694195  0.016424    0.014626     0.018251       0.0000        13968        12734               26702  4.458327e-14\n",
      "   23 BiLSTM_GloVe300 BiLSTM_Attn_300 0.710619 0.704671  0.005948    0.004299     0.007647       0.0000        11612        11395               23007  1.544319e-01\n",
      "   23 BiLSTM_GloVe300    CNN_GloVe100 0.710619 0.692363  0.018255    0.016256     0.020247       0.0000        18929        14835               33764 3.516339e-110\n",
      "   23 BiLSTM_GloVe300    CNN_GloVe300 0.710619 0.688758  0.021861    0.019936     0.023799       0.0000        16942        13710               30652  3.480846e-76\n",
      "   23 BiLSTM_Attn_100 BiLSTM_Attn_300 0.694195 0.704671 -0.010476   -0.012329    -0.008624       0.0000        12755        13772               26527  4.409271e-10\n",
      "   23 BiLSTM_Attn_100    CNN_GloVe100 0.694195 0.692363  0.001831   -0.000158     0.003832       0.0750        18168        15308               33476  4.173979e-55\n",
      "   23 BiLSTM_Attn_100    CNN_GloVe300 0.694195 0.688758  0.005436    0.003376     0.007485       0.0000        17267        15269               32536  1.664647e-28\n",
      "   23 BiLSTM_Attn_300    CNN_GloVe100 0.704671 0.692363  0.012308    0.010282     0.014370       0.0000        19473        15596               35069  2.343936e-95\n",
      "   23 BiLSTM_Attn_300    CNN_GloVe300 0.704671 0.688758  0.015913    0.013924     0.017911       0.0000        17288        14273               31561  1.186005e-64\n",
      "   23    CNN_GloVe100    CNN_GloVe300 0.692363 0.688758  0.003605    0.001845     0.005351       0.0000        12936        13798               26734  1.391844e-07\n",
      "\n",
      "Aggregate across seeds (means; Stouffer-combined p-values):\n",
      "        model_A         model_B  F1_A_mean  F1_B_mean  ΔF1_mean  bootstrap p (Stouffer)  McNemar p (Stouffer)\n",
      "BiLSTM_GloVe300    CNN_GloVe100   0.707217   0.689312  0.017905                     0.0         6.948629e-244\n",
      "BiLSTM_Attn_300    CNN_GloVe100   0.707203   0.689312  0.017891                     0.0         6.939437e-227\n",
      "BiLSTM_GloVe300    CNN_GloVe300   0.707217   0.693935  0.013282                     0.0         3.391192e-135\n",
      "BiLSTM_Attn_300    CNN_GloVe300   0.707203   0.693935  0.013268                     0.0         1.993859e-123\n",
      "BiLSTM_GloVe100    CNN_GloVe100   0.701293   0.689312  0.011981                     0.0         4.940948e-159\n",
      "BiLSTM_Attn_100    CNN_GloVe100   0.699426   0.689312  0.010114                     0.0         5.335615e-150\n",
      "BiLSTM_GloVe300 BiLSTM_Attn_100   0.707217   0.699426  0.007791                     0.0          5.821816e-18\n",
      "BiLSTM_GloVe100    CNN_GloVe300   0.701293   0.693935  0.007358                     0.0          1.052726e-62\n",
      "BiLSTM_Attn_100    CNN_GloVe300   0.699426   0.693935  0.005491                     0.0          1.313600e-59\n",
      "BiLSTM_GloVe100 BiLSTM_Attn_100   0.701293   0.699426  0.001867                     0.0          8.310928e-01\n",
      "BiLSTM_GloVe300 BiLSTM_Attn_300   0.707217   0.707203  0.000014                     0.0          2.688369e-01\n",
      "   CNN_GloVe100    CNN_GloVe300   0.689312   0.693935 -0.004623                     0.0          4.386122e-26\n",
      "BiLSTM_GloVe100 BiLSTM_Attn_300   0.701293   0.707203 -0.005910                     0.0          1.752099e-13\n",
      "BiLSTM_GloVe100 BiLSTM_GloVe300   0.701293   0.707217 -0.005924                     0.0          3.472506e-18\n",
      "BiLSTM_Attn_100 BiLSTM_Attn_300   0.699426   0.707203 -0.007777                     0.0          1.096881e-14\n",
      "\n",
      "Saved: ./out_fair/Classic_DL_Suite/PAIRWISE_significance_per_seed.csv\n",
      "Saved: ./out_fair/Classic_DL_Suite/PAIRWISE_significance_aggregate.csv\n"
     ]
    }
   ],
   "source": [
    "import os, itertools, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# OPTIONAL: McNemar exact p-value (SciPy). If missing, script still runs (p=None).\n",
    "try:\n",
    "    from scipy.stats import binomtest, norm\n",
    "    HAS_SCIPY = True\n",
    "except Exception:\n",
    "    HAS_SCIPY = False\n",
    "\n",
    "# =========================\n",
    "# 1) Where your DL artifacts live\n",
    "# =========================\n",
    "OUT_ROOT = \"./out_fair/Classic_DL_Suite\"  # <- same as CFG[\"out_dir\"] you used\n",
    "SEEDS    = [13, 17, 23]                   # <- same as CFG[\"seeds\"]\n",
    "\n",
    "# Which models to compare (must match folder names under OUT_ROOT)\n",
    "MODELS = [\n",
    "    \"BiLSTM_GloVe100\",\n",
    "    \"BiLSTM_GloVe300\",\n",
    "    \"BiLSTM_Attn_100\",\n",
    "    \"BiLSTM_Attn_300\",\n",
    "    \"CNN_GloVe100\",\n",
    "    \"CNN_GloVe300\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 2) Helpers\n",
    "# =========================\n",
    "def f1_binary(y, p):\n",
    "    y = np.asarray(y).astype(int)\n",
    "    p = np.asarray(p).astype(int)\n",
    "    tp = np.sum((y == 1) & (p == 1))\n",
    "    fp = np.sum((y == 0) & (p == 1))\n",
    "    fn = np.sum((y == 1) & (p == 0))\n",
    "    if tp == 0 and (fp + fn) == 0:\n",
    "        return 1.0\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "\n",
    "def paired_bootstrap_delta_f1(y, pA, pB, BBOOT=10000, rng_seed=13):\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    n = len(y)\n",
    "    diffs = np.empty(BBOOT, dtype=float)\n",
    "\n",
    "    f1_A = f1_binary(y, pA)\n",
    "    f1_B = f1_binary(y, pB)\n",
    "    obs  = f1_A - f1_B\n",
    "\n",
    "    for i in range(BBOOT):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        diffs[i] = f1_binary(y[idx], pA[idx]) - f1_binary(y[idx], pB[idx])\n",
    "\n",
    "    lo, hi = np.percentile(diffs, [2.5, 97.5])\n",
    "    p_boot = 2 * min(np.mean(diffs >= 0.0), np.mean(diffs <= 0.0))\n",
    "    return dict(f1_A=f1_A, f1_B=f1_B, delta=obs, ci_lo=lo, ci_hi=hi, p_boot=float(min(p_boot,1.0)))\n",
    "\n",
    "def mcnemar_exact(y, pA, pB):\n",
    "    Aok = (pA == y); Bok = (pB == y)\n",
    "    n01 = int(np.sum(Aok & ~Bok))   # A correct, B wrong\n",
    "    n10 = int(np.sum(~Aok & Bok))   # A wrong, B correct\n",
    "    disc = n01 + n10\n",
    "    if disc == 0:\n",
    "        return dict(n01=n01, n10=n10, discordant=disc, p=None if not HAS_SCIPY else 1.0)\n",
    "    if not HAS_SCIPY:\n",
    "        return dict(n01=n01, n10=n10, discordant=disc, p=None)\n",
    "    pval = binomtest(k=min(n01, n10), n=disc, p=0.5, alternative=\"two-sided\").pvalue\n",
    "    return dict(n01=n01, n10=n10, discordant=disc, p=float(pval))\n",
    "\n",
    "def stouffer_meta(pvals):\n",
    "    \"\"\"Combine p-values across seeds (two-sided) via Stouffer's Z.\"\"\"\n",
    "    pvals = [p for p in pvals if p is not None]\n",
    "    if not pvals or not HAS_SCIPY:\n",
    "        return None\n",
    "    Zs = [norm.isf(p/2.0) * (1 if p<=0.5 else -1) for p in pvals]  # signed Z by two-sided p\n",
    "    Z  = np.sum(Zs) / math.sqrt(len(Zs))\n",
    "    p_comb = 2 * norm.sf(abs(Z))\n",
    "    return float(p_comb)\n",
    "\n",
    "# =========================\n",
    "# 3) Load predictions for every model × seed, check same y\n",
    "# =========================\n",
    "preds = {}  # (model, seed) -> dict(y, pred)\n",
    "for m in MODELS:\n",
    "    for s in SEEDS:\n",
    "        run = os.path.join(OUT_ROOT, m, f\"seed_{s}\")\n",
    "        y_path = os.path.join(run, \"y_test.npy\")\n",
    "        p_path = os.path.join(run, \"preds_test.npy\")\n",
    "        if not (os.path.exists(y_path) and os.path.exists(p_path)):\n",
    "            raise FileNotFoundError(f\"Missing npy files for {m}, seed {s}: {run}\")\n",
    "        y = np.load(y_path).astype(int)\n",
    "        p = np.load(p_path).astype(int)\n",
    "        preds[(m,s)] = dict(y=y, pred=p)\n",
    "\n",
    "# Sanity: same y across models for each seed\n",
    "for s in SEEDS:\n",
    "    ys = [preds[(m,s)]['y'] for m in MODELS]\n",
    "    for i in range(1, len(ys)):\n",
    "        if not np.array_equal(ys[0], ys[i]):\n",
    "            raise AssertionError(f\"Ground truth mismatch at seed {s} between {MODELS[0]} and {MODELS[i]}.\")\n",
    "\n",
    "# =========================\n",
    "# 4) Per-seed pairwise tests\n",
    "# =========================\n",
    "rows = []\n",
    "for s in SEEDS:\n",
    "    y = preds[(MODELS[0], s)]['y']  # shared\n",
    "    for A, B in itertools.combinations(MODELS, 2):\n",
    "        pA = preds[(A,s)]['pred']; pB = preds[(B,s)]['pred']\n",
    "        boot = paired_bootstrap_delta_f1(y, pA, pB, BBOOT=10000, rng_seed=13)\n",
    "        mc   = mcnemar_exact(y, pA, pB)\n",
    "        rows.append({\n",
    "            \"seed\": s, \"model_A\": A, \"model_B\": B,\n",
    "            \"F1_A\": boot[\"f1_A\"], \"F1_B\": boot[\"f1_B\"],\n",
    "            \"ΔF1(A-B)\": boot[\"delta\"], \"95% CI low\": boot[\"ci_lo\"], \"95% CI high\": boot[\"ci_hi\"],\n",
    "            \"bootstrap p\": boot[\"p_boot\"],\n",
    "            \"McNemar n01\": mc[\"n01\"], \"McNemar n10\": mc[\"n10\"],\n",
    "            \"McNemar discordant\": mc[\"discordant\"], \"McNemar p\": mc[\"p\"]\n",
    "        })\n",
    "\n",
    "per_seed_df = pd.DataFrame(rows)\n",
    "print(\"\\nPer-seed pairwise comparisons:\")\n",
    "print(per_seed_df.to_string(index=False))\n",
    "\n",
    "# =========================\n",
    "# 5) Aggregate across seeds (mean F1; Stouffer p-combo for McNemar/Bootstrap)\n",
    "# =========================\n",
    "agg_rows = []\n",
    "for A, B in itertools.combinations(MODELS, 2):\n",
    "    sub = per_seed_df[(per_seed_df[\"model_A\"]==A) & (per_seed_df[\"model_B\"]==B)].copy()\n",
    "    F1_A_mean = sub[\"F1_A\"].mean(); F1_B_mean = sub[\"F1_B\"].mean()\n",
    "    d_mean    = sub[\"ΔF1(A-B)\"].mean()\n",
    "    # combine p-values across seeds\n",
    "    p_boot_comb = stouffer_meta(sub[\"bootstrap p\"].tolist()) if HAS_SCIPY else None\n",
    "    p_mcn_comb  = stouffer_meta(sub[\"McNemar p\"].tolist())   if HAS_SCIPY else None\n",
    "    agg_rows.append({\n",
    "        \"model_A\": A, \"model_B\": B,\n",
    "        \"F1_A_mean\": F1_A_mean, \"F1_B_mean\": F1_B_mean, \"ΔF1_mean\": d_mean,\n",
    "        \"bootstrap p (Stouffer)\": p_boot_comb, \"McNemar p (Stouffer)\": p_mcn_comb\n",
    "    })\n",
    "agg_df = pd.DataFrame(agg_rows).sort_values(\"ΔF1_mean\", ascending=False)\n",
    "print(\"\\nAggregate across seeds (means; Stouffer-combined p-values):\")\n",
    "print(agg_df.to_string(index=False))\n",
    "\n",
    "# =========================\n",
    "# 6) Save CSVs\n",
    "# =========================\n",
    "per_seed_df.to_csv(os.path.join(OUT_ROOT, \"PAIRWISE_significance_per_seed.csv\"), index=False)\n",
    "agg_df.to_csv(os.path.join(OUT_ROOT, \"PAIRWISE_significance_aggregate.csv\"), index=False)\n",
    "print(f\"\\nSaved: {OUT_ROOT}/PAIRWISE_significance_per_seed.csv\")\n",
    "print(f\"Saved: {OUT_ROOT}/PAIRWISE_significance_aggregate.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4551afb-3f76-4adb-8782-097514d11aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ LogReg_TFIDF ================\n",
      "\n",
      "\n",
      "=== LogReg_TFIDF — Aggregate (3 seeds) ===\n",
      "                         acc: 0.7015 ± 0.0000\n",
      "                   precision: 0.7212 ± 0.0000\n",
      "                      recall: 0.6721 ± 0.0000\n",
      "                          f1: 0.6958 ± 0.0000\n",
      "   f1_at_best_threshold_test: 0.7187 ± 0.0000\n",
      "                       brier: 0.1939 ± 0.0000\n",
      "                   ece_15bin: 0.1975 ± 0.0000\n",
      "       latency_ms_per_sample: 0.0277 ± 0.0001\n",
      "              train_time_sec: 1.2743 ± 0.0463\n",
      "\n",
      "All done. Artifacts under: /Users/evelinaivanova/Dissertation/out_fair/Baselines_LinearSuite\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Linear Baselines Suite — Per-Seed, Transformer-Compatible Artifacts\n",
    "# Models:\n",
    "#   - Logistic Regression (LR)\n",
    "#   - (Optional) Linear SVM with Platt scaling\n",
    "# Protocol:\n",
    "#   - TF-IDF (1–2 grams, max_features=10k) + optional engineered features\n",
    "#   - 70/10/20 stratified split (fixed indices)\n",
    "#   - Seeds {0,1,2}\n",
    "#   - Threshold tuning on validation (F1)\n",
    "#   - Save per-seed: preds_test.csv (text,y,pred,p_sarc,source), reports, CM, misclassified, calibration bins, meta\n",
    "#   - Aggregate mean±std across seeds\n",
    "# ===============================================\n",
    "\n",
    "import os, json, time, random, re, joblib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, brier_score_loss)\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# -------------------------------\n",
    "# EXPECTED INPUT\n",
    "# -------------------------------\n",
    "# You must have df_filtered in memory with at least:\n",
    "#   columns: 'text' (str), 'label' (0/1)\n",
    "# Optional:\n",
    "#   'source' (str) for domain metrics\n",
    "#   engineered features: 'negation_count', 'exclamations', 'question_marks'\n",
    "assert 'df_filtered' in globals(), \"df_filtered must exist with columns: text,label,(optional)source,(optional) extra features\"\n",
    "assert {'text','label'}.issubset(df_filtered.columns), \"df_filtered must contain 'text' and 'label'\"\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "CFG = {\n",
    "    \"out_dir\": \"./out_fair/Baselines_LinearSuite\",\n",
    "    \"seeds\": [0, 1, 2],\n",
    "    \"max_features\": 10000,\n",
    "    \"ngram_range\": (1, 2),\n",
    "    # If your df_filtered lacks extra features, set this True to derive them from text on the fly.\n",
    "    \"AUTO_DERIVE_EXTRA_FEATURES\": True,\n",
    "    # If you already have the three columns in df_filtered, set USE_PROVIDED_EXTRA=True.\n",
    "    \"USE_PROVIDED_EXTRA\": False,\n",
    "    \"extra_feature_cols\": [\"negation_count\", \"exclamations\", \"question_marks\"],\n",
    "    \"split_random_state\": 42,   # fixed split indices\n",
    "    \"run_svm\": False,           # set True to also run Linear SVM (slower due to probability=True)\n",
    "}\n",
    "\n",
    "OUT_ROOT = Path(CFG[\"out_dir\"])\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HAS_SOURCE = 'source' in df_filtered.columns\n",
    "\n",
    "# -------------------------------\n",
    "# Reproducibility\n",
    "# -------------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "# -------------------------------\n",
    "# Patterns & helpers\n",
    "# -------------------------------\n",
    "_NEGATION_RE = re.compile(r\"\\b(no|not|never|n't|cannot|can't|won't|don'?t)\\b\", re.IGNORECASE)\n",
    "_EXCLAM_RE   = re.compile(r\"!+\")\n",
    "_HYPER_RE    = re.compile(r\"\\b(always|never|literally|absolutely|everyone|no one|best|worst|totally|completely)\\b\", re.IGNORECASE)\n",
    "_OVERCONF_RE = re.compile(r\"\\b(of course|obviously|clearly|as everyone knows|without a doubt)\\b\", re.IGNORECASE)\n",
    "\n",
    "def pattern_flags(text: str):\n",
    "    return {\n",
    "        \"negation\": bool(_NEGATION_RE.search(text)),\n",
    "        \"exclamation\": bool(_EXCLAM_RE.search(text)),\n",
    "        \"hyperbole\": bool(_HYPER_RE.search(text)),\n",
    "        \"overconfidence\": bool(_OVERCONF_RE.search(text)),\n",
    "    }\n",
    "\n",
    "def ece_score(y_true, p_pos, n_bins=15):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    p_pos  = np.asarray(p_pos).astype(float)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    rows = []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        mask = (p_pos > lo) & (p_pos <= hi) if i > 0 else (p_pos >= lo) & (p_pos <= hi)\n",
    "        if not np.any(mask):\n",
    "            rows.append((float(lo), float(hi), 0, np.nan, np.nan))\n",
    "            continue\n",
    "        conf = p_pos[mask].mean()\n",
    "        acc  = (y_true[mask] == (p_pos[mask] >= 0.5)).mean()\n",
    "        w    = mask.mean()\n",
    "        ece += w * abs(acc - conf)\n",
    "        rows.append((float(lo), float(hi), int(mask.sum()), float(acc), float(conf)))\n",
    "    return float(ece), pd.DataFrame(rows, columns=[\"bin_lo\",\"bin_hi\",\"count\",\"bin_acc\",\"bin_conf\"])\n",
    "\n",
    "def threshold_tune(p_val, y_val):\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in np.linspace(0.05, 0.95, 19):\n",
    "        f1 = f1_score(y_val, (p_val >= t).astype(int), zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "def mean_std(values):\n",
    "    if len(values) == 1: return float(values[0]), 0.0\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    return float(arr.mean()), float(arr.std(ddof=1))\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare data & (optionally) auto-derive extra features\n",
    "# -------------------------------\n",
    "df = df_filtered.copy()\n",
    "df['text']  = df['text'].astype(str)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "if CFG[\"AUTO_DERIVE_EXTRA_FEATURES\"]:\n",
    "    df['negation_count'] = df['text'].str.count(_NEGATION_RE)\n",
    "    df['exclamations']   = df['text'].str.count(_EXCLAM_RE)\n",
    "    df['question_marks'] = df['text'].str.count(r\"\\?\")\n",
    "    USE_EXTRA = True\n",
    "elif CFG[\"USE_PROVIDED_EXTRA\"] and all(c in df.columns for c in CFG[\"extra_feature_cols\"]):\n",
    "    USE_EXTRA = True\n",
    "else:\n",
    "    USE_EXTRA = False\n",
    "\n",
    "texts_all   = df['text'].tolist()\n",
    "labels_all  = df['label'].to_numpy()\n",
    "sources_all = df['source'].astype(str).tolist() if HAS_SOURCE else [\"_\"]*len(df)\n",
    "\n",
    "# Fixed split\n",
    "X_tmp_texts, X_test_texts, y_tmp, y_test, s_tmp, s_test, idx_tmp, idx_test = train_test_split(\n",
    "    texts_all, labels_all, sources_all, np.arange(len(labels_all)),\n",
    "    test_size=0.20, stratify=labels_all, random_state=CFG[\"split_random_state\"]\n",
    ")\n",
    "X_train_texts, X_val_texts, y_train, y_val, s_train, s_val, idx_train, idx_val = train_test_split(\n",
    "    X_tmp_texts, y_tmp, s_tmp, idx_tmp, test_size=0.125, stratify=y_tmp, random_state=CFG[\"split_random_state\"]\n",
    ")\n",
    "\n",
    "# Prepare extra features if used\n",
    "if USE_EXTRA:\n",
    "    extra = df[CFG[\"extra_feature_cols\"]].to_numpy(dtype=float)\n",
    "    X_extra_train = csr_matrix(extra[idx_train])\n",
    "    X_extra_val   = csr_matrix(extra[idx_val])\n",
    "    X_extra_test  = csr_matrix(extra[idx_test])\n",
    "    scaler = MaxAbsScaler()  # safe for sparse\n",
    "    X_extra_train = scaler.fit_transform(X_extra_train)\n",
    "    X_extra_val   = scaler.transform(X_extra_val)\n",
    "    X_extra_test  = scaler.transform(X_extra_test)\n",
    "\n",
    "# Save split\n",
    "split_df = pd.DataFrame({\n",
    "    \"split\": ([\"train\"]*len(X_train_texts) + [\"val\"]*len(X_val_texts) + [\"test\"]*len(X_test_texts)),\n",
    "    \"text\":  X_train_texts + X_val_texts + X_test_texts,\n",
    "    \"label\": list(y_train) + list(y_val) + list(y_test),\n",
    "    \"source\": list(s_train) + list(s_val) + list(s_test),\n",
    "})\n",
    "split_df.to_csv(OUT_ROOT/\"data_splits.csv\", index=False)\n",
    "\n",
    "# TF-IDF (fit on train only)\n",
    "vectorizer = TfidfVectorizer(max_features=CFG[\"max_features\"], ngram_range=CFG[\"ngram_range\"])\n",
    "X_tfidf_train = vectorizer.fit_transform(X_train_texts)\n",
    "X_tfidf_val   = vectorizer.transform(X_val_texts)\n",
    "X_tfidf_test  = vectorizer.transform(X_test_texts)\n",
    "\n",
    "def combine(X_text_sparse, X_extra_or_none):\n",
    "    if USE_EXTRA:\n",
    "        return hstack([X_text_sparse, X_extra_or_none], format='csr')\n",
    "    return X_text_sparse\n",
    "\n",
    "X_train = combine(X_tfidf_train, X_extra_train if USE_EXTRA else None)\n",
    "X_val   = combine(X_tfidf_val,   X_extra_val   if USE_EXTRA else None)\n",
    "X_test  = combine(X_tfidf_test,  X_extra_test  if USE_EXTRA else None)\n",
    "\n",
    "# Persist vectorizer & scaler\n",
    "joblib.dump(vectorizer, OUT_ROOT/\"tfidf_vectorizer.joblib\")\n",
    "if USE_EXTRA:\n",
    "    joblib.dump(scaler, OUT_ROOT/\"extra_scaler.joblib\")\n",
    "\n",
    "# -------------------------------\n",
    "# Model builders\n",
    "# -------------------------------\n",
    "def build_logreg():\n",
    "    return LogisticRegression(max_iter=2000, solver=\"lbfgs\")\n",
    "\n",
    "def build_svc():\n",
    "    return SVC(kernel=\"linear\", C=1.0, probability=True)\n",
    "\n",
    "MODEL_SPECS = [(\"LogReg_TFIDF\", build_logreg)]\n",
    "if CFG[\"run_svm\"]:\n",
    "    MODEL_SPECS.append((\"LinearSVM_TFIDF\", build_svc))\n",
    "\n",
    "# -------------------------------\n",
    "# Train & evaluate per model × seed\n",
    "# -------------------------------\n",
    "all_model_aggregates = {}\n",
    "\n",
    "for model_name, builder in MODEL_SPECS:\n",
    "    print(f\"\\n================ {model_name} ================\\n\")\n",
    "    per_seed_rows = []\n",
    "    model_dir = OUT_ROOT / model_name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for seed in CFG[\"seeds\"]:\n",
    "        set_all_seeds(seed)\n",
    "        run_dir = model_dir / f\"seed_{seed}\"\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Train\n",
    "        model = builder()\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - t0\n",
    "\n",
    "        # Probs (Platt scaling for SVM; LR has predict_proba)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            p_val  = model.predict_proba(X_val)[:, 1]\n",
    "            p_test = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            # Shouldn't happen with our builders, but keep a fallback\n",
    "            sig = lambda z: 1.0 / (1.0 + np.exp(-z))\n",
    "            p_val  = sig(model.decision_function(X_val))\n",
    "            p_test = sig(model.decision_function(X_test))\n",
    "\n",
    "        # Threshold tuning\n",
    "        best_t, _ = threshold_tune(p_val, y_val)\n",
    "\n",
    "        # Default & tuned preds\n",
    "        pred_test_default = (p_test >= 0.5).astype(int)\n",
    "        pred_test_tuned   = (p_test >= best_t).astype(int)\n",
    "\n",
    "        # Metrics (default)\n",
    "        acc  = accuracy_score(y_test, pred_test_default)\n",
    "        prec = precision_score(y_test, pred_test_default, zero_division=0)\n",
    "        rec  = recall_score(y_test, pred_test_default, zero_division=0)\n",
    "        f1   = f1_score(y_test, pred_test_default, zero_division=0)\n",
    "\n",
    "        # Calibration\n",
    "        ece, ece_table = ece_score(y_test, p_test, n_bins=15)\n",
    "        brier = brier_score_loss(y_test, p_test)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, pred_test_default)\n",
    "\n",
    "        # Misclassifications (+ tags)\n",
    "        texts_test_arr = np.array(X_test_texts, dtype=object)\n",
    "        mis_mask = pred_test_default != y_test\n",
    "        df_miss = pd.DataFrame({\n",
    "            \"text\": texts_test_arr[mis_mask],\n",
    "            \"label_true\": np.asarray(y_test)[mis_mask],\n",
    "            \"prob_pred\": p_test[mis_mask],\n",
    "            \"pred_label\": pred_test_default[mis_mask]\n",
    "        }).reset_index(drop=True)\n",
    "        for k in [\"negation\",\"exclamation\",\"hyperbole\",\"overconfidence\"]:\n",
    "            df_miss[k] = df_miss[\"text\"].apply(lambda t, kk=k: pattern_flags(t)[kk])\n",
    "\n",
    "        # Domain metrics\n",
    "        domain_metrics = {}\n",
    "        if HAS_SOURCE:\n",
    "            df_pred = pd.DataFrame({\n",
    "                \"source\": np.array(s_test),\n",
    "                \"y\": np.array(y_test),\n",
    "                \"pred\": pred_test_default,\n",
    "                \"p_pos\": p_test,\n",
    "                \"text\": texts_test_arr\n",
    "            })\n",
    "            for src, g in df_pred.groupby(\"source\"):\n",
    "                m = {\n",
    "                    \"acc\": accuracy_score(g[\"y\"], g[\"pred\"]),\n",
    "                    \"prec\": precision_score(g[\"y\"], g[\"pred\"], zero_division=0),\n",
    "                    \"rec\": recall_score(g[\"y\"], g[\"pred\"], zero_division=0),\n",
    "                    \"f1\": f1_score(g[\"y\"], g[\"pred\"], zero_division=0),\n",
    "                }\n",
    "                e_src, _ = ece_score(g[\"y\"].values, g[\"p_pos\"].values, n_bins=10)\n",
    "                b_src = brier_score_loss(g[\"y\"].values, g[\"p_pos\"].values)\n",
    "                domain_metrics[src] = {**m, \"ece\": float(e_src), \"brier\": float(b_src), \"n\": int(len(g))}\n",
    "\n",
    "        # Latency (ms/sample): predict on 1 sample, repeat\n",
    "        sample_vec = X_test[0]\n",
    "        reps = min(100, max(20, len(y_test)//10))\n",
    "        _ = model.predict_proba(sample_vec) if hasattr(model, \"predict_proba\") else model.decision_function(sample_vec)\n",
    "        t1 = time.time()\n",
    "        for _ in range(reps):\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                _ = model.predict_proba(sample_vec)\n",
    "            else:\n",
    "                _ = model.decision_function(sample_vec)\n",
    "        t2 = time.time()\n",
    "        latency_ms = (t2 - t1) / reps * 1000.0\n",
    "\n",
    "        # -------- SAVE ARTIFACTS (transformer-compatible) --------\n",
    "        # 1) preds_test.csv — SAME schema as transformer runs\n",
    "        preds_df = pd.DataFrame({\n",
    "            \"text\": X_test_texts,\n",
    "            \"y\":    y_test,\n",
    "            \"pred\": pred_test_default,\n",
    "            \"p_sarc\": p_test,          # probability of sarcasm (class=1)\n",
    "            \"source\": s_test\n",
    "        })\n",
    "        preds_df.to_csv(run_dir/\"preds_test.csv\", index=False)\n",
    "\n",
    "        # 2) other artifacts\n",
    "        joblib.dump(model, run_dir/\"model.joblib\")\n",
    "        pd.DataFrame(cm, index=[0,1], columns=[0,1]).to_csv(run_dir/\"confusion_matrix.csv\")\n",
    "        ece_table.to_csv(run_dir/\"calibration_bins.csv\", index=False)\n",
    "        df_miss.to_csv(run_dir/\"misclassified.csv\", index=False)\n",
    "\n",
    "        report = classification_report(y_test, pred_test_default, output_dict=True, zero_division=0)\n",
    "        pd.DataFrame(report).T.to_csv(run_dir/\"classification_report.csv\")\n",
    "\n",
    "        meta = {\n",
    "            \"model\": model_name,\n",
    "            \"seed\": seed,\n",
    "            \"acc\": float(acc),\n",
    "            \"precision\": float(prec),\n",
    "            \"recall\": float(rec),\n",
    "            \"f1\": float(f1),\n",
    "            \"f1_at_best_threshold_test\": float(f1_score(y_test, pred_test_tuned, zero_division=0)),\n",
    "            \"brier\": float(brier),\n",
    "            \"ece_15bin\": float(ece),\n",
    "            \"best_threshold_val_f1\": float(best_t),\n",
    "            \"latency_ms_per_sample\": float(latency_ms),\n",
    "            \"train_time_sec\": float(train_time),\n",
    "        }\n",
    "        with open(run_dir/\"meta.json\", \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        # domain metrics\n",
    "        if HAS_SOURCE:\n",
    "            with open(run_dir/\"domain_metrics.json\", \"w\") as f:\n",
    "                json.dump(domain_metrics, f, indent=2)\n",
    "\n",
    "        # For aggregation\n",
    "        per_seed_rows.append(meta)\n",
    "\n",
    "    # Aggregate per model\n",
    "    df_seeds = pd.DataFrame(per_seed_rows)\n",
    "    df_seeds.to_csv(model_dir/\"per_seed_metrics.csv\", index=False)\n",
    "\n",
    "    agg = {}\n",
    "    for col in [\"acc\",\"precision\",\"recall\",\"f1\",\"f1_at_best_threshold_test\",\"brier\",\"ece_15bin\",\"latency_ms_per_sample\",\"train_time_sec\"]:\n",
    "        mu, sd = mean_std(df_seeds[col].tolist())\n",
    "        agg[col] = {\"mean\": mu, \"std\": sd}\n",
    "\n",
    "    with open(model_dir/\"aggregate_mean_std.json\", \"w\") as f:\n",
    "        json.dump(agg, f, indent=2)\n",
    "\n",
    "    print(f\"\\n=== {model_name} — Aggregate (3 seeds) ===\")\n",
    "    for k, vs in agg.items():\n",
    "        print(f\"{k:>28}: {vs['mean']:.4f} ± {vs['std']:.4f}\")\n",
    "\n",
    "# Cross-model summary\n",
    "with open(OUT_ROOT/\"ALL_LINEAR_MODELS_aggregate_summary.json\",\"w\") as f:\n",
    "    json.dump({\"models\": [m for m,_ in MODEL_SPECS]}, f, indent=2)\n",
    "\n",
    "print(\"\\nAll done. Artifacts under:\", OUT_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e54d3-72c9-47eb-8686-2a23ae8b4d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
