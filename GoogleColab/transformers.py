# -*- coding: utf-8 -*-
"""Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13t6RN6V33L41xsJoxJz-8R6Ew0cP7XV3
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import AdamW

# ✅ Load dataset
df = pd.read_csv("/content/df_filtered.csv")
print(df.head())

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["text"].tolist(),
    df["label"].tolist(),
    test_size=0.2,
    random_state=42
)

# ✅ Tokenizer
model_name = "bert-base-uncased"  # can change to "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_data(texts, labels):
    tokens = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )
    return TensorDataset(
        tokens["input_ids"], tokens["attention_mask"], torch.tensor(labels, dtype=torch.long)  # ✅ Make labels long here
    )

train_dataset = tokenize_data(train_texts, train_labels)
val_dataset   = tokenize_data(val_texts, val_labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=16)

# ✅ Model
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

epochs = 2  # number of epochs

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()

        # ✅ Pass labels directly → model computes loss automatically
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss  # ✅ Use built-in loss

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

model.eval()
preds, true_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

print("Accuracy:", accuracy_score(true_labels, preds))
print("F1:", f1_score(true_labels, preds))
print("Precision:", precision_score(true_labels, preds))
print("Recall:", recall_score(true_labels, preds))

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

model.eval()
preds, true_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

# Overall metrics
print("Accuracy:", accuracy_score(true_labels, preds))
print("F1 (macro avg):", f1_score(true_labels, preds, average='macro'))
print("Precision (macro avg):", precision_score(true_labels, preds, average='macro'))
print("Recall (macro avg):", recall_score(true_labels, preds, average='macro'))

# Per-class metrics
precision_per_class = precision_score(true_labels, preds, average=None)
recall_per_class = recall_score(true_labels, preds, average=None)
f1_per_class = f1_score(true_labels, preds, average=None)

print("\nPrecision per class:", precision_per_class)
print("Recall per class:", recall_per_class)
print("F1 per class:", f1_per_class)

# Detailed report
print("\nClassification Report:\n")
print(classification_report(true_labels, preds, target_names=["Class 0 (Not Sarcastic)", "Class 1 (Sarcastic)"]))

# ✅ Tokenizer
model_name = "roberta-base"  # can change to "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_data(texts, labels):
    tokens = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )
    return TensorDataset(
        tokens["input_ids"], tokens["attention_mask"], torch.tensor(labels, dtype=torch.long)  # ✅ Make labels long here
    )

train_dataset = tokenize_data(train_texts, train_labels)
val_dataset   = tokenize_data(val_texts, val_labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=16)

# ✅ Model
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

epochs = 2  # number of epochs

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()

        # ✅ Pass labels directly → model computes loss automatically
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss  # ✅ Use built-in loss

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

model.eval()
preds, true_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

print("Accuracy:", accuracy_score(true_labels, preds))
print("F1:", f1_score(true_labels, preds))
print("Precision:", precision_score(true_labels, preds))
print("Recall:", recall_score(true_labels, preds))

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

model.eval()
preds, true_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

# Overall metrics
print("Accuracy:", accuracy_score(true_labels, preds))
print("F1 (macro avg):", f1_score(true_labels, preds, average='macro'))
print("Precision (macro avg):", precision_score(true_labels, preds, average='macro'))
print("Recall (macro avg):", recall_score(true_labels, preds, average='macro'))

# Per-class metrics
precision_per_class = precision_score(true_labels, preds, average=None)
recall_per_class = recall_score(true_labels, preds, average=None)
f1_per_class = f1_score(true_labels, preds, average=None)

print("\nPrecision per class:", precision_per_class)
print("Recall per class:", recall_per_class)
print("F1 per class:", f1_per_class)

# Detailed report
print("\nClassification Report:\n")
print(classification_report(true_labels, preds, target_names=["Class 0 (Not Sarcastic)", "Class 1 (Sarcastic)"]))

import torch
from torch.utils.data import TensorDataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW

# ✅ Use DistilRoBERTa
model_name = "distilroberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_data(texts, labels):
    tokens = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )
    return TensorDataset(
        tokens["input_ids"], tokens["attention_mask"], torch.tensor(labels, dtype=torch.long)
    )

# Assuming you already have train_texts, train_labels, val_texts, val_labels
train_dataset = tokenize_data(train_texts, train_labels)
val_dataset   = tokenize_data(val_texts, val_labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=16)

# ✅ Device setup
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

epochs = 2  # Change as needed

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

model.eval()
preds, true_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

print("Accuracy:", accuracy_score(true_labels, preds))
print("F1:", f1_score(true_labels, preds))
print("Precision:", precision_score(true_labels, preds))
print("Recall:", recall_score(true_labels, preds))

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

model.eval()
preds, true_labels = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
        true_labels.extend(labels.cpu().numpy())

# Overall metrics
print("Accuracy:", accuracy_score(true_labels, preds))
print("F1 (macro avg):", f1_score(true_labels, preds, average='macro'))
print("Precision (macro avg):", precision_score(true_labels, preds, average='macro'))
print("Recall (macro avg):", recall_score(true_labels, preds, average='macro'))

# Per-class metrics
precision_per_class = precision_score(true_labels, preds, average=None)
recall_per_class = recall_score(true_labels, preds, average=None)
f1_per_class = f1_score(true_labels, preds, average=None)

print("\nPrecision per class:", precision_per_class)
print("Recall per class:", recall_per_class)
print("F1 per class:", f1_per_class)

# Detailed report
print("\nClassification Report:\n")
print(classification_report(true_labels, preds, target_names=["Class 0 (Not Sarcastic)", "Class 1 (Sarcastic)"]))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from torch.optim import AdamW

# ✅ Tokenizers for DistilRoBERTa and Emotion Model
base_model_name = "distilroberta-base"
emotion_model_name = "joeddav/distilbert-base-uncased-go-emotions-student"

base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_name)

# ✅ Custom Dataset
def tokenize_data(texts, labels):
    base_tokens = base_tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )
    emotion_tokens = emotion_tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )
    return torch.utils.data.TensorDataset(
        base_tokens["input_ids"], base_tokens["attention_mask"],
        emotion_tokens["input_ids"], emotion_tokens["attention_mask"],
        torch.tensor(labels, dtype=torch.long)
    )

# Assuming you already have train_texts, train_labels, val_texts, val_labels
train_dataset = tokenize_data(train_texts, train_labels)
val_dataset   = tokenize_data(val_texts, val_labels)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=8)

# ✅ Combined Model
class DistilRoBERTaWithEmotion(nn.Module):
    def __init__(self, base_model_name, emotion_model_name, num_labels):
        super().__init__()
        self.base_model = AutoModel.from_pretrained(base_model_name)
        self.emotion_model = AutoModel.from_pretrained(emotion_model_name)
        self.classifier = nn.Linear(
            self.base_model.config.hidden_size + self.emotion_model.config.hidden_size,
            num_labels
        )

    def forward(self, base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask):
        base_outputs = self.base_model(input_ids=base_input_ids, attention_mask=base_attention_mask)
        emo_outputs = self.emotion_model(input_ids=emo_input_ids, attention_mask=emo_attention_mask)

        base_cls = base_outputs.last_hidden_state[:, 0, :]
        emo_cls = emo_outputs.last_hidden_state[:, 0, :]

        combined = torch.cat((base_cls, emo_cls), dim=1)
        logits = self.classifier(combined)
        return logits

# ✅ Setup
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")
model = DistilRoBERTaWithEmotion(base_model_name, emotion_model_name, num_labels=2)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)
epochs = 2

# ✅ Training Loop
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()
        logits = model(base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask)

        loss = nn.CrossEntropyLoss()(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}")

from sklearn.metrics import classification_report, precision_score, recall_score, f1_score
import torch

model.eval()
y_true = []
y_pred = []

with torch.no_grad():
    for batch in val_loader:
        base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask, labels = [b.to(device) for b in batch]

        logits = model(base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask)
        preds = torch.argmax(logits, dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

# ✅ Get F1 Score Per Class
print("Classification Report:")
print(classification_report(y_true, y_pred, digits=3))

# If you just want F1 separately for 0 and 1:
f1_0 = f1_score(y_true, y_pred, pos_label=0)
f1_1 = f1_score(y_true, y_pred, pos_label=1)

print(f"F1 Score for class 0 (not sarcastic): {f1_0:.3f}")
print(f"F1 Score for class 1 (sarcastic): {f1_1:.3f}")

# ===============================================
# Sarcasm Detection: Evaluate BERT Family Models
# Includes:
# 1. BERT
# 2. RoBERTa
# 3. DistilRoBERTa
# 4. DistilRoBERTa + Emotion Embeddings
# ===============================================

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, TensorDataset
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import time
from torch.optim import AdamW

# =========================
# Config
# =========================
BATCH_SIZE = 16
EPOCHS = 2
LR = 2e-5
MAX_LEN = 128

# =========================
# Prepare data
# =========================
df = pd.read_csv("/content/df_filtered.csv")
texts = df["text"].astype(str).tolist()
labels = df["label"].tolist()

# Lock consistent train/val split
X_train, val_texts, y_train, val_labels = train_test_split(
    texts, labels, test_size=0.2, stratify=labels, random_state=42
)

# =========================
# Unified Evaluation Function
# =========================
def evaluate_transformer(model, dataloader, device, X_text, y_true_all, model_name="Transformer", split_name="Validation"):
    model.eval()
    y_true, y_pred = [], []
    start_time = time.time()

    with torch.no_grad():
        for batch in dataloader:
            if len(batch) == 3:
                input_ids, attention_mask, labels = [b.to(device) for b in batch]
                outputs = model(input_ids, attention_mask=attention_mask)
                logits = outputs.logits
            elif len(batch) == 5:
                input_ids, attention_mask, emo_input_ids, emo_attention_mask, labels = [b.to(device) for b in batch]
                logits = model(input_ids, attention_mask, emo_input_ids, emo_attention_mask)
            else:
                raise ValueError("Unexpected batch size")

            preds = torch.argmax(logits, dim=1)
            y_pred.extend(preds.cpu().numpy())
            y_true.extend(labels.cpu().numpy())

    infer_time = time.time() - start_time

    assert len(y_true) == len(X_text), "Mismatch in sample count!"

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print(f"\n=== {split_name} Set - {model_name} ===")
    print(f"Inference Time: {infer_time:.4f}s for {len(y_true)} samples")
    print(f"  Accuracy : {acc:.3f}")
    print(f"  Precision: {prec:.3f}")
    print(f"  Recall   : {rec:.3f}")
    print(f"  F1 Score : {f1:.3f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=["Class 0 (Not Sarcastic)", "Class 1 (Sarcastic)"], digits=3))

    precision_per_class = precision_score(y_true, y_pred, average=None)
    recall_per_class = recall_score(y_true, y_pred, average=None)
    f1_per_class = f1_score(y_true, y_pred, average=None)

    print("\nPrecision per class:", precision_per_class)
    print("Recall per class:", recall_per_class)
    print("F1 per class:", f1_per_class)

    misclassified_idx = np.where(np.array(y_true) != np.array(y_pred))[0]
    print(f"\nMisclassified: {len(misclassified_idx)} / {len(y_true)}")

    misclassified_df = pd.DataFrame({
        "text": np.array(X_text)[misclassified_idx],
        "true_label": np.array(y_true_all)[misclassified_idx],
        "pred_label": np.array(y_pred)[misclassified_idx]
    }).reset_index(drop=True)

    print(f"\nFirst 5 Misclassified Examples ({split_name} - {model_name}):")
    for i in range(min(5, len(misclassified_df))):
        row = misclassified_df.iloc[i]
        print(f"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}")

# =========================
# Dataset creation
# =========================
def make_dataloader(model_name, texts, labels):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokens = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors="pt")
    dataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'], torch.tensor(labels, dtype=torch.long))
    return DataLoader(dataset, batch_size=BATCH_SIZE), tokenizer

# =========================
# DistilRoBERTa + Emotion Model
# =========================
class DistilRoBERTaWithEmotion(nn.Module):
    def __init__(self, base_model_name, emotion_model_name, num_labels):
        super().__init__()
        self.base_model = AutoModel.from_pretrained(base_model_name)
        self.emotion_model = AutoModel.from_pretrained(emotion_model_name)
        self.classifier = nn.Linear(self.base_model.config.hidden_size + self.emotion_model.config.hidden_size, num_labels)

    def forward(self, base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask):
        base_out = self.base_model(base_input_ids, attention_mask=base_attention_mask).last_hidden_state[:, 0, :]
        emo_out = self.emotion_model(emo_input_ids, attention_mask=emo_attention_mask).last_hidden_state[:, 0, :]
        combined = torch.cat((base_out, emo_out), dim=1)
        return self.classifier(combined)

# =========================
# Run All Models
# =========================
models = {
    "BERT": "bert-base-uncased",
    "RoBERTa": "roberta-base",
    "DistilRoBERTa": "distilroberta-base",
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for name, model_name in models.items():
    print(f"\n\n====== Training {name} ======")
    train_loader, tokenizer = make_dataloader(model_name, X_train, y_train)
    val_loader, _ = make_dataloader(model_name, val_texts, val_labels)

    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
    model.to(device)
    optimizer = AdamW(model.parameters(), lr=LR)

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for batch in train_loader:
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}")

    evaluate_transformer(model, val_loader, device, val_texts, val_labels, model_name=name)

# =========================
# DistilRoBERTa + Emotion
# =========================
print("\n\n====== Training DistilRoBERTa + Emotion ======")
base_model = "distilroberta-base"
emo_model = "joeddav/distilbert-base-uncased-go-emotions-student"

base_tokenizer = AutoTokenizer.from_pretrained(base_model)
emo_tokenizer = AutoTokenizer.from_pretrained(emo_model)

def tokenize_both_models(texts, labels):
    base = base_tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors="pt")
    emo = emo_tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors="pt")
    return DataLoader(TensorDataset(
        base['input_ids'], base['attention_mask'],
        emo['input_ids'], emo['attention_mask'],
        torch.tensor(labels, dtype=torch.long)
    ), batch_size=BATCH_SIZE)

train_loader = tokenize_both_models(X_train, y_train)
val_loader = tokenize_both_models(val_texts, val_labels)

model = DistilRoBERTaWithEmotion(base_model, emo_model, num_labels=2)
model.to(device)
optimizer = AdamW(model.parameters(), lr=LR)

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in train_loader:
        base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask, labels = [b.to(device) for b in batch]
        optimizer.zero_grad()
        logits = model(base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask)
        loss = nn.CrossEntropyLoss()(logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}")

evaluate_transformer(model, val_loader, device, val_texts, val_labels, model_name="DistilRoBERTa + Emotion")

"""Starting a new

Loading the data
"""

def evaluate_transformer(model, dataloader, device, X_text, y_true_all, model_name="Transformer", split_name="Validation"):
    model.eval()
    y_true, y_pred = [], []
    start_time = time.time()

    with torch.no_grad():
        for batch in dataloader:
            if len(batch) == 3:
                input_ids, attention_mask, labels = [b.to(device) for b in batch]
                outputs = model(input_ids, attention_mask=attention_mask)
                logits = outputs.logits
            elif len(batch) == 5:
                input_ids, attention_mask, emo_input_ids, emo_attention_mask, labels = [b.to(device) for b in batch]
                logits = model(input_ids, attention_mask, emo_input_ids, emo_attention_mask)
            else:
                raise ValueError("Unexpected batch size")

            preds = torch.argmax(logits, dim=1)
            y_pred.extend(preds.cpu().numpy())
            y_true.extend(labels.cpu().numpy())

    infer_time = time.time() - start_time

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    precision_per_class = precision_score(y_true, y_pred, average=None)
    recall_per_class = recall_score(y_true, y_pred, average=None)
    f1_per_class = f1_score(y_true, y_pred, average=None)

    misclassified_idx = np.where(np.array(y_true) != np.array(y_pred))[0]
    misclassified_df = pd.DataFrame({
        "text": np.array(X_text)[misclassified_idx],
        "true_label": np.array(y_true_all)[misclassified_idx],
        "pred_label": np.array(y_pred)[misclassified_idx]
    }).reset_index(drop=True)

    # Print section
    print(f"\n=== {split_name} Set - {model_name} ===")
    print(f"Inference Time: {infer_time:.4f}s for {len(y_true)} samples")
    print(f"  Accuracy : {acc:.3f}")
    print(f"  Precision: {prec:.3f}")
    print(f"  Recall   : {rec:.3f}")
    print(f"  F1 Score : {f1:.3f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=["Class 0 (Not Sarcastic)", "Class 1 (Sarcastic)"], digits=3))
    print("\nPrecision per class:", precision_per_class)
    print("Recall per class:", recall_per_class)
    print("F1 per class:", f1_per_class)
    print(f"\nMisclassified: {len(misclassified_idx)} / {len(y_true)}")

    print(f"\nFirst 5 Misclassified Examples ({split_name} - {model_name}):")
    for i in range(min(5, len(misclassified_df))):
        row = misclassified_df.iloc[i]
        print(f"[{i+1}] True: {row['true_label']} | Pred: {row['pred_label']} | Text: {row['text']}")

    return {
        "model": model_name,
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "f1": f1,
        "precision_per_class": precision_per_class.tolist(),
        "recall_per_class": recall_per_class.tolist(),
        "f1_per_class": f1_per_class.tolist(),
        "inference_time": infer_time,
        "n_samples": len(y_true),
        "n_misclassified": len(misclassified_idx),
        "misclassified_df": misclassified_df
    }

electra_results = evaluate_transformer(
    electra_model, electra_val_loader, device,
    val_texts, val_labels,
    model_name="ELECTRA-small"
)

from transformers import AutoModelForSequenceClassification, AutoTokenizer
from torch.optim import AdamW
from torch.utils.data import DataLoader, TensorDataset
import torch

# 1. Reload the DistilRoBERTa tokenizer and data
model_name = "distilroberta-base"
train_loader, tokenizer = make_dataloader(model_name, X_train, y_train)
val_loader, _ = make_dataloader(model_name, val_texts, val_labels)

# 2. Define and train model
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)
optimizer = AdamW(model.parameters(), lr=LR)

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1} - Training Loss: {total_loss/len(train_loader):.4f}")

# 3. Evaluate (optional)
evaluate_transformer(model, val_loader, device, val_texts, val_labels, model_name="DistilRoBERTa")

# 4. Save trained model and tokenizer
model.save_pretrained("saved_distilroberta")
tokenizer.save_pretrained("saved_distilroberta")

# 5. Zip and download
!zip -r distilroberta_model.zip saved_distilroberta

from google.colab import files
files.download("distilroberta_model.zip")





# ===============================
# DistilRoBERTa + GoEmotions (Dual Encoder) — ONLY
# - Uses early stopping (patience=1) by default
# - Grad checkpointing + AMP to avoid OOM
# - Saves best checkpoint to disk (no huge RAM copy)
# - Writes preds/logits/meta/reports under ./out_fair/
# ===============================

import os, time, json, random, gc
from pathlib import Path

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, classification_report)

from transformers import (
    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup, set_seed as hf_set_seed
)

# -------------------------------
# Config
# -------------------------------
DATA_CSV       = "/content/df_filtered.csv"  # update if needed
TEXT_COL       = "text"
LABEL_COL      = "label"

# Dual-encoder specific, tuned to avoid OOM
MAX_LEN_DUAL   = 128     # if needed, try 96
BATCH_TRAIN_DUAL = 8     # if needed, try 4
BATCH_EVAL_DUAL  = 16

LEARNING_RATE  = 2e-5
MAX_EPOCHS     = 4
PATIENCE       = 1
SEEDS          = [0, 1, 2]
BASE_SEED      = 42
WARMUP_RATIO   = 0.10
PROTOCOL       = "early_stopping"   # or "fixed_epochs"

OUT_DIR        = Path("./out_fair")

# Dual-encoder model names
EMO_BASE_MODEL    = "distilroberta-base"
EMO_EMOTION_MODEL = "joeddav/distilbert-base-uncased-go-emotions-student"
EMO_NAME          = "DistilRoBERTa+Emotion"

device = torch.device(
    "cuda" if torch.cuda.is_available()
    else "mps" if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available()
    else "cpu"
)
AMP_ENABLED = torch.cuda.is_available()  # AMP only on CUDA

# -------------------------------
# Utils
# -------------------------------
def set_all_seeds(seed: int):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    hf_set_seed(seed)

def sanitize(name: str) -> str:
    return name.replace("/", "_").replace(" ", "_")

def metrics(y, p):
    return {
        "acc": accuracy_score(y,p),
        "prec": precision_score(y,p, zero_division=0),
        "rec":  recall_score(y,p, zero_division=0),
        "f1":   f1_score(y,p, zero_division=0),
        "f1_macro": f1_score(y,p, average="macro", zero_division=0),
    }

def measure_latency_ms(model, tokenizer, sample_text, reps=50):
    sample = tokenizer([sample_text], truncation=True, padding=True, max_length=MAX_LEN_DUAL, return_tensors="pt")
    sample = {k:v.to(device) for k,v in sample.items()}
    model.eval()
    with torch.no_grad(): _ = model(**sample)  # warmup
    t0 = time.time()
    with torch.no_grad():
        for _ in range(reps):
            _ = model(**sample)
    t1 = time.time()
    return (t1 - t0) / reps * 1000.0

def cleanup(*objs):
    for o in objs:
        try: del o
        except: pass
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# -------------------------------
# Data (70/10/20, stratified)
# -------------------------------
df = pd.read_csv(DATA_CSV)
assert {TEXT_COL, LABEL_COL}.issubset(df.columns), f"CSV must contain {TEXT_COL}, {LABEL_COL}"

X_all = df[TEXT_COL].astype(str).tolist()
y_all = df[LABEL_COL].astype(int).tolist()

X_train, X_temp, y_train, y_temp = train_test_split(
    X_all, y_all, test_size=0.30, stratify=y_all, random_state=BASE_SEED
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(2/3), stratify=y_temp, random_state=BASE_SEED
)

OUT_DIR.mkdir(parents=True, exist_ok=True)

# -------------------------------
# Dual-encoder definition + loaders
# -------------------------------
class DualEncoderEmotion(nn.Module):
    def __init__(self, base_name, emo_name, num_labels=2):
        super().__init__()
        self.base = AutoModel.from_pretrained(base_name)
        self.emo  = AutoModel.from_pretrained(emo_name)

        # Memory saver
        if hasattr(self.base, "gradient_checkpointing_enable"):
            self.base.gradient_checkpointing_enable()
        if hasattr(self.emo, "gradient_checkpointing_enable"):
            self.emo.gradient_checkpointing_enable()

        h = self.base.config.hidden_size + self.emo.config.hidden_size
        self.cls = nn.Linear(h, num_labels)

    def forward(self, base_input_ids, base_attention_mask, emo_input_ids, emo_attention_mask):
        b = self.base(input_ids=base_input_ids, attention_mask=base_attention_mask).last_hidden_state[:,0,:]
        e = self.emo (input_ids=emo_input_ids,  attention_mask=emo_attention_mask ).last_hidden_state[:,0,:]
        z = torch.cat([b,e], dim=1)
        return self.cls(z)

def make_dual_loader(base_tok, emo_tok, texts, labels, batch_size=32, shuffle=False, max_len=128):
    b = base_tok(list(texts), truncation=True, padding=True, max_length=max_len, return_tensors="pt")
    e = emo_tok (list(texts), truncation=True, padding=True, max_length=max_len, return_tensors="pt")
    ds = TensorDataset(b["input_ids"], b["attention_mask"], e["input_ids"], e["attention_mask"],
                       torch.tensor(list(labels), dtype=torch.long))
    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)

@torch.no_grad()
def eval_dual(model, loader):
    model.eval()
    ys, preds, probs, logits_all = [], [], [], []
    for bi, ba, ei, ea, labels in loader:
        bi, ba, ei, ea = bi.to(device), ba.to(device), ei.to(device), ea.to(device)
        logits = model(bi, ba, ei, ea)
        logits_all.append(logits.cpu())
        pr = torch.softmax(logits, dim=1).cpu().numpy()
        probs.extend(pr.tolist()); preds.extend(pr.argmax(1)); ys.extend(labels.numpy())
    logits_all = torch.cat(logits_all, dim=0).numpy()
    return np.array(ys), np.array(preds), np.array(probs), np.array(logits_all)

# -------------------------------
# Train dual-encoder (one seed)
# -------------------------------
def train_dual_encoder(seed: int):
    set_all_seeds(seed)

    base_tok = AutoTokenizer.from_pretrained(EMO_BASE_MODEL)
    emo_tok  = AutoTokenizer.from_pretrained(EMO_EMOTION_MODEL)
    model    = DualEncoderEmotion(EMO_BASE_MODEL, EMO_EMOTION_MODEL, num_labels=2).to(device)

    train_loader = make_dual_loader(base_tok, emo_tok, X_train, y_train,
                                    batch_size=BATCH_TRAIN_DUAL, shuffle=True, max_len=MAX_LEN_DUAL)
    val_loader   = make_dual_loader(base_tok, emo_tok, X_val,   y_val,
                                    batch_size=BATCH_EVAL_DUAL,  shuffle=False, max_len=MAX_LEN_DUAL)
    test_loader  = make_dual_loader(base_tok, emo_tok, X_test,  y_test,
                                    batch_size=BATCH_EVAL_DUAL,  shuffle=False, max_len=MAX_LEN_DUAL)

    optim = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    total_steps = MAX_EPOCHS * len(train_loader)
    warmup_steps = int(WARMUP_RATIO * total_steps)
    scheduler = get_linear_schedule_with_warmup(optim, warmup_steps, total_steps)

    scaler = torch.cuda.amp.GradScaler(enabled=AMP_ENABLED)

    best_val_f1 = -1.0
    epochs_no_improve = 0
    epochs_run = 0
    best_ckpt_path = None

    save_root = OUT_DIR / sanitize(EMO_NAME) / f"{PROTOCOL}_seed_{seed}"
    save_root.mkdir(parents=True, exist_ok=True)

    for ep in range(MAX_EPOCHS):
        epochs_run += 1
        model.train(); tot_loss = 0.0

        for bi, ba, ei, ea, labels in train_loader:
            bi, ba, ei, ea, labels = bi.to(device), ba.to(device), ei.to(device), ea.to(device), labels.to(device)
            optim.zero_grad(set_to_none=True)

            if AMP_ENABLED:
                with torch.cuda.amp.autocast():
                    logits = model(bi, ba, ei, ea)
                    loss = nn.CrossEntropyLoss()(logits, labels)
                scaler.scale(loss).backward()
                scaler.step(optim); scaler.update()
            else:
                logits = model(bi, ba, ei, ea)
                loss = nn.CrossEntropyLoss()(logits, labels)
                loss.backward(); optim.step()

            scheduler.step()
            tot_loss += loss.item()

        # Validation
        yv, pv, qv, lv = eval_dual(model, val_loader)
        val_f1 = f1_score(yv, pv, zero_division=0)
        print(f"[{EMO_NAME}][seed {seed}] epoch {ep+1}/{MAX_EPOCHS} loss={tot_loss/len(train_loader):.4f} val_f1={val_f1:.4f}")

        if PROTOCOL == "early_stopping":
            if val_f1 > best_val_f1 + 1e-6:
                best_val_f1 = val_f1
                best_ckpt_path = save_root / "best_state.pt"
                torch.save(model.state_dict(), best_ckpt_path)
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve > PATIENCE:
                    print(f"Early stopping at epoch {ep+1} (best val F1={best_val_f1:.4f})")
                    break

    # Load best checkpoint if available
    if PROTOCOL == "early_stopping" and best_ckpt_path is not None and best_ckpt_path.exists():
        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))

    # Final eval
    yv, pv, qv, lv = eval_dual(model, val_loader)
    yt, pt, qt, lt = eval_dual(model, test_loader)

    # Latency proxy (single DistilRoBERTa classifier)
    latency_ms = measure_latency_ms(
        AutoModelForSequenceClassification.from_pretrained(EMO_BASE_MODEL, num_labels=2).to(device),
        AutoTokenizer.from_pretrained(EMO_BASE_MODEL),
        X_test[0] if len(X_test) else "ok", reps=50
    )

    # Save artifacts
    torch.save(model.state_dict(), save_root/"pytorch_model.bin")
    base_tok.save_pretrained(save_root/"base_tok")
    emo_tok.save_pretrained(save_root/"emo_tok")

    size_mb = 0.0
    for p in save_root.rglob("*"):
        if p.is_file(): size_mb += p.stat().st_size
    size_mb /= (1024*1024)

    pd.DataFrame({"y": yv, "pred": pv, "p_sarc": qv[:,1]}).to_csv(save_root/"preds_val.csv", index=False)
    pd.DataFrame({"y": yt, "pred": pt, "p_sarc": qt[:,1]}).to_csv(save_root/"preds_test.csv", index=False)
    np.savez(save_root/"logits_val.npz", logits=lv, y=yv)
    np.savez(save_root/"logits_test.npz", logits=lt, y=yt)

    with open(save_root/"meta.json","w") as f:
        json.dump({
            "model_pretrained": {"base": EMO_BASE_MODEL, "emotion": EMO_EMOTION_MODEL},
            "pretty_name": EMO_NAME,
            "protocol": PROTOCOL,
            "seed": seed,
            "max_epochs": MAX_EPOCHS, "patience": PATIENCE,
            "lr": LEARNING_RATE,
            "batch_train_dual": BATCH_TRAIN_DUAL, "batch_eval_dual": BATCH_EVAL_DUAL,
            "max_len_dual": MAX_LEN_DUAL, "warmup_ratio": WARMUP_RATIO,
            "epochs_run": epochs_run,
            "best_val_f1": float(f1_score(yv, pv, zero_division=0)),
            "latency_ms_per_sample_proxy": float(latency_ms),
            "size_mb": float(size_mb)
        }, f, indent=2)

    with open(save_root/"report_test.txt","w") as f:
        f.write(classification_report(yt, pt, target_names=["Not Sarcastic","Sarcastic"], digits=3))

    m_val = metrics(yv, pv)
    m_tst = metrics(yt, pt)

    # Cleanup RAM/VRAM
    cleanup(model, base_tok, emo_tok, train_loader, val_loader, test_loader)

    return {
        "model": EMO_NAME,
        "seed": seed,
        "protocol": PROTOCOL,
        "val": {"y": yv, "pred": pv, "probs": qv, "logits": lv},
        "test":{"y": yt, "pred": pt, "probs": qt, "logits": lt},
        "latency_ms": latency_ms,
        "size_mb": size_mb,
        "train_dir": str(save_root),
        "metrics_val": m_val,
        "metrics_test": m_tst
    }

# -------------------------------
# Run only the dual-encoder across seeds
# -------------------------------
all_runs = []
for s in SEEDS:
    print(f"\n=== {EMO_NAME} | seed {s} | protocol: {PROTOCOL} ===")
    res = train_dual_encoder(seed=s)
    all_runs.append(res)

# -------------------------------
# Aggregate (TEST) and save tables
# -------------------------------
rows = []
for r in all_runs:
    y, p = r["test"]["y"], r["test"]["pred"]
    m = metrics(y,p)
    rows.append({
        "model": r["model"], "seed": r["seed"], "protocol": r["protocol"],
        **m,
        "latency_ms": r["latency_ms"], "size_mb": r["size_mb"], "train_dir": r["train_dir"]
    })

df_runs = pd.DataFrame(rows)
df_runs.to_csv(OUT_DIR/"runs_detailed_dual_only.csv", index=False)

summary = df_runs.groupby("model").agg(
    f1_mean=("f1","mean"), f1_std=("f1","std"),
    f1macro_mean=("f1_macro","mean"), f1macro_std=("f1_macro","std"),
    acc_mean=("acc","mean"), acc_std=("acc","std"),
    prec_mean=("prec","mean"), prec_std=("prec","std"),
    rec_mean=("rec","mean"), rec_std=("rec","std"),
    latency_ms_mean=("latency_ms","mean"),
    size_mb_mean=("size_mb","mean"),
    n_runs=("f1","count")
).reset_index().sort_values("f1_mean", ascending=False)

summary.round(3).to_csv(OUT_DIR/"summary_dual_only_mean_std.csv", index=False)
print("\n=== Summary (TEST; mean ± std) — Dual Only ===")
print(summary.round(3))

import os, json, glob, math, re, time, random
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, brier_score_loss
)
from sklearn.model_selection import train_test_split

# SciPy >=1.9 uses binomtest; keep backward-compat
try:
    from scipy.stats import binomtest as _binomtest
except ImportError:
    from scipy.stats import binom_test as _binomtest  # older SciPy

# --------------------------
# Config
# --------------------------
DATA_CSV    = "df_filtered.csv"        # cleaned data
TEXT_COL    = "text"
LABEL_COL   = "label"
SOURCE_COL  = "source"
BASE_SEED   = 42                        # must match training
OUT_ROOT    = Path("./out_fair")        # where all runs are stored
ECE_BINS    = 10

# --------------------------
# Utils
# --------------------------
def metrics(y_true, y_pred):
    return {
        "acc": accuracy_score(y_true, y_pred),
        "prec": precision_score(y_true, y_pred, zero_division=0),
        "rec":  recall_score(y_true, y_pred, zero_division=0),
        "f1":   f1_score(y_true, y_pred, zero_division=0),
        "f1_macro": f1_score(y_true, y_pred, average="macro", zero_division=0),
    }

def expected_calibration_error(y_true, p_pos, n_bins=ECE_BINS):
    # y_true ∈ {0,1}, p_pos ∈ [0,1]
    y_true = np.asarray(y_true).astype(int)
    p_pos  = np.asarray(p_pos)
    bins = np.linspace(0., 1., n_bins+1)
    ece = 0.0
    for i in range(n_bins):
        lo, hi = bins[i], bins[i+1]
        mask = (p_pos > lo) & (p_pos <= hi) if i>0 else (p_pos >= lo) & (p_pos <= hi)
        if not np.any(mask):
            continue
        conf = p_pos[mask].mean()
        acc  = (y_true[mask] == (p_pos[mask] >= 0.5)).mean()
        w    = mask.mean()
        ece += w * abs(acc - conf)
    return float(ece)

def temperature_scale(logits, y_true):
    """
    Fit temperature T by minimizing NLL on validation logits.
    logits: shape [N,2], y_true in {0,1}
    Returns scalar T (>0). Implementation: simple 1D search.
    """
    from scipy.optimize import fminbound
    y = np.asarray(y_true).astype(int)
    z = np.asarray(logits, dtype=np.float64)

    def nll(T):
        T = max(T, 1e-3)
        zt = z / T
        ez = np.exp(zt - zt.max(axis=1, keepdims=True))
        p  = ez / ez.sum(axis=1, keepdims=True)
        # negative log likelihood
        p_y = p[np.arange(len(y)), y]
        return -np.log(np.clip(p_y, 1e-12, 1)).mean()

    T_hat = fminbound(nll, 1e-2, 50.0, maxfun=200, disp=0)
    return float(T_hat)

def softmax(logits):
    z = logits - logits.max(axis=1, keepdims=True)
    ez = np.exp(z)
    return ez / ez.sum(axis=1, keepdims=True)

def paired_bootstrap_delta_f1(y, p1, p2, B=5000, seed=123):
    """
    y: labels; p1,p2: predictions (0/1) from two models on SAME samples.
    Returns: (delta_mean, 95% CI tuple)
    """
    rng = np.random.default_rng(seed)
    y = np.asarray(y); p1 = np.asarray(p1); p2 = np.asarray(p2)
    n = len(y)
    deltas = []
    for _ in range(B):
        idx = rng.integers(0, n, n)
        f1_1 = f1_score(y[idx], p1[idx], zero_division=0)
        f1_2 = f1_score(y[idx], p2[idx], zero_division=0)
        deltas.append(f1_2 - f1_1)
    deltas = np.array(deltas)
    lo, hi = np.percentile(deltas, [2.5, 97.5])
    return float(deltas.mean()), (float(lo), float(hi))

def mcnemar_test(p1, p2, y):
    """
    McNemar's test on paired predictions.
    Returns: b (p1 right, p2 wrong), c (p1 wrong, p2 right), p-value (binomial exact).
    """
    y = np.asarray(y); p1 = np.asarray(p1); p2 = np.asarray(p2)
    b = int(np.sum((p1 == y) & (p2 != y)))  # p1 correct, p2 wrong
    c = int(np.sum((p1 != y) & (p2 == y)))  # p1 wrong, p2 correct
    if b + c == 0:
        pval = 1.0
    else:
        # two-sided exact binomial test (SciPy new API)
        pval = _binomtest(min(b, c), n=b+c, p=0.5, alternative="two-sided").pvalue
    return b, c, float(pval)

def simple_error_tags(text):
    t = str(text)
    tags = []
    if re.search(r"[\"“”']", t): tags.append("quotes")
    if "?" in t: tags.append("rhet_q")
    if "!" in t: tags.append("exclaim")
    if re.search(r"\b(not|never|no|cannot|can't|don't|won't)\b", t, flags=re.I): tags.append("negation")
    if re.search(r":[\)\(DPp]|😂|🤣|😒|😉|🙄", t): tags.append("emoji")
    if re.search(r"#\w+", t): tags.append("hashtag")
    return ",".join(tags) if tags else "none"

# --------------------------
# 1) Discover runs and load outputs
# --------------------------
runs = []  # each item: dict(model, seed, paths, metrics, meta)
for meta_path in OUT_ROOT.rglob("meta.json"):
    save_root = meta_path.parent
    try:
        meta = json.loads(Path(meta_path).read_text())
    except Exception:
        continue
    model_name = meta.get("pretty_name") or meta.get("model_pretrained", {}).get("base", str(save_root.parent.name))
    seed = meta.get("seed", None)
    preds_test = save_root/"preds_test.csv"
    preds_val  = save_root/"preds_val.csv"
    if preds_test.exists() and preds_val.exists():
        df_t = pd.read_csv(preds_test)   # columns: y, pred, p_sarc
        df_v = pd.read_csv(preds_val)
        m = metrics(df_t["y"], df_t["pred"])
        runs.append({
            "model": model_name,
            "seed": seed,
            "root": str(save_root),
            "meta": meta,
            "test": df_t,
            "val": df_v,
            "metrics_test": m
        })

# Summary across models/seeds
rows = []
for r in runs:
    meta = r["meta"]
    rows.append({
        "model": r["model"],
        "seed": r["seed"],
        **r["metrics_test"],
        "latency_ms": meta.get("latency_ms_per_sample_proxy", np.nan),
        "size_mb": meta.get("size_mb", np.nan),
        "root": r["root"]
    })
df_runs = pd.DataFrame(rows)
print("\n=== All runs (per seed) — TEST ===")
print(df_runs.sort_values(["model","seed"]).round(4))

summary = df_runs.groupby("model").agg(
    f1_mean=("f1","mean"), f1_std=("f1","std"),
    f1macro_mean=("f1_macro","mean"), f1macro_std=("f1_macro","std"),
    acc_mean=("acc","mean"), acc_std=("acc","std"),
    prec_mean=("prec","mean"), prec_std=("prec","std"),
    rec_mean=("rec","mean"), rec_std=("rec","std"),
    latency_ms_mean=("latency_ms","mean"),
    size_mb_mean=("size_mb","mean"),
    n_runs=("f1","count")
).reset_index().sort_values("f1_mean", ascending=False)

print("\n=== Summary (TEST; mean ± std) ===")
print(summary.round(4))

# --------------------------
# 2) Significance: compare top two models (if available)
# --------------------------
if summary.shape[0] >= 2:
    A = summary.iloc[0]["model"]   # top by mean F1
    B = summary.iloc[1]["model"]
    print(f"\n=== Significance between top two: {A} vs {B} ===")

    # Reconstruct held-out TEST indices used at training time
    df = pd.read_csv(DATA_CSV)
    X = df[TEXT_COL].astype(str).tolist()
    y = df[LABEL_COL].astype(int).tolist()

    X_train, X_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(
        X, y, np.arange(len(X)), test_size=0.30, stratify=y, random_state=BASE_SEED
    )
    X_val, X_test, y_val, y_test, idx_val, idx_test = train_test_split(
        X_temp, y_temp, idx_temp, test_size=(2/3), stratify=y_temp, random_state=BASE_SEED
    )

    def first_run_for(model_name):
        for r in runs:
            if r["model"] == model_name:
                return r
        return None

    rA = first_run_for(A); rB = first_run_for(B)
    if rA is not None and rB is not None:
        y_true = np.asarray(rA["test"]["y"])
        pA     = np.asarray(rA["test"]["pred"])
        pB     = np.asarray(rB["test"]["pred"])

        d_mean, (ci_lo, ci_hi) = paired_bootstrap_delta_f1(y_true, pA, pB, B=5000, seed=123)
        b,c,pv = mcnemar_test(pA, pB, y_true)
        print(f"ΔF1 (B−A) = {d_mean:.4f}  95% CI [{ci_lo:.4f}, {ci_hi:.4f}]")
        print(f"McNemar: b={b} (A correct/B wrong), c={c} (A wrong/B correct), p={pv:.4g}")
    else:
        print("Could not find both models for significance test; adjust model names.")

# --------------------------
# 3) Calibration (ECE) + Temperature scaling (fit on VAL, report on TEST)
# --------------------------
cal_rows = []
for r in runs:
    model = r["model"]
    val_logits_path = Path(r["root"]) / "logits_val.npz"
    tst_logits_path = Path(r["root"]) / "logits_test.npz"
    if val_logits_path.exists() and tst_logits_path.exists():
        val_npz = np.load(val_logits_path)
        tst_npz = np.load(tst_logits_path)
        yv = val_npz["y"]; lv = val_npz["logits"]
        yt = tst_npz["y"]; lt = tst_npz["logits"]
        # unscaled probs
        pv_test_raw = softmax(lt)[:,1]
        ece_raw = expected_calibration_error(yt, pv_test_raw)
        brier_raw = brier_score_loss(yt, pv_test_raw)

        # temperature from VAL → apply to TEST
        T = temperature_scale(lv, yv)
        pv_test_cal = softmax(lt / T)[:,1]
        ece_cal = expected_calibration_error(yt, pv_test_cal)
        brier_cal = brier_score_loss(yt, pv_test_cal)

        cal_rows.append({
            "model": model, "seed": r["seed"], "T": T,
            "ece_test_raw": ece_raw, "ece_test_cal": ece_cal,
            "brier_test_raw": brier_raw, "brier_test_cal": brier_cal
        })
    else:
        # fall back to using saved probabilities (no temp scaling possible)
        t = r["test"]
        ece_raw = expected_calibration_error(t["y"], t["p_sarc"])
        brier_raw = brier_score_loss(t["y"], t["p_sarc"])
        cal_rows.append({
            "model": model, "seed": r["seed"], "T": np.nan,
            "ece_test_raw": ece_raw, "ece_test_cal": np.nan,
            "brier_test_raw": brier_raw, "brier_test_cal": np.nan
        })

df_cal = pd.DataFrame(cal_rows)
print("\n=== Calibration (TEST) — ECE/Brier before vs after Temperature Scaling ===")
print(df_cal.groupby("model").agg(
    ece_raw_mean=("ece_test_raw","mean"),
    ece_cal_mean=("ece_test_cal","mean"),
    brier_raw_mean=("brier_test_raw","mean"),
    brier_cal_mean=("brier_test_cal","mean"),
    n=("ece_test_raw","count")
).round(4).reset_index())

# --------------------------
# 4) Misclassification table (+ simple error tags)
# --------------------------
df_all = pd.read_csv(DATA_CSV)
X = df_all[TEXT_COL].astype(str).tolist()
y = df_all[LABEL_COL].astype(int).tolist()
src = df_all[SOURCE_COL].astype(str).tolist()

_, X_temp, _, y_temp, _, idx_temp = train_test_split(
    X, y, np.arange(len(X)), test_size=0.30, stratify=y, random_state=BASE_SEED
)
_, X_test, _, y_test, _, idx_test = train_test_split(
    X_temp, y_temp, idx_temp, test_size=(2/3), stratify=y_temp, random_state=BASE_SEED
)

deploy_model = summary.iloc[0]["model"] if len(summary) else (runs[0]["model"] if runs else None)
if deploy_model:
    r_dep = [r for r in runs if r["model"]==deploy_model][0]
    t = r_dep["test"]  # y, pred, p_sarc in the same order as test set
    df_err = pd.DataFrame({
        "row_id": idx_test,
        "source": np.array(src)[idx_test],
        "text":   np.array(X)[idx_test],
        "y_true": t["y"].values,
        "y_pred": t["pred"].values,
        "p_sarc": t["p_sarc"].values
    })
    df_wrong = df_err[df_err["y_true"] != df_err["y_pred"]].copy()
    df_wrong["tags"] = df_wrong["text"].map(simple_error_tags)

    # Top-confidence mistakes (most overconfident wrongs)
    df_wrong["conf"] = np.where(df_wrong["y_pred"]==1, df_wrong["p_sarc"], 1.0 - df_wrong["p_sarc"])
    df_wrong_sorted = df_wrong.sort_values("conf", ascending=False)

    print(f"\n=== Misclassifications for {deploy_model} (TEST) ===")
    print("Counts by source:")
    print(df_wrong.groupby("source").size().sort_values(ascending=False))
    print("\nTop 10 overconfident mistakes:")
    print(df_wrong_sorted[["source","y_true","y_pred","conf","tags","text"]].head(10).to_string(index=False))

# --------------------------
# 5) Per‑domain (source) metrics on TEST for every model
# --------------------------
print("\n=== Per‑domain metrics (TEST) ===")
per_domain_rows = []
for r in runs:
    model = r["model"]
    t = r["test"]
    # align with test indices we reconstructed
    df_join = pd.DataFrame({
        "source": np.array(src)[idx_test],
        "y": t["y"].values,
        "pred": t["pred"].values
    })
    for sname, g in df_join.groupby("source"):
        m = metrics(g["y"].values, g["pred"].values)
        per_domain_rows.append({"model": model, "source": sname, **m})
df_per_domain = pd.DataFrame(per_domain_rows)
for sname, g in df_per_domain.groupby("source"):
    print(f"\n[{sname}]")
    print(g.drop(columns=["acc","prec","rec","f1_macro"]).round(4).sort_values("f1", ascending=False))

# Are the big run objects still alive?
'in_globals', 'all_runs' in globals(), 'summary' in globals()

# If yes, what finished?
[len(all_runs), [r['model'] for r in all_runs]]

# If BERT/RoBERTa/DistilRoBERTa runs are there, show where they were saved:
[r['train_dir'] for r in all_runs]

# ===============================
# Sarcasm Detection — Fair Protocol Runner (Singles Only, Separate Folder)
# - Train/Val/Test (70/10/20, stratified, fixed seed)
# - Multi-seed (default 3)
# - Early stopping (patience=1) OR fixed epochs
# - Linear scheduler w/ 10% warmup
# - Saves per-model×seed artifacts (preds/probs/logits/ckpt/meta)
# ===============================

import os, time, json, random
from pathlib import Path

import numpy as np
import pandas as pd

import torch
from torch.utils.data import DataLoader, TensorDataset

from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, classification_report)

from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup, set_seed as hf_set_seed
)

# -------------------------------
# Config
# -------------------------------
DATA_CSV      = "/content/df_filtered.csv"   # <-- update if needed
TEXT_COL      = "text"
LABEL_COL     = "label"

MAX_LEN       = 128
BATCH_TRAIN   = 16
BATCH_EVAL    = 32
LEARNING_RATE = 2e-5
MAX_EPOCHS    = 4             # upper bound; early stopping may stop earlier
PATIENCE      = 1             # early-stopping patience in epochs (no improvement)
SEEDS         = [0, 1, 2]
BASE_SEED     = 42            # for splits
WARMUP_RATIO  = 0.10          # 10% warmup
PROTOCOL      = "early_stopping"  # "early_stopping" or "fixed_epochs"

OUT_DIR       = Path("./out_fair_singles")  # NEW FOLDER NAME

# Models to run (SINGLE-ENCODER ONLY)
MODELS = {
    "BERT": "bert-base-uncased",
    "RoBERTa": "roberta-base",
    "DistilRoBERTa": "distilroberta-base",
}

device = torch.device("cuda" if torch.cuda.is_available()
                      else "mps" if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available()
                      else "cpu")

# -------------------------------
# Helpers
# -------------------------------
def set_all_seeds(seed: int):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    hf_set_seed(seed)

def sanitize(name: str) -> str:
    return name.replace("/", "_").replace(" ", "_")

def make_loader(tokenizer, texts, labels, batch_size=32, shuffle=False, max_len=128):
    toks = tokenizer(list(texts), truncation=True, padding=True, max_length=max_len, return_tensors="pt")
    ds = TensorDataset(toks["input_ids"], toks["attention_mask"], torch.tensor(list(labels), dtype=torch.long))
    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)

@torch.no_grad()
def eval_split(model, dataloader):
    model.eval()
    ys, preds, probs, logits_all = [], [], [], []
    for input_ids, attn, labels in dataloader:
        input_ids, attn = input_ids.to(device), attn.to(device)
        out = model(input_ids, attention_mask=attn)
        logits = out.logits
        logits_all.append(logits.cpu())
        pr = torch.softmax(logits, dim=1).cpu().numpy()
        probs.extend(pr.tolist())
        preds.extend(pr.argmax(1))
        ys.extend(labels.numpy())
    logits_all = torch.cat(logits_all, dim=0).numpy()
    ys, preds, probs = np.array(ys), np.array(preds), np.array(probs)
    return ys, preds, probs, logits_all

def metrics(y, p):
    return {
        "acc": accuracy_score(y,p),
        "prec": precision_score(y,p, zero_division=0),
        "rec": recall_score(y,p, zero_division=0),
        "f1": f1_score(y,p, zero_division=0),
        "f1_macro": f1_score(y,p, average="macro", zero_division=0),
    }

def measure_latency_ms(model, tokenizer, sample_text, reps=50):
    sample = tokenizer([sample_text], truncation=True, padding=True, max_length=MAX_LEN, return_tensors="pt")
    sample = {k:v.to(device) for k,v in sample.items()}
    model.eval()
    with torch.no_grad(): _ = model(**sample)  # warmup
    t0 = time.time()
    with torch.no_grad():
        for _ in range(reps):
            _ = model(**sample)
    t1 = time.time()
    return (t1 - t0) / reps * 1000.0

def save_dir_size_mb(path: Path) -> float:
    total = 0
    for p in path.rglob("*"):
        if p.is_file(): total += p.stat().st_size
    return total / (1024*1024)

# -------------------------------
# Data split (70/10/20 stratified)
# -------------------------------
df = pd.read_csv(DATA_CSV)
assert {TEXT_COL, LABEL_COL}.issubset(df.columns), f"CSV must contain {TEXT_COL}, {LABEL_COL}"

X_all = df[TEXT_COL].astype(str).tolist()
y_all = df[LABEL_COL].astype(int).tolist()

X_train, X_temp, y_train, y_temp = train_test_split(
    X_all, y_all, test_size=0.30, stratify=y_all, random_state=BASE_SEED
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(2/3), stratify=y_temp, random_state=BASE_SEED
)

OUT_DIR.mkdir(parents=True, exist_ok=True)

# -------------------------------
# Single-encoder trainer
# -------------------------------
def train_single_encoder(pretrained_name: str, pretty_name: str, seed: int):
    set_all_seeds(seed)
    model = AutoModelForSequenceClassification.from_pretrained(pretrained_name, num_labels=2).to(device)
    tokenizer = AutoTokenizer.from_pretrained(pretrained_name)

    train_loader = make_loader(tokenizer, X_train, y_train, batch_size=BATCH_TRAIN, shuffle=True, max_len=MAX_LEN)
    val_loader   = make_loader(tokenizer, X_val,   y_val,   batch_size=BATCH_EVAL,  shuffle=False, max_len=MAX_LEN)
    test_loader  = make_loader(tokenizer, X_test,  y_test,  batch_size=BATCH_EVAL,  shuffle=False, max_len=MAX_LEN)

    # Optimizer + Scheduler
    optim = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    total_steps = MAX_EPOCHS * len(train_loader)
    warmup_steps = int(WARMUP_RATIO * total_steps)
    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

    best_val_f1 = -1.0
    best_state  = None
    epochs_run  = 0
    epochs_no_improve = 0

    for ep in range(MAX_EPOCHS):
        epochs_run += 1
        model.train()
        tot_loss = 0.0
        for input_ids, attn, labels in train_loader:
            input_ids, attn, labels = input_ids.to(device), attn.to(device), labels.to(device)
            optim.zero_grad()
            out = model(input_ids, attention_mask=attn, labels=labels)
            loss = out.loss
            loss.backward()
            optim.step()
            scheduler.step()
            tot_loss += loss.item()

        # Validation
        yv, pv, qv, lv = eval_split(model, val_loader)
        val_f1 = f1_score(yv, pv, zero_division=0)

        print(f"[{pretty_name}][seed {seed}] epoch {ep+1}/{MAX_EPOCHS} "
              f"train_loss={tot_loss/len(train_loader):.4f} val_f1={val_f1:.4f}")

        # Early stopping
        if PROTOCOL == "early_stopping":
            if val_f1 > best_val_f1 + 1e-6:
                best_val_f1 = val_f1
                best_state  = {k: v.cpu().clone() for k, v in model.state_dict().items()}
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve > PATIENCE:
                    print(f"Early stopping at epoch {ep+1} (best val F1={best_val_f1:.4f})")
                    break

    if PROTOCOL == "early_stopping" and best_state is not None:
        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})

    # Final eval
    yv, pv, qv, lv = eval_split(model, val_loader)
    yt, pt, qt, lt = eval_split(model, test_loader)

    latency_ms = measure_latency_ms(model, tokenizer, X_test[0] if len(X_test) else "ok")
    save_path = OUT_DIR / sanitize(pretty_name) / f"{PROTOCOL}_seed_{seed}"
    save_path.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    size_mb = save_dir_size_mb(save_path)

    # Save artifacts
    pd.DataFrame({"y": yv, "pred": pv, "p_sarc": qv[:,1]}).to_csv(save_path/"preds_val.csv", index=False)
    pd.DataFrame({"y": yt, "pred": pt, "p_sarc": qt[:,1]}).to_csv(save_path/"preds_test.csv", index=False)
    np.savez(save_path/"logits_val.npz", logits=lv, y=yv)
    np.savez(save_path/"logits_test.npz", logits=lt, y=yt)

    with open(save_path/"meta.json","w") as f:
        json.dump({
            "model_pretrained": pretrained_name,
            "pretty_name": pretty_name,
            "protocol": PROTOCOL,
            "seed": seed,
            "max_epochs": MAX_EPOCHS,
            "patience": PATIENCE,
            "lr": LEARNING_RATE,
            "batch_train": BATCH_TRAIN,
            "batch_eval": BATCH_EVAL,
            "max_len": MAX_LEN,
            "warmup_ratio": WARMUP_RATIO,
            "epochs_run": epochs_run,
            "best_val_f1": float(f1_score(yv, pv, zero_division=0)),
            "latency_ms_per_sample": float(latency_ms),
            "size_mb": float(size_mb)
        }, f, indent=2)

    with open(save_path/"report_test.txt","w") as f:
        f.write(classification_report(yt, pt, target_names=["Not Sarcastic","Sarcastic"], digits=3))

    return {
        "model": pretty_name,
        "seed": seed,
        "protocol": PROTOCOL,
        "val": {"y": yv, "pred": pv, "probs": qv, "logits": lv},
        "test":{"y": yt, "pred": pt, "probs": qt, "logits": lt},
        "latency_ms": latency_ms,
        "size_mb": size_mb,
        "train_dir": str(save_path)
    }

# -------------------------------
# Run all models × seeds
# -------------------------------
all_runs = []
for pretty, pretrained in MODELS.items():
    for s in SEEDS:
        print(f"\n=== {pretty} | seed {s} | protocol: {PROTOCOL} ===")
        res = train_single_encoder(pretrained, pretty, seed=s)
        all_runs.append(res)

# -------------------------------
# Aggregate (TEST) and save tables
# -------------------------------
rows = []
for r in all_runs:
    y, p = r["test"]["y"], r["test"]["pred"]
    m = metrics(y,p)
    rows.append({
        "model": r["model"], "seed": r["seed"], "protocol": r["protocol"],
        **m,
        "latency_ms": r["latency_ms"], "size_mb": r["size_mb"], "train_dir": r["train_dir"]
    })

df_runs = pd.DataFrame(rows)
OUT_DIR.mkdir(parents=True, exist_ok=True)
df_runs.to_csv(OUT_DIR/"runs_detailed_singles.csv", index=False)

summary = df_runs.groupby("model").agg(
    f1_mean=("f1","mean"), f1_std=("f1","std"),
    f1macro_mean=("f1_macro","mean"), f1macro_std=("f1_macro","std"),
    acc_mean=("acc","mean"), acc_std=("acc","std"),
    prec_mean=("prec","mean"), prec_std=("prec","std"),
    rec_mean=("rec","mean"), rec_std=("rec","std"),
    latency_ms_mean=("latency_ms","mean"),
    size_mb_mean=("size_mb","mean"),
    n_runs=("f1","count")
).reset_index().sort_values("f1_mean", ascending=False)

summary.round(3).to_csv(OUT_DIR/"summary_singles_mean_std.csv", index=False)
print("\n=== Summary (TEST; mean ± std) — Singles Only ===")
print(summary.round(3))

from google.colab import drive
drive.mount('/content/drive')

# 1) Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# 2) Make a persistent target dir
import os, shutil, pathlib
src = pathlib.Path('out_fair_singles')              # where your run saved
dst = pathlib.Path('/content/drive/MyDrive/sarcasm/out_fair_singles')
dst.mkdir(parents=True, exist_ok=True)

# 3) Copy everything (safe, recursive)
def copytree(src, dst):
    src = pathlib.Path(src); dst = pathlib.Path(dst)
    for p in src.rglob('*'):
        rel = p.relative_to(src)
        target = dst/rel
        if p.is_dir():
            target.mkdir(parents=True, exist_ok=True)
        else:
            target.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(p, target)

copytree(src, dst)
print('Copied to:', dst)

# ===============================
# Sarcasm Detection — Singles (Seeds → GDrive, 3 Epochs, Rich Analysis)
# ===============================
import os, time, json, random, math, datetime, re
from pathlib import Path
from collections import defaultdict, Counter

import numpy as np
import pandas as pd

import torch
from torch.utils.data import DataLoader, TensorDataset

from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, classification_report, brier_score_loss)

from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup, set_seed as hf_set_seed
)

# -------------------------------
# Google Drive (Colab) — persistence
# -------------------------------
try:
    from google.colab import drive  # type: ignore
    IN_COLAB = True
except Exception:
    IN_COLAB = False

if IN_COLAB:
    drive.mount('/content/drive')

# -------------------------------
# Config
# -------------------------------
DATA_CSV      = "/content/df_filtered.csv"   # update if needed
TEXT_COL      = "text"
LABEL_COL     = "label"
SOURCE_COL    = "source"  # optional; if absent, domain analysis is skipped

MAX_LEN       = 128
BATCH_TRAIN   = 16
BATCH_EVAL    = 32
LEARNING_RATE = 2e-5

# EXACTLY 3 EPOCHS (fixed)
MAX_EPOCHS    = 3
WARMUP_RATIO  = 0.10
SEEDS         = [0, 1, 2]
BASE_SEED     = 42
PROTOCOL      = "fixed_epochs"  # ← guarantees 3 epochs

# Output to Google Drive
RUN_TAG       = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
OUT_BASE      = Path("/content/drive/MyDrive/sarcasm_runs_singles_v2") if IN_COLAB else Path("./sarcasm_runs_singles_v2")
OUT_DIR       = OUT_BASE / f"run_{RUN_TAG}"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# Models to run (single-encoder only)
MODELS = {
    "BERT": "bert-base-uncased",
    "RoBERTa": "roberta-base",
    "DistilRoBERTa": "distilroberta-base",
}

device = torch.device(
    "cuda" if torch.cuda.is_available()
    else "mps" if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available()
    else "cpu"
)

# -------------------------------
# Helpers
# -------------------------------
def set_all_seeds(seed: int):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    hf_set_seed(seed)

def sanitize(name: str) -> str:
    return name.replace("/", "_").replace(" ", "_")

def make_loader(tokenizer, texts, labels, batch_size=32, shuffle=False, max_len=128):
    toks = tokenizer(list(texts), truncation=True, padding=True, max_length=max_len, return_tensors="pt")
    ds = TensorDataset(toks["input_ids"], toks["attention_mask"], torch.tensor(list(labels), dtype=torch.long))
    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)

@torch.no_grad()
def eval_split(model, dataloader):
    model.eval()
    ys, preds, probs, logits_all = [], [], [], []
    for input_ids, attn, labels in dataloader:
        input_ids, attn = input_ids.to(device), attn.to(device)
        out = model(input_ids, attention_mask=attn)
        logits = out.logits
        logits_all.append(logits.cpu())
        pr = torch.softmax(logits, dim=1).cpu().numpy()
        probs.extend(pr.tolist())
        preds.extend(pr.argmax(1))
        ys.extend(labels.numpy())
    logits_all = torch.cat(logits_all, dim=0).numpy()
    ys, preds, probs = np.array(ys), np.array(preds), np.array(probs)
    return ys, preds, probs, logits_all

def basic_metrics(y, p):
    return {
        "acc": accuracy_score(y,p),
        "prec": precision_score(y,p, zero_division=0),
        "rec": recall_score(y,p, zero_division=0),
        "f1": f1_score(y,p, zero_division=0),
        "f1_macro": f1_score(y,p, average="macro", zero_division=0),
    }

def measure_latency_ms(model, tokenizer, sample_text, reps=50):
    sample = tokenizer([sample_text], truncation=True, padding=True, max_length=MAX_LEN, return_tensors="pt")
    sample = {k:v.to(device) for k,v in sample.items()}
    model.eval()
    with torch.no_grad(): _ = model(**sample)  # warmup
    t0 = time.time()
    with torch.no_grad():
        for _ in range(reps):
            _ = model(**sample)
    t1 = time.time()
    return (t1 - t0) / reps * 1000.0

def save_dir_size_mb(path: Path) -> float:
    total = 0
    for p in path.rglob("*"):
        if p.is_file(): total += p.stat().st_size
    return total / (1024*1024)

def ece_score(y_true, p_pos, n_bins=15):
    """Expected Calibration Error for binary classification."""
    y_true = np.asarray(y_true).astype(int)
    p_pos  = np.asarray(p_pos).astype(float)
    bins = np.linspace(0.0, 1.0, n_bins+1)
    ece = 0.0
    bin_stats = []
    for i in range(n_bins):
        lo, hi = bins[i], bins[i+1]
        mask = (p_pos > lo) & (p_pos <= hi) if i>0 else (p_pos >= lo) & (p_pos <= hi)
        if not np.any(mask):
            bin_stats.append({"bin_lo": lo, "bin_hi": hi, "n": 0, "conf": None, "acc": None})
            continue
        conf = p_pos[mask].mean()
        acc  = (y_true[mask] == (p_pos[mask] >= 0.5)).mean()
        w    = mask.mean()
        ece += w * abs(acc - conf)
        bin_stats.append({"bin_lo": lo, "bin_hi": hi, "n": int(mask.sum()), "conf": float(conf), "acc": float(acc)})
    return float(ece), bin_stats

# Heuristic pattern flags for misclassification analysis
_NEGATION_RE   = re.compile(r"\b(no|not|never|n't|cannot|can't|won't|don'?t)\b", re.IGNORECASE)
_EXCLAM_RE     = re.compile(r"!{1,}")
_HYPERBOLE_RE  = re.compile(r"\b(always|never|literally|absolutely|everyone|no one|best|worst|totally|completely)\b", re.IGNORECASE)
_OVERCONF_RE   = re.compile(r"\b(of course|obviously|clearly|as everyone knows|without a doubt)\b", re.IGNORECASE)

def pattern_flags(text):
    return {
        "negation": bool(_NEGATION_RE.search(text)),
        "exclamation": bool(_EXCLAM_RE.search(text)),
        "hyperbole": bool(_HYPERBOLE_RE.search(text)),
        "overconfidence": bool(_OVERCONF_RE.search(text)),
    }

# -------------------------------
# Data split (70/10/20 stratified)
# -------------------------------
df = pd.read_csv(DATA_CSV)
assert {TEXT_COL, LABEL_COL}.issubset(df.columns), f"CSV must contain {TEXT_COL}, {LABEL_COL}"

has_source = SOURCE_COL in df.columns
if not has_source:
    print(f"[info] No '{SOURCE_COL}' column detected → domain analysis will be skipped.")

X_all = df[TEXT_COL].astype(str).tolist()
y_all = df[LABEL_COL].astype(int).tolist()
src_all = df[SOURCE_COL].astype(str).tolist() if has_source else ["_"] * len(df)

X_train, X_temp, y_train, y_temp, s_train, s_temp = train_test_split(
    X_all, y_all, src_all, test_size=0.30, stratify=y_all, random_state=BASE_SEED
)
X_val, X_test, y_val, y_test, s_val, s_test = train_test_split(
    X_temp, y_temp, s_temp, test_size=(2/3), stratify=y_temp, random_state=BASE_SEED
)

# Save the split indices/texts to the run folder for reproducibility
(pd.DataFrame({ "split":["train"]*len(X_train)+["val"]*len(X_val)+["test"]*len(X_test),
                "text":X_train+X_val+X_test,
                "label":y_train+y_val+y_test,
                "source":s_train+s_val+s_test if has_source else ["_"]*(len(X_train)+len(X_val)+len(X_test))})
 .to_csv(OUT_DIR/"data_splits.csv", index=False))

# -------------------------------
# Training (fixed 3 epochs)
# -------------------------------
def train_single_encoder(pretrained_name: str, pretty_name: str, seed: int):
    set_all_seeds(seed)
    model = AutoModelForSequenceClassification.from_pretrained(pretrained_name, num_labels=2).to(device)
    tokenizer = AutoTokenizer.from_pretrained(pretrained_name)

    train_loader = make_loader(tokenizer, X_train, y_train, batch_size=BATCH_TRAIN, shuffle=True,  max_len=MAX_LEN)
    val_loader   = make_loader(tokenizer, X_val,   y_val,   batch_size=BATCH_EVAL,  shuffle=False, max_len=MAX_LEN)
    test_loader  = make_loader(tokenizer, X_test,  y_test,  batch_size=BATCH_EVAL,  shuffle=False, max_len=MAX_LEN)

    optim = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    total_steps = MAX_EPOCHS * len(train_loader)
    warmup_steps = int(WARMUP_RATIO * total_steps)
    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

    for ep in range(MAX_EPOCHS):
        model.train()
        tot_loss = 0.0
        for input_ids, attn, labels in train_loader:
            input_ids, attn, labels = input_ids.to(device), attn.to(device), labels.to(device)
            optim.zero_grad()
            out = model(input_ids, attention_mask=attn, labels=labels)
            loss = out.loss
            loss.backward()
            optim.step()
            scheduler.step()
            tot_loss += loss.item()
        yv, pv, qv, _ = eval_split(model, val_loader)
        print(f"[{pretty_name}][seed {seed}] epoch {ep+1}/{MAX_EPOCHS} "
              f"train_loss={tot_loss/len(train_loader):.4f} val_f1={f1_score(yv,pv,zero_division=0):.4f}")

    # Final eval on test
    yt, pt, qt, lt = eval_split(model, test_loader)
    latency_ms = measure_latency_ms(model, tokenizer, X_test[0] if len(X_test) else "ok")

    save_path = OUT_DIR / sanitize(pretty_name) / f"{PROTOCOL}_seed_{seed}"
    save_path.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    size_mb = save_dir_size_mb(save_path)

    # Save artifacts
    pd.DataFrame({"text": X_test, "y": yt, "pred": pt, "p_sarc": qt[:,1], "source": s_test}).to_csv(save_path/"preds_test.csv", index=False)
    np.savez(save_path/"logits_test.npz", logits=lt, y=yt)

    with open(save_path/"report_test.txt","w") as f:
        f.write(classification_report(yt, pt, target_names=["Not Sarcastic","Sarcastic"], digits=3))

    # Calibration (binary)
    p_pos = qt[:,1]
    ece, bins = ece_score(yt, p_pos, n_bins=15)
    brier = brier_score_loss(yt, p_pos)
    with open(save_path/"calibration.json","w") as f:
        json.dump({"ece": ece, "brier": brier, "bins": bins}, f, indent=2)

    # Domain analysis by source (if available)
    domain_metrics = {}
    if has_source:
        df_pred = pd.DataFrame({"source": s_test, "y": yt, "pred": pt, "p_pos": p_pos, "text": X_test})
        for src, g in df_pred.groupby("source"):
            m = basic_metrics(g["y"].values, g["pred"].values)
            e_src, _ = ece_score(g["y"].values, g["p_pos"].values, n_bins=10)
            b_src = brier_score_loss(g["y"].values, g["p_pos"].values)
            domain_metrics[src] = {**m, "ece": e_src, "brier": b_src, "n": int(len(g))}
        with open(save_path/"domain_metrics.json","w") as f:
            json.dump(domain_metrics, f, indent=2)

    # Misclassification analysis
    df_err = pd.DataFrame({"text": X_test, "y": yt, "pred": pt, "p_pos": p_pos, "source": s_test})
    df_err["is_error"] = df_err["y"] != df_err["pred"]
    df_err["negation"] = df_err["text"].apply(lambda t: pattern_flags(t)["negation"])
    df_err["exclamation"] = df_err["text"].apply(lambda t: pattern_flags(t)["exclamation"])
    df_err["hyperbole"] = df_err["text"].apply(lambda t: pattern_flags(t)["hyperbole"])
    df_err["overconfidence"] = df_err["text"].apply(lambda t: pattern_flags(t)["overconfidence"])

    mis_counts = {
        "total_errors": int(df_err["is_error"].sum()),
        "negation_errors": int(((df_err["is_error"]) & (df_err["negation"])).sum()),
        "exclamation_errors": int(((df_err["is_error"]) & (df_err["exclamation"])).sum()),
        "hyperbole_errors": int(((df_err["is_error"]) & (df_err["hyperbole"])).sum()),
        "overconfidence_errors": int(((df_err["is_error"]) & (df_err["overconfidence"])).sum()),
    }
    df_err.to_csv(save_path/"errors_tagged.csv", index=False)
    with open(save_path/"misclassification_summary.json","w") as f:
        json.dump(mis_counts, f, indent=2)

    # Return summary row (for per-model aggregation later)
    m = basic_metrics(yt, pt)
    return {
        "model": pretty_name,
        "seed": seed,
        **m,
        "latency_ms": float(latency_ms),
        "size_mb": float(size_mb),
        "ece": float(ece),
        "brier": float(brier),
        "train_dir": str(save_path),
        "domain_metrics": domain_metrics if has_source else None,
        "mis_summary": mis_counts,
    }

# -------------------------------
# Run per model (aggregate across seeds) and print per-model report
# -------------------------------
def mean_std(series):
    return float(np.mean(series)), float(np.std(series, ddof=1)) if len(series)>1 else 0.0

all_rows = []
for pretty, pretrained in MODELS.items():
    print(f"\n================ {pretty} — running seeds {SEEDS} ================")
    per_seed = []
    for s in SEEDS:
        print(f"\n=== {pretty} | seed {s} | protocol: {PROTOCOL} | epochs={MAX_EPOCHS} ===")
        res = train_single_encoder(pretrained, pretty, seed=s)
        per_seed.append(res)
        all_rows.append(res)

    # Aggregate across seeds for this model
    def agg(key): return [r[key] for r in per_seed]
    f1_mu, f1_sd         = mean_std(agg("f1"))
    acc_mu, acc_sd       = mean_std(agg("acc"))
    prec_mu, prec_sd     = mean_std(agg("prec"))
    rec_mu, rec_sd       = mean_std(agg("rec"))
    lat_mu, lat_sd       = mean_std(agg("latency_ms"))
    ece_mu, ece_sd       = mean_std(agg("ece"))
    brier_mu, brier_sd   = mean_std(agg("brier"))
    size_mu, size_sd     = mean_std(agg("size_mb"))

    # Aggregate domain metrics if present
    domain_table = None
    if any(r["domain_metrics"] is not None for r in per_seed):
        # collect per-domain stats across seeds
        buckets = defaultdict(list)
        for r in per_seed:
            for dom, met in r["domain_metrics"].items():
                buckets[dom].append(met)
        rows = []
        for dom, lst in buckets.items():
            rows.append({
                "domain": dom,
                "n_mean": float(np.mean([x["n"] for x in lst])),
                "f1_mean": float(np.mean([x["f1"] for x in lst])),
                "f1_std": float(np.std([x["f1"] for x in lst], ddof=1)) if len(lst)>1 else 0.0,
                "acc_mean": float(np.mean([x["acc"] for x in lst])),
                "ece_mean": float(np.mean([x["ece"] for x in lst])),
                "brier_mean": float(np.mean([x["brier"] for x in lst])),
            })
        domain_table = pd.DataFrame(rows).sort_values("f1_mean", ascending=False)
        domain_table.to_csv(OUT_DIR/sanitize(pretty)/"domain_analysis_mean.csv", index=False)

    # Aggregate misclassification tags
    mis_total = sum(r["mis_summary"]["total_errors"] for r in per_seed)
    mis_neg   = sum(r["mis_summary"]["negation_errors"] for r in per_seed)
    mis_excl  = sum(r["mis_summary"]["exclamation_errors"] for r in per_seed)
    mis_hyp   = sum(r["mis_summary"]["hyperbole_errors"] for r in per_seed)
    mis_over  = sum(r["mis_summary"]["overconfidence_errors"] for r in per_seed)

    # PRINT the per-transformer (across seeds) report
    print(f"\n--- {pretty} | Seeds {SEEDS} — Model Comparison Block ---")
    print(f"F1: {f1_mu:.3f} ± {f1_sd:.3f} | Acc: {acc_mu:.3f} ± {acc_sd:.3f} | "
          f"Prec: {prec_mu:.3f} ± {prec_sd:.3f} | Rec: {rec_mu:.3f} ± {rec_sd:.3f}")
    print(f"Latency (ms/sample): {lat_mu:.1f} ± {lat_sd:.1f} | Size (MB): {size_mu:.1f} ± {size_sd:.1f}")
    print(f"Calibration → ECE: {ece_mu:.3f} ± {ece_sd:.3f} | Brier: {brier_mu:.3f} ± {brier_sd:.3f}")
    if domain_table is not None:
        print("\nDomain analysis (mean across seeds):")
        print(domain_table.round(3).to_string(index=False))
    else:
        print("\nDomain analysis: [skipped — no 'source' column]")
    print("\nMisclassification analysis (total across seeds):")
    print(f"  total_errors={mis_total} | negation={mis_neg} | exclamation={mis_excl} | "
          f"hyperbole={mis_hyp} | overconfidence={mis_over}")
    print("-------------------------------------------------------")

# -------------------------------
# Save overall CSVs for the whole run
# -------------------------------
rows = []
for r in all_rows:
    rows.append({
        "model": r["model"], "seed": r["seed"],
        "f1": r["f1"], "acc": r["acc"], "prec": r["prec"], "rec": r["rec"],
        "latency_ms": r["latency_ms"], "size_mb": r["size_mb"],
        "ece": r["ece"], "brier": r["brier"], "train_dir": r["train_dir"]
    })
df_runs = pd.DataFrame(rows)
df_runs.to_csv(OUT_DIR/"runs_detailed.csv", index=False)

summary = df_runs.groupby("model").agg(
    f1_mean=("f1","mean"), f1_std=("f1","std"),
    acc_mean=("acc","mean"), acc_std=("acc","std"),
    prec_mean=("prec","mean"), prec_std=("prec","std"),
    rec_mean=("rec","mean"), rec_std=("rec","std"),
    latency_ms_mean=("latency_ms","mean"), latency_ms_std=("latency_ms","std"),
    ece_mean=("ece","mean"), ece_std=("ece","std"),
    brier_mean=("brier","mean"), brier_std=("brier","std"),
    size_mb_mean=("size_mb","mean"), size_mb_std=("size_mb","std"),
    n_runs=("f1","count")
).reset_index().sort_values("f1_mean", ascending=False)

summary.round(3).to_csv(OUT_DIR/"summary_mean_std.csv", index=False)
print("\n=== Overall Summary (TEST; mean ± std) — All Singles ===")
print(summary.round(3))
print(f"\nArtifacts saved under: {OUT_DIR}")

import numpy as np
import pandas as pd

# OPTIONAL: if SciPy is available, keep this import; otherwise comment it out.
from scipy.stats import binomtest  # SciPy >= 1.7

# -------------------------
# 1) Point to your artifacts
# -------------------------
# Set your actual run directory (the timestamped one you showed in Drive)
RUN_DIR = "/content/drive/MyDrive/sarcasm_runs_singles_v2/run_20250826_172106"

# Choose models and seed to compare
MODEL_A = "RoBERTa"
MODEL_B = "DistilRoBERTa"
SEED    = 0  # pick 0, 1, or 2

# Build paths to preds_test.csv for each model/seed
roberta_csv = f"{RUN_DIR}/{MODEL_A}/fixed_epochs_seed_{SEED}/preds_test.csv"
distil_csv  = f"{RUN_DIR}/{MODEL_B}/fixed_epochs_seed_{SEED}/preds_test.csv"

# Load predictions
A = pd.read_csv(roberta_csv)     # Model A (e.g., RoBERTa)
B = pd.read_csv(distil_csv)      # Model B (e.g., DistilRoBERTa)

# Align on the same rows (splits were fixed, so order should match)
assert len(A) == len(B), "Mismatch in test set length; check you used the same run/split."
y_true = A["y"].to_numpy().astype(int)
pred_A = A["pred"].to_numpy().astype(int)
pred_B = B["pred"].to_numpy().astype(int)

# -------------------------
# 2) Utility: F1 (binary)
# -------------------------
def f1_binary(y, p):
    tp = np.sum((y == 1) & (p == 1))
    fp = np.sum((y == 0) & (p == 1))
    fn = np.sum((y == 1) & (p == 0))
    if tp == 0 and (fp + fn) == 0:
        return 1.0  # degenerate perfect case
    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0

# -------------------------
# 3) Paired bootstrap on F1
# -------------------------
rng = np.random.default_rng(13)
n = len(y_true)
BBOOT = 10000  # 10k resamples

f1_A = f1_binary(y_true, pred_A)
f1_B = f1_binary(y_true, pred_B)
obs_diff = f1_A - f1_B

idx = np.arange(n)
diffs = np.empty(BBOOT, dtype=float)
for i in range(BBOOT):
    samp = rng.integers(0, n, size=n)  # sample indices with replacement
    diffs[i] = f1_binary(y_true[samp], pred_A[samp]) - f1_binary(y_true[samp], pred_B[samp])

# 95% CI from percentiles
ci_lo, ci_hi = np.percentile(diffs, [2.5, 97.5])

# Two-sided p-value (common bootstrap approximation)
p_two_sided = 2 * min(np.mean(diffs >= 0.0), np.mean(diffs <= 0.0))
p_two_sided = float(min(p_two_sided, 1.0))

print(f"Paired bootstrap (F1_A - F1_B): obs={obs_diff:.4f}, 95% CI=({ci_lo:.4f}, {ci_hi:.4f}), p≈{p_two_sided:.4f}")

# -------------------------
# 4) McNemar’s exact test
# -------------------------
# Build 2x2 on disagreements:
# n01: A correct, B wrong
# n10: A wrong,   B correct
A_correct = (pred_A == y_true)
B_correct = (pred_B == y_true)

n01 = int(np.sum(A_correct & ~B_correct))
n10 = int(np.sum(~A_correct & B_correct))
n_discordant = n01 + n10

print(f"McNemar counts: n01={n01}, n10={n10}, discordant={n_discordant}")

if n_discordant == 0:
    p_mcnemar = 1.0
    print("McNemar exact p-value: 1.0000 (no discordant pairs)")
else:
    # two-sided exact p-value via binomial test under H0: p=0.5
    p_mcnemar = binomtest(k=min(n01, n10), n=n_discordant, p=0.5, alternative='two-sided').pvalue
    print(f"McNemar exact p-value: {p_mcnemar:.4f}")

# -------------------------
# 5) One-liners to report
# -------------------------
print("\nReport:")
print(f"Models: {MODEL_A} vs {MODEL_B} (seed {SEED})")
print(f"F1(A)={f1_A:.3f}, F1(B)={f1_B:.3f}, ΔF1={obs_diff:.3f}, 95% CI[{ci_lo:.3f}, {ci_hi:.3f}], bootstrap p≈{p_two_sided:.4f}")
if n_discordant > 0:
    print(f"McNemar: n01={n01}, n10={n10}, exact p={p_mcnemar:.4f}")
else:
    print("McNemar: no discordant pairs.")

import itertools
import numpy as np
import pandas as pd

# OPTIONAL: if SciPy is unavailable, comment this out and the McNemar block will be skipped automatically.
try:
    from scipy.stats import binomtest  # SciPy >= 1.7
    HAS_SCIPY = True
except Exception:
    HAS_SCIPY = False

# =========================
# 1) Point to your artifacts
# =========================
RUN_DIR = "/content/drive/MyDrive/sarcasm_runs_singles_v2/run_20250826_172106"  # <-- set your timestamped run dir
SEED    = 0  # choose 0, 1, or 2

# Models to compare (folder names exactly as saved by your training script)
MODELS = ["RoBERTa", "DistilRoBERTa", "BERT"]

# =========================
# 2) Helpers
# =========================
def f1_binary(y, p):
    y = np.asarray(y).astype(int)
    p = np.asarray(p).astype(int)
    tp = np.sum((y == 1) & (p == 1))
    fp = np.sum((y == 0) & (p == 1))
    fn = np.sum((y == 1) & (p == 0))
    if tp == 0 and (fp + fn) == 0:
        return 1.0
    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0

def load_preds(run_dir, model_name, seed):
    path = f"{run_dir}/{model_name}/fixed_epochs_seed_{seed}/preds_test.csv"
    df = pd.read_csv(path)
    if not {"y","pred"}.issubset(df.columns):
        raise ValueError(f"preds_test.csv for {model_name} missing required columns.")
    return df

def paired_bootstrap_delta_f1(y, pA, pB, BBOOT=10000, rng_seed=13):
    rng = np.random.default_rng(rng_seed)
    n = len(y)
    diffs = np.empty(BBOOT, dtype=float)
    idx = np.arange(n)

    f1_A = f1_binary(y, pA)
    f1_B = f1_binary(y, pB)
    obs_diff = f1_A - f1_B

    for i in range(BBOOT):
        samp = rng.integers(0, n, size=n)  # bootstrap indices
        diffs[i] = f1_binary(y[samp], pA[samp]) - f1_binary(y[samp], pB[samp])

    ci_lo, ci_hi = np.percentile(diffs, [2.5, 97.5])
    # simple two-sided bootstrap p-value (how often the sign is opposite or zero)
    p_two = 2 * min(np.mean(diffs >= 0.0), np.mean(diffs <= 0.0))
    p_two = float(min(p_two, 1.0))
    return {
        "f1_A": f1_A, "f1_B": f1_B, "delta_f1": obs_diff,
        "ci_lo": ci_lo, "ci_hi": ci_hi, "p_bootstrap": p_two
    }

def mcnemar_exact(y, pA, pB):
    A_correct = (pA == y)
    B_correct = (pB == y)
    n01 = int(np.sum(A_correct & ~B_correct))  # A right, B wrong
    n10 = int(np.sum(~A_correct & B_correct))  # A wrong, B right
    discordant = n01 + n10
    if discordant == 0:
        return {"n01": n01, "n10": n10, "discordant": discordant, "p_mcnemar": 1.0}
    if not HAS_SCIPY:
        return {"n01": n01, "n10": n10, "discordant": discordant, "p_mcnemar": None}  # SciPy not available
    pval = binomtest(k=min(n01, n10), n=discordant, p=0.5, alternative="two-sided").pvalue
    return {"n01": n01, "n10": n10, "discordant": discordant, "p_mcnemar": float(pval)}

# =========================
# 3) Load and sanity-check
# =========================
# Load all model predictions into a dict {name: DataFrame}
preds = {m: load_preds(RUN_DIR, m, SEED) for m in MODELS}

# Ensure all test lengths match and y labels align identically
lengths = {m: len(df) for m, df in preds.items()}
if len(set(lengths.values())) != 1:
    raise AssertionError(f"Mismatch in test lengths: {lengths}")

# Prefer strict check that y is identical across models
y_ref = preds[MODELS[0]]["y"].to_numpy().astype(int)
for m, df in preds.items():
    y_chk = df["y"].to_numpy().astype(int)
    if not np.array_equal(y_ref, y_chk):
        raise AssertionError(f"'y' mismatch between {MODELS[0]} and {m}. Ensure same split/run.")
y_true = y_ref  # shared ground truth

# For reference: per-model F1
per_model_rows = []
for m, df in preds.items():
    f1m = f1_binary(y_true, df["pred"].to_numpy().astype(int))
    per_model_rows.append({"model": m, "f1": f1m})
per_model = pd.DataFrame(per_model_rows).sort_values("f1", ascending=False).reset_index(drop=True)
print("Per-model F1 on the same test set:")
print(per_model.to_string(index=False))

# =========================
# 4) Pairwise significance
# =========================
pair_rows = []
for A_name, B_name in itertools.combinations(MODELS, 2):
    pA = preds[A_name]["pred"].to_numpy().astype(int)
    pB = preds[B_name]["pred"].to_numpy().astype(int)

    boot = paired_bootstrap_delta_f1(y_true, pA, pB, BBOOT=10000, rng_seed=13)
    mc   = mcnemar_exact(y_true, pA, pB)

    pair_rows.append({
        "model_A": A_name,
        "model_B": B_name,
        "F1_A": boot["f1_A"],
        "F1_B": boot["f1_B"],
        "ΔF1(A-B)": boot["delta_f1"],
        "95% CI low": boot["ci_lo"],
        "95% CI high": boot["ci_hi"],
        "bootstrap p": boot["p_bootstrap"],
        "McNemar n01": mc["n01"],
        "McNemar n10": mc["n10"],
        "McNemar discordant": mc["discordant"],
        "McNemar p": mc["p_mcnemar"],
    })

pair_df = pd.DataFrame(pair_rows)

# Nice formatting for display
def fmt(x, nd=3):
    return None if x is None else (f"{x:.{nd}f}")

display_cols = [
    "model_A","model_B","F1_A","F1_B","ΔF1(A-B)","95% CI low","95% CI high","bootstrap p",
    "McNemar n01","McNemar n10","McNemar discordant","McNemar p"
]
print("\nPairwise comparisons (F1 bootstrap + McNemar):")
print(pair_df[display_cols].applymap(lambda v: fmt(v) if isinstance(v, (float, np.floating)) else v).to_string(index=False))

# Optional: save results
pair_df.to_csv(f"{RUN_DIR}/pairwise_significance_seed_{SEED}.csv", index=False)
per_model.to_csv(f"{RUN_DIR}/per_model_f1_seed_{SEED}.csv", index=False)
print(f"\nSaved CSVs to: {RUN_DIR}")

import itertools, math
import numpy as np
import pandas as pd

# OPTIONAL: if SciPy is available, we can combine p-values across seeds.
try:
    from scipy.stats import binomtest, norm
    HAS_SCIPY = True
except Exception:
    HAS_SCIPY = False

# =========================
# 1) Point to your artifacts
# =========================
RUN_DIR = "/content/drive/MyDrive/sarcasm_runs_singles_v2/run_20250826_172106"
SEEDS   = [0,1,2]   # loop through all your seeds
MODELS  = ["RoBERTa", "DistilRoBERTa", "BERT"]

# =========================
# 2) Helpers
# =========================
def f1_binary(y, p):
    tp = np.sum((y == 1) & (p == 1))
    fp = np.sum((y == 0) & (p == 1))
    fn = np.sum((y == 1) & (p == 0))
    if tp == 0 and (fp + fn) == 0:
        return 1.0
    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    return 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0

def load_preds(run_dir, model_name, seed):
    path = f"{run_dir}/{model_name}/fixed_epochs_seed_{seed}/preds_test.csv"
    df = pd.read_csv(path)
    if not {"y","pred"}.issubset(df.columns):
        raise ValueError(f"Missing columns in {path}")
    return df

def paired_bootstrap_delta_f1(y, pA, pB, BBOOT=10000, rng_seed=13):
    rng = np.random.default_rng(rng_seed)
    n   = len(y)
    f1_A = f1_binary(y, pA)
    f1_B = f1_binary(y, pB)
    obs  = f1_A - f1_B
    diffs = np.empty(BBOOT)
    for i in range(BBOOT):
        idx = rng.integers(0, n, size=n)
        diffs[i] = f1_binary(y[idx], pA[idx]) - f1_binary(y[idx], pB[idx])
    lo, hi = np.percentile(diffs,[2.5,97.5])
    p_boot = 2*min(np.mean(diffs>=0), np.mean(diffs<=0))
    return dict(f1_A=f1_A,f1_B=f1_B,delta=obs,ci_lo=lo,ci_hi=hi,p_boot=float(min(p_boot,1.0)))

def mcnemar_exact(y, pA, pB):
    Aok = (pA==y); Bok = (pB==y)
    n01 = int(np.sum(Aok & ~Bok))
    n10 = int(np.sum(~Aok & Bok))
    disc = n01+n10
    if disc==0:
        return dict(n01=n01,n10=n10,disc=disc,p=1.0 if HAS_SCIPY else None)
    if not HAS_SCIPY:
        return dict(n01=n01,n10=n10,disc=disc,p=None)
    pval = binomtest(k=min(n01,n10), n=disc, p=0.5, alternative="two-sided").pvalue
    return dict(n01=n01,n10=n10,disc=disc,p=float(pval))

def stouffer_meta(pvals):
    if not HAS_SCIPY: return None
    vals = [p for p in pvals if p is not None]
    if not vals: return None
    Zs = [norm.isf(p/2.0) * (1 if p<=0.5 else -1) for p in vals]
    Z  = np.sum(Zs)/math.sqrt(len(Zs))
    return float(2*norm.sf(abs(Z)))

# =========================
# 3) Per-seed comparisons
# =========================
rows = []
for seed in SEEDS:
    preds = {m: load_preds(RUN_DIR,m,seed) for m in MODELS}
    # check y consistency
    y_ref = preds[MODELS[0]]["y"].to_numpy().astype(int)
    for m,df in preds.items():
        if not np.array_equal(y_ref, df["y"].to_numpy().astype(int)):
            raise AssertionError(f"y mismatch at seed {seed} for {m}")
    y = y_ref
    for A,B in itertools.combinations(MODELS,2):
        pA = preds[A]["pred"].to_numpy().astype(int)
        pB = preds[B]["pred"].to_numpy().astype(int)
        boot = paired_bootstrap_delta_f1(y,pA,pB)
        mc   = mcnemar_exact(y,pA,pB)
        rows.append({
            "seed":seed,"model_A":A,"model_B":B,
            "F1_A":boot["f1_A"],"F1_B":boot["f1_B"],"ΔF1":boot["delta"],
            "95%CI_lo":boot["ci_lo"],"95%CI_hi":boot["ci_hi"],"bootstrap_p":boot["p_boot"],
            "McNemar_n01":mc["n01"],"McNemar_n10":mc["n10"],"McNemar_disc":mc["disc"],"McNemar_p":mc["p"]
        })
per_seed_df = pd.DataFrame(rows)
print("\nPer-seed pairwise results:")
print(per_seed_df.to_string(index=False))

per_seed_df.to_csv(f"{RUN_DIR}/pairwise_significance_per_seed.csv",index=False)

# =========================
# 4) Aggregate across seeds
# =========================
agg_rows = []
for A,B in itertools.combinations(MODELS,2):
    sub = per_seed_df[(per_seed_df.model_A==A)&(per_seed_df.model_B==B)]
    F1_A_mean, F1_B_mean, d_mean = sub["F1_A"].mean(), sub["F1_B"].mean(), sub["ΔF1"].mean()
    p_boot_comb = stouffer_meta(sub["bootstrap_p"].tolist())
    p_mc_comb   = stouffer_meta(sub["McNemar_p"].tolist())
    agg_rows.append({
        "model_A":A,"model_B":B,
        "F1_A_mean":F1_A_mean,"F1_B_mean":F1_B_mean,"ΔF1_mean":d_mean,
        "bootstrap_p_comb":p_boot_comb,"McNemar_p_comb":p_mc_comb
    })
agg_df = pd.DataFrame(agg_rows)
print("\nAggregate across seeds:")
print(agg_df.to_string(index=False))
agg_df.to_csv(f"{RUN_DIR}/pairwise_significance_aggregate.csv",index=False)

print(f"\nSaved per-seed and aggregate CSVs to {RUN_DIR}")

